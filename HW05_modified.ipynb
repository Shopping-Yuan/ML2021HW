{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhkN9KOKHt2CbNQkHqz7tT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW05_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "part 1 load file (finish)\n",
        "part 2 clean data (finish on 12/5)\n",
        "part 3 tokenize : sentencepiece (finish on 12/6)\n",
        "part 4 build model encoder decoder (finish on 12/7)\n",
        "part 5 beam search (finish on 12/8)\n",
        "part 6 label smooth (finish on 12/9)\n",
        "\n",
        "part 7 test (finish on 12/10)\n"
      ],
      "metadata": {
        "id": "WTv4XN2qB_fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "shutil.copyfile('/content/drive/MyDrive/ted2020.tgz','/content/ted2020.tgz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "X23RaTWe9hoU",
        "outputId": "d9b80c4f-68eb-4da7-e10f-f1f3478d9013"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/ted2020.tgz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "rawdata_file = \"/content/ted2020.tgz\"\n",
        "unzip_path = \"/content/train_dev/\"\n",
        "# open file\n",
        "file = tarfile.open(rawdata_file)\n",
        "\n",
        "# extracting file\n",
        "file.extractall(unzip_path)\n",
        "\n",
        "file.close()\n"
      ],
      "metadata": {
        "id": "2GhVMRgifeKi"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_lang = \"raw.en\"\n",
        "tgt_lang = \"raw.zh\"\n",
        "\n",
        "src_path = f\"{unzip_path}{src_lang}\"\n",
        "tgt_path = f\"{unzip_path}{tgt_lang}\"\n",
        "print(tgt_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH2bCPFbvhK2",
        "outputId": "35bfba63-7a51-46ac-e971-117a93d9de4c"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/train_dev/raw.zh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chr(10022).join([chr(21407), chr(31070), chr(21855), chr(21205)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jldqXuqB6wdm",
        "outputId": "3dbd25fb-b2a0-4d7f-9396-296a8eb29b96"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "原✦神✦啟✦動\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for path in [src_path,tgt_path]:\n",
        "#   with open(path, \"r\") as f:\n",
        "  #  data = f.readlines()\n",
        "  #  print(type(data),data[0:5])#<class 'list'> ['Thank you so much, Chris.\\n', \"And it'......on.\\n']\n",
        "    # data = f.read().splitlines()\n",
        "    # print(type(data),data[0:5],len(data))"
      ],
      "metadata": {
        "id": "BjIUpASA076k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "# print(ord(\"，\"))\n",
        "# print(chr(65292))"
      ],
      "metadata": {
        "id": "ZhEDa7SN6us_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "def to_halfwidth(string):\n",
        "  return \"\".join(unicodedata.normalize('NFKC',letter) for letter in string)\n",
        "# print(to_halfwidth(\"ＡＳＤＦＦＦＦＱ\")) #ASDFFFFQ"
      ],
      "metadata": {
        "id": "hFgPn-1i1EZR"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "def clean_s_zh(s):\n",
        "    s = to_halfwidth(s)\n",
        "    # step 1 : delete — _\n",
        "    delete = \" _()[]\"\n",
        "    delete_rules = s.maketrans(\"\",\"\",delete)\n",
        "    s = s.translate(delete_rules)\n",
        "\n",
        "    # step 2 : replace “” with \"\"\n",
        "    to_be_replace = '“”'\n",
        "    replace = '\"\"'\n",
        "    replace_dict = dict(zip(to_be_replace,replace))\n",
        "    # print(replace_dict.items()) : dict_items([('“', '\"'), ('”', '\"')])\n",
        "\n",
        "    # step 3 : add **END** before and after punctuation\n",
        "\n",
        "    \"\"\"\n",
        "    The number of sentences in one line may be different\n",
        "    in line pairs of source and target set.\n",
        "    so I try to use \"。!?\" or \".!?\" to split sentences.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "#    punctuation = \"。,;!?()\\\"~「」\"\n",
        "    punctuation = \"♪♫。!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    zh_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return zh_list\n",
        "\n",
        "def clean_s_en(s):\n",
        "    s = to_halfwidth(s)\n",
        "\n",
        "    replace_dict = {}\n",
        "\n",
        "    delete = \"-()[]\"\n",
        "    for char in delete:\n",
        "      replace_dict[char] = \"\"\n",
        "\n",
        "    punctuation = \"♪♫!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "    s = pattern.sub(\"**END**\",s)\n",
        "\n",
        "    en_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return en_list"
      ],
      "metadata": {
        "id": "5dwWeVz-sPI2"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "# result = pattern.sub(\"**END**\",\"There are many people in U.S. w.r.t. in Taiwan.Thank you.\")\n",
        "# print(result)"
      ],
      "metadata": {
        "id": "tt8FnBUBrSZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_file(path,fuction):\n",
        "  with open(path, \"r\") as f:\n",
        "    data = f.read()\n",
        "    return(fuction(data))\n",
        "src_list = load_file(src_path,clean_s_en)\n",
        "tgt_list = load_file(tgt_path,clean_s_zh)"
      ],
      "metadata": {
        "id": "yeSGu6mwmbzh"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(tgt_path, \"r\") as f:\n",
        "#     data = f.readlines()\n",
        "#     print(data[17])\n",
        "# print(tgt_list[17])"
      ],
      "metadata": {
        "id": "iXOFN_BQN7ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_by_END(s):\n",
        "    list_s = []\n",
        "    for line_string in s.strip(\"**END**\").split(\"**END**\"):\n",
        "      if line_string not in [\"\",\" \"]:\n",
        "         list_s.append(line_string)\n",
        "    return(list_s)\n",
        "\n",
        "def devide_en_again(s,punctuation = \":;\"):\n",
        "    replace_dict = {}\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules_src = s.maketrans(replace_dict)\n",
        "    new_s = divide_by_END(s.translate(replace_rules_src))\n",
        "    return new_s\n",
        "def remove_too_long(src_list,tgt_list,s_threshold = 400, t_threshold = 120):\n",
        "    too_long_src = 0\n",
        "    too_long_tgt = 0\n",
        "    remove = False\n",
        "    new_s = []\n",
        "    new_t = []\n",
        "    for i in range(len(src_list)):\n",
        "      if ((len(src_list[i])>s_threshold)):\n",
        "        remove = True\n",
        "        too_long_src += 1\n",
        "      if (len(tgt_list[i])>t_threshold):\n",
        "        remove = True\n",
        "        too_long_tgt += 1\n",
        "      if remove == False:\n",
        "        new_s.append(src_list[i])\n",
        "        new_t.append(tgt_list[i])\n",
        "      else :\n",
        "        remove = False\n",
        "    return(new_s,new_t,too_long_src,too_long_tgt)\n"
      ],
      "metadata": {
        "id": "0RRwmoBw59gS"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_data_pairs(src_list,tgt_list):\n",
        "    index = 0\n",
        "    new_src_list = []\n",
        "    new_tgt_list = []\n",
        "\n",
        "    same = 0\n",
        "    add_next = 0\n",
        "    split_again = 0\n",
        "    not_use = 0\n",
        "\n",
        "    while(index < len(src_list)):\n",
        "\n",
        "      src = divide_by_END(src_list[index])\n",
        "      tgt = divide_by_END(tgt_list[index])\n",
        "      # case 1 : src is as long as tgt , finished.\n",
        "      if len(src) == len(tgt):\n",
        "        new_src_list += src\n",
        "        new_tgt_list += tgt\n",
        "        same += 1\n",
        "        index += 1\n",
        "\n",
        "      else :\n",
        "        # if it is not the last one : both src and tgt add next sentence\n",
        "        if index != len(src_list)-1:\n",
        "          src_add_next = divide_by_END(src_list[index] + src_list[index+1])\n",
        "          tgt_add_next = divide_by_END(tgt_list[index] + tgt_list[index+1])\n",
        "          # case 2 : src_add_next is as long as tgt_add_next , finished.\n",
        "          if len(src_add_next) == len(tgt_add_next):\n",
        "            new_src_list += src_add_next\n",
        "            new_tgt_list += tgt_add_next\n",
        "            add_next += 2\n",
        "            index += 2\n",
        "\n",
        "          # using new punctuation to divide tgt (english) sentence.\n",
        "          else :\n",
        "            src_add_next = devide_en_again(src_list[index] + src_list[index+1])\n",
        "            # case 3 : src_add_next is as long as tgt_add_next , finished.\n",
        "            if len(src_add_next) == len(tgt_add_next):\n",
        "              new_src_list += src_add_next\n",
        "              new_tgt_list += tgt_add_next\n",
        "              split_again +=2\n",
        "              index += 2\n",
        "\n",
        "            # case 4 : sentence will not be used.\n",
        "            else :\n",
        "              not_use += 1\n",
        "              # if to_do == 1 :\n",
        "              #   print(index,src_add_next,tgt_add_next,len(src_add_next),len(tgt_add_next))\n",
        "              index += 1\n",
        "\n",
        "        # if it is the last one\n",
        "        else :\n",
        "          not_use += 1\n",
        "          index += 1\n",
        "    print(len(new_src_list))\n",
        "    new_src_list,new_tgt_list,too_long_src,too_long_tgt = remove_too_long(new_src_list,new_tgt_list)\n",
        "    print(index,same,add_next,split_again,not_use,too_long_src,too_long_tgt,len(new_src_list))\n",
        "\n",
        "    return(new_src_list,new_tgt_list)"
      ],
      "metadata": {
        "id": "UVYvYCpuDQDa"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_src_list,new_tgt_list = check_data_pairs(src_list,tgt_list)\n",
        "# print(dataset[0][0],dataset[1][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibAIaACprYoO",
        "outputId": "3ef7f9b6-ece7-46ed-aa0f-73b8588b786d"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "389602\n",
            "394066 350421 12284 2980 28381 677 605 388728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/train_dev/data.txt\"\n",
        "label_path = \"/content/train_dev/label.txt\"\n",
        "with open(data_path, \"w\") as f:\n",
        "  f.write(\"\\n\".join(new_src_list))\n",
        "with open(label_path, \"w\") as f:\n",
        "  f.write(\"\\n\".join(new_tgt_list))"
      ],
      "metadata": {
        "id": "DNf0E5FDPsj8"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max(l):\n",
        "  exceed = 0\n",
        "  maxize_sentence_length = 0\n",
        "  for sentence in l:\n",
        "    if len(sentence) > maxize_sentence_length:\n",
        "      maxize_sentence_length = len(sentence)\n",
        "  return(maxize_sentence_length)\n",
        "\n",
        "data_info = {\n",
        "    \"data\":{\"path\":data_path,\"lang\":\"en\",\"max_l\":get_max(new_src_list)},\n",
        "    \"label\":{\"path\":label_path,\"lang\":\"zh\",\"max_l\":get_max(new_tgt_list)},\n",
        "}\n",
        "print(get_max(new_src_list),get_max(new_tgt_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeGRxKJu64ib",
        "outputId": "003c90b0-29b9-4d78-ba2d-bc056836e9b1"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400 120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ti-UGuHNhh",
        "outputId": "cf587654-3739-48b1-9cb1-1d9049e842aa"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "def tokenized(data_or_label,data_info=data_info):\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      input=data_info[data_or_label][\"path\"],\n",
        "      model_prefix=f'spm{vocab_size}_{data_info[data_or_label][\"lang\"]}',\n",
        "      vocab_size=vocab_size,\n",
        "      character_coverage=1,\n",
        "      model_type='unigram', # 'bpe' works as well\n",
        "      input_sentence_size=400000,\n",
        "      shuffle_input_sentence=True,\n",
        "      normalization_rule_name='nmt_nfkc_cf',\n",
        "      unk_id=1,\n",
        "      bos_id=2,\n",
        "      eos_id=3,\n",
        "      pad_id=0,\n",
        "  )"
      ],
      "metadata": {
        "id": "LoNi7CeA3P9G"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized(\"data\")\n",
        "tokenized(\"label\")"
      ],
      "metadata": {
        "id": "dJPM2p4QfG33"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "def bos_eos_padding(dataset,src_max,tgt_max):\n",
        "  s_en = spm.SentencePieceProcessor(model_file=\"/content/spm8000_en.model\")\n",
        "  s_zh = spm.SentencePieceProcessor(model_file=\"/content/spm8000_zh.model\")\n",
        "  padding_src = []\n",
        "  padding_tgt = []\n",
        "  for src,tgt in dataset:\n",
        "    s = s_en.encode(src, out_type=int)\n",
        "    s = np.append([2],np.pad(s,(0, src_max-len(src)+1), constant_values = 0))\n",
        "    s = np.append(s,[3])\n",
        "    padding_src.append(s)\n",
        "\n",
        "    t = s_zh.encode(tgt, out_type=int)\n",
        "    t = np.append([2],np.pad(t,(0, tgt_max-len(tgt)+1), constant_values = 0))\n",
        "    t = np.append(t,[3])\n",
        "    padding_tgt.append(t)\n",
        "  return(list(zip(padding_src,padding_tgt)))"
      ],
      "metadata": {
        "id": "ByrUmAvFkKk9"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bos_eos_padding([([5,7],[6,5])],5,10)"
      ],
      "metadata": {
        "id": "s7NABiz1nWfd"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "def data_set_preparing():\n",
        "    src_set = []\n",
        "    tgt_set = []\n",
        "\n",
        "    with open(\"/content/train_dev/data.txt\",\"r\") as in_f :\n",
        "      for line in in_f:\n",
        "        src_set.append(line)\n",
        "    with open(\"/content/train_dev/label.txt\",\"r\") as in_f :\n",
        "      for line in in_f:\n",
        "        tgt_set.append(line)\n",
        "\n",
        "    dataset = list(zip(src_set,tgt_set))\n",
        "    dataset = bos_eos_padding(dataset,data_info[\"data\"][\"max_l\"],data_info[\"label\"][\"max_l\"])\n",
        "    train_set, valid_set = data.random_split(dataset,[0.9,0.1])\n",
        "    print(train_set[0][0])\n",
        "\n",
        "    with open(\"/content/train_dev/tokenized_train_data.txt\", 'w') as out_f:\n",
        "      for line_pair in train_set:\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[0])+\"\\n\")\n",
        "    with open(\"/content/train_dev/tokenized_valid_data.txt\", 'w') as out_f:\n",
        "      for line_pair in valid_set:\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[0])+\"\\n\")\n",
        "    with open(\"/content/train_dev/tokenized_train_label.txt\", 'w') as out_f:\n",
        "      for line_pair in train_set:\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[1])+\"\\n\")\n",
        "    with open(\"/content/train_dev/tokenized_valid_label.txt\", 'w') as out_f:\n",
        "      for line_pair in valid_set:\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[1])+\"\\n\")\n",
        "data_set_preparing()"
      ],
      "metadata": {
        "id": "sX_7znJc170w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "233b6dc9-5a15-4688-c2f3-da1100603a15"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2   23 1256   11   20  144   12 3390  108  138   12   17  211   68\n",
            "  756  602   34   53 3843   34   53  293    6  172  471   34   53  864\n",
            "  189  234   34   69   14   88   93    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class myDataset(Dataset):\n",
        "  def __init__(self,mode):\n",
        "    if mode == \"train\":\n",
        "      self.data_path = \"/content/train_dev/tokenized_train_data.txt\"\n",
        "      self.label_path = \"/content/train_dev/tokenized_train_label.txt\"\n",
        "    elif mode == \"val\":\n",
        "      self.data_path = \"/content/train_dev/tokenized_valid_data.txt\"\n",
        "      self.label_path = \"/content/train_dev/tokenized_valid_label.txt\"\n",
        "\n",
        "    data = []\n",
        "    with open(self.data_path,\"r\") as f :\n",
        "      d_l = f.readlines()\n",
        "      for line in d_l:\n",
        "        add = np.array(line.strip().split(\" \")).astype(int)\n",
        "        data = np.append(data,add)\n",
        "    self.data = torch.LongTensor(data)\n",
        "\n",
        "    label = []\n",
        "    with open(self.label_path,\"r\") as f :\n",
        "      l_l = f.readlines()\n",
        "      for line in l_l:\n",
        "        add = np.array(line.strip().split(\" \")).astype(int)\n",
        "        label = np.append(label,add)\n",
        "    self.label = torch.LongTensor(np.array(label))\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.data, self.label\n"
      ],
      "metadata": {
        "id": "dbfi-DvrlDhI"
      },
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "with open(\"/content/train_dev/tokenized_train_data.txt\",\"r\") as f :\n",
        "  d_l = f.readlines()\n",
        "  print(len(d_l))\n",
        "  print(d_l[0].strip().split(\" \"))\n",
        "  add = np.array(line.strip().split(\" \")).astype(int)\n",
        "data = np.append(data,add)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyjGbovh-zzX",
        "outputId": "2b930343-42ae-4bba-b78b-4cbdf5cd8e2b"
      },
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "349856\n",
            "['2', '23', '1256', '11', '20', '144', '12', '3390', '108', '138', '12', '17', '211', '68', '756', '602', '34', '53', '3843', '34', '53', '293', '6', '172', '471', '34', '53', '864', '189', '234', '34', '69', '14', '88', '93', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 400\n",
        "\n",
        "train_loader = DataLoader(\n",
        "  myDataset(\"train\"),\n",
        "  batch_size=batch_size,\n",
        "  shuffle=True,\n",
        "  num_workers=8,\n",
        "  pin_memory=True,\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "  myDataset(\"val\"),\n",
        "  batch_size=batch_size,\n",
        "  num_workers=8,\n",
        "  pin_memory=True,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "L5SsVH3Q6-Ie",
        "outputId": "90a1cef9-05da-4597-88e2-0d9582299285"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-298-2974b250bdda>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m train_loader = DataLoader(\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mmyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-292-c64749e550c8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md_l\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0madd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5442\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5443\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5444\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch = {\n",
        "#   \"id\": id, # 每個 example 的 id\n",
        "#   \"nsentences\": len(samples), # batch size 句子數\n",
        "#   \"ntokens\": ntokens, # batch size 字數\n",
        "#   \"net_input\": {\n",
        "#       \"src_tokens\": src_tokens, # 來源語言的序列\n",
        "#       \"src_lengths\": src_lengths, # 每句話沒有 pad 過的長度\n",
        "#       \"prev_output_tokens\": prev_output_tokens, # 上面提到右 shift 一格後的目標序列\n",
        "#   },\n",
        "#   \"target\": target, # 目標序列\n",
        "# }"
      ],
      "metadata": {
        "id": "0VgNDQMVlGLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cuda_env = utils.CudaEnvironment()\n",
        "# utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "cYgg7zwHk1jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # HINT: transformer 架構\n",
        "# # from fairseq.models.transformer import (\n",
        "# #     TransformerEncoder,\n",
        "# #     TransformerDecoder,\n",
        "# # )\n",
        "\n",
        "# def build_model(args, task):\n",
        "#     \"\"\" 按照參數設定建置模型 \"\"\"\n",
        "#     src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "#     # 詞嵌入\n",
        "#     encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "#     decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "\n",
        "#     # 編碼器與解碼器\n",
        "#     # TODO: 替換成 TransformerEncoder 和 TransformerDecoder\n",
        "#     encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
        "#     decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "#     # 序列到序列模型\n",
        "#     model = Seq2Seq(args, encoder, decoder)\n",
        "\n",
        "#     # 序列到序列模型的初始化很重要 需要特別處理\n",
        "#     def init_params(module):\n",
        "#         from fairseq.modules import MultiheadAttention\n",
        "#         if isinstance(module, nn.Linear):\n",
        "#             module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "#         if isinstance(module, nn.Embedding):\n",
        "#             module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             if module.padding_idx is not None:\n",
        "#                 module.weight.data[module.padding_idx].zero_()\n",
        "#         if isinstance(module, MultiheadAttention):\n",
        "#             module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#         if isinstance(module, nn.RNNBase):\n",
        "#             for name, param in module.named_parameters():\n",
        "#                 if \"weight\" in name or \"bias\" in name:\n",
        "#                     param.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "#     # 初始化模型\n",
        "#     model.apply(init_params)\n",
        "#     return model"
      ],
      "metadata": {
        "id": "-Zy7vAVXlTAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "#     def __init__(self, args, encoder, decoder):\n",
        "#         super().__init__(encoder, decoder)\n",
        "#         self.args = args\n",
        "\n",
        "#     def forward(\n",
        "#         self,\n",
        "#         src_tokens,\n",
        "#         src_lengths,\n",
        "#         prev_output_tokens,\n",
        "#         return_all_hiddens: bool = True,\n",
        "#     ):\n",
        "#         \"\"\"\n",
        "#         Run the forward pass for an encoder-decoder model.\n",
        "#         \"\"\"\n",
        "#         encoder_out = self.encoder(\n",
        "#             src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "#         )\n",
        "#         logits, extra = self.decoder(\n",
        "#             prev_output_tokens,\n",
        "#             encoder_out=encoder_out,\n",
        "#             src_lengths=src_lengths,\n",
        "#             return_all_hiddens=return_all_hiddens,\n",
        "#         )\n",
        "#         return logits, extra"
      ],
      "metadata": {
        "id": "hBNhGpgS08gI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}