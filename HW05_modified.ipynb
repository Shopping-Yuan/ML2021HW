{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIt/nCmzbZ+SJFNR87xtCj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW05_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "part 1 load file (finish)\n",
        "part 2 clean data (finish on 12/5)\n",
        "part 3 tokenize : sentencepiece (finish on 12/6)\n",
        "part 4 build model encoder decoder (finish on 12/7)\n",
        "part 5 beam search (finish on 12/8)\n",
        "part 6 label smooth (finish on 12/9)\n",
        "\n",
        "part 7 test (finish on 12/10)\n"
      ],
      "metadata": {
        "id": "WTv4XN2qB_fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "shutil.copyfile('/content/drive/MyDrive/ted2020.tgz','/content/ted2020.tgz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "X23RaTWe9hoU",
        "outputId": "68ed5ba8-b782-485f-b919-66e2aa9c708b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/ted2020.tgz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "rawdata_file = \"/content/ted2020.tgz\"\n",
        "unzip_path = \"/content/train_dev/\"\n",
        "# open file\n",
        "file = tarfile.open(rawdata_file)\n",
        "\n",
        "# extracting file\n",
        "file.extractall(unzip_path)\n",
        "\n",
        "file.close()\n"
      ],
      "metadata": {
        "id": "2GhVMRgifeKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_lang = \"raw.en\"\n",
        "tgt_lang = \"raw.zh\"\n",
        "\n",
        "src_path = f\"{unzip_path}{src_lang}\"\n",
        "tgt_path = f\"{unzip_path}{tgt_lang}\"\n",
        "print(tgt_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH2bCPFbvhK2",
        "outputId": "3764ecc1-48a5-4525-fda4-01a9f9771f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/train_dev/raw.zh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chr(10022).join([chr(21407), chr(31070), chr(21855), chr(21205)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jldqXuqB6wdm",
        "outputId": "e4274eca-db45-43b5-a699-4c57f859276b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "原✦神✦啟✦動\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for path in [src_path,tgt_path]:\n",
        "#   with open(path, \"r\") as f:\n",
        "  #  data = f.readlines()\n",
        "  #  print(type(data),data[0:5])#<class 'list'> ['Thank you so much, Chris.\\n', \"And it'......on.\\n']\n",
        "    # data = f.read().splitlines()\n",
        "    # print(type(data),data[0:5],len(data))"
      ],
      "metadata": {
        "id": "BjIUpASA076k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "# print(ord(\"，\"))\n",
        "# print(chr(65292))"
      ],
      "metadata": {
        "id": "ZhEDa7SN6us_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "def to_halfwidth(string):\n",
        "  return \"\".join(unicodedata.normalize('NFKC',letter) for letter in string)\n",
        "# print(to_halfwidth(\"ＡＳＤＦＦＦＦＱ\")) #ASDFFFFQ"
      ],
      "metadata": {
        "id": "hFgPn-1i1EZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "def clean_s_zh(s):\n",
        "    s = to_halfwidth(s)\n",
        "    # step 1 : delete — _\n",
        "    delete = \" _()[]\"\n",
        "    delete_rules = s.maketrans(\"\",\"\",delete)\n",
        "    s = s.translate(delete_rules)\n",
        "\n",
        "    # step 2 : replace “” with \"\"\n",
        "    to_be_replace = '“”'\n",
        "    replace = '\"\"'\n",
        "    replace_dict = dict(zip(to_be_replace,replace))\n",
        "    # print(replace_dict.items()) : dict_items([('“', '\"'), ('”', '\"')])\n",
        "\n",
        "    # step 3 : add **END** before and after punctuation\n",
        "\n",
        "    \"\"\"\n",
        "    The number of sentences in one line may be different\n",
        "    in line pairs of source and target set.\n",
        "    so I try to use \"。!?\" or \".!?\" to split sentences.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "#    punctuation = \"。,;!?()\\\"~「」\"\n",
        "    punctuation = \"♪♫。!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    zh_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return zh_list\n",
        "\n",
        "def clean_s_en(s):\n",
        "    s = to_halfwidth(s)\n",
        "\n",
        "    replace_dict = {}\n",
        "\n",
        "    delete = \"-()[]\"\n",
        "    for char in delete:\n",
        "      replace_dict[char] = \"\"\n",
        "\n",
        "    punctuation = \"♪♫!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "    s = pattern.sub(\"**END**\",s)\n",
        "\n",
        "    en_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return en_list"
      ],
      "metadata": {
        "id": "5dwWeVz-sPI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "# result = pattern.sub(\"**END**\",\"There are many people in U.S. w.r.t. in Taiwan.Thank you.\")\n",
        "# print(result)"
      ],
      "metadata": {
        "id": "tt8FnBUBrSZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_file(path,fuction):\n",
        "  with open(path, \"r\") as f:\n",
        "    data = f.read()\n",
        "    return(fuction(data))\n",
        "src_list = load_file(src_path,clean_s_en)\n",
        "tgt_list = load_file(tgt_path,clean_s_zh)"
      ],
      "metadata": {
        "id": "yeSGu6mwmbzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(tgt_path, \"r\") as f:\n",
        "#     data = f.readlines()\n",
        "#     print(data[17])\n",
        "# print(tgt_list[17])"
      ],
      "metadata": {
        "id": "iXOFN_BQN7ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_by_END(s):\n",
        "    list_s = []\n",
        "    for line_string in s.strip(\"**END**\").split(\"**END**\"):\n",
        "      if line_string not in [\"\",\" \"]:\n",
        "         list_s.append(line_string)\n",
        "    return(list_s)\n",
        "\n",
        "def devide_en_again(s,punctuation = \":;\"):\n",
        "    replace_dict = {}\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules_src = s.maketrans(replace_dict)\n",
        "    new_s = divide_by_END(s.translate(replace_rules_src))\n",
        "    return new_s\n",
        "def remove_too_long(src_list,tgt_list,s_threshold = 400, t_threshold = 120):\n",
        "    too_long_src = 0\n",
        "    too_long_tgt = 0\n",
        "    remove = False\n",
        "    new_s = []\n",
        "    new_t = []\n",
        "    for i in range(len(src_list)):\n",
        "      if ((len(src_list[i])>s_threshold)):\n",
        "        remove = True\n",
        "        too_long_src += 1\n",
        "      if (len(tgt_list[i])>t_threshold):\n",
        "        remove = True\n",
        "        too_long_tgt += 1\n",
        "      if remove == False:\n",
        "        new_s.append(src_list[i])\n",
        "        new_t.append(tgt_list[i])\n",
        "      else :\n",
        "        remove = False\n",
        "    return(new_s,new_t,too_long_src,too_long_tgt)\n"
      ],
      "metadata": {
        "id": "0RRwmoBw59gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_data_pairs(src_list,tgt_list):\n",
        "    index = 0\n",
        "    new_src_list = []\n",
        "    new_tgt_list = []\n",
        "\n",
        "    same = 0\n",
        "    add_next = 0\n",
        "    split_again = 0\n",
        "    not_use = 0\n",
        "\n",
        "    while(index < len(src_list)):\n",
        "\n",
        "      src = divide_by_END(src_list[index])\n",
        "      tgt = divide_by_END(tgt_list[index])\n",
        "      # case 1 : src is as long as tgt , finished.\n",
        "      if len(src) == len(tgt):\n",
        "        new_src_list += src\n",
        "        new_tgt_list += tgt\n",
        "        same += 1\n",
        "        index += 1\n",
        "\n",
        "      else :\n",
        "        # if it is not the last one : both src and tgt add next sentence\n",
        "        if index != len(src_list)-1:\n",
        "          src_add_next = divide_by_END(src_list[index] + src_list[index+1])\n",
        "          tgt_add_next = divide_by_END(tgt_list[index] + tgt_list[index+1])\n",
        "          # case 2 : src_add_next is as long as tgt_add_next , finished.\n",
        "          if len(src_add_next) == len(tgt_add_next):\n",
        "            new_src_list += src_add_next\n",
        "            new_tgt_list += tgt_add_next\n",
        "            add_next += 2\n",
        "            index += 2\n",
        "\n",
        "          # using new punctuation to divide tgt (english) sentence.\n",
        "          else :\n",
        "            src_add_next = devide_en_again(src_list[index] + src_list[index+1])\n",
        "            # case 3 : src_add_next is as long as tgt_add_next , finished.\n",
        "            if len(src_add_next) == len(tgt_add_next):\n",
        "              new_src_list += src_add_next\n",
        "              new_tgt_list += tgt_add_next\n",
        "              split_again +=2\n",
        "              index += 2\n",
        "\n",
        "            # case 4 : sentence will not be used.\n",
        "            else :\n",
        "              not_use += 1\n",
        "              # if to_do == 1 :\n",
        "              #   print(index,src_add_next,tgt_add_next,len(src_add_next),len(tgt_add_next))\n",
        "              index += 1\n",
        "\n",
        "        # if it is the last one\n",
        "        else :\n",
        "          not_use += 1\n",
        "          index += 1\n",
        "    print(len(new_src_list))\n",
        "    new_src_list,new_tgt_list,too_long_src,too_long_tgt = remove_too_long(new_src_list,new_tgt_list)\n",
        "    print(index,same,add_next,split_again,not_use,too_long_src,too_long_tgt,len(new_src_list))\n",
        "\n",
        "    return(new_src_list,new_tgt_list)"
      ],
      "metadata": {
        "id": "UVYvYCpuDQDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_src_list,new_tgt_list = check_data_pairs(src_list,tgt_list)\n",
        "# print(dataset[0][0],dataset[1][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibAIaACprYoO",
        "outputId": "4af83082-9bee-417c-fd57-77dbd736345c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "389602\n",
            "394066 350421 12284 2980 28381 677 605 388728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/train_dev/data.txt\"\n",
        "label_path = \"/content/train_dev/label.txt\"\n",
        "with open(data_path, \"w\") as f:\n",
        "  f.write(\"\\n\".join(new_src_list))\n",
        "with open(label_path, \"w\") as f:\n",
        "  f.write(\"\\n\".join(new_tgt_list))"
      ],
      "metadata": {
        "id": "DNf0E5FDPsj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max(l):\n",
        "  exceed = 0\n",
        "  maxize_sentence_length = 0\n",
        "  for sentence in l:\n",
        "    if len(sentence) > maxize_sentence_length:\n",
        "      maxize_sentence_length = len(sentence)\n",
        "  return(maxize_sentence_length)\n",
        "\n",
        "data_info = {\n",
        "    \"data\":{\"path\":data_path,\"lang\":\"en\",\"max_l\":get_max(new_src_list)},\n",
        "    \"label\":{\"path\":label_path,\"lang\":\"zh\",\"max_l\":get_max(new_tgt_list)},\n",
        "}\n",
        "print(get_max(new_src_list),get_max(new_tgt_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeGRxKJu64ib",
        "outputId": "55d81f60-a040-4fff-d04b-571ca5298f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400 120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ti-UGuHNhh",
        "outputId": "1655562b-8e90-4f60-bdc8-1795e9281f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "def tokenized(data_or_label,data_info=data_info):\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      input=data_info[data_or_label][\"path\"],\n",
        "      model_prefix=f'spm{vocab_size}_{data_info[data_or_label][\"lang\"]}',\n",
        "      vocab_size=vocab_size,\n",
        "      character_coverage=1,\n",
        "      model_type='unigram', # 'bpe' works as well\n",
        "      input_sentence_size=400000,\n",
        "      shuffle_input_sentence=True,\n",
        "      normalization_rule_name='nmt_nfkc_cf',\n",
        "      unk_id=1,\n",
        "      bos_id=2,\n",
        "      eos_id=3,\n",
        "      pad_id=0,\n",
        "  )"
      ],
      "metadata": {
        "id": "LoNi7CeA3P9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized(\"data\")\n",
        "tokenized(\"label\")"
      ],
      "metadata": {
        "id": "dJPM2p4QfG33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "def bos_eos_padding(dataset,src_max,tgt_max):\n",
        "  s_en = spm.SentencePieceProcessor(model_file=\"/content/spm8000_en.model\")\n",
        "  s_zh = spm.SentencePieceProcessor(model_file=\"/content/spm8000_zh.model\")\n",
        "  padding_src = []\n",
        "  padding_tgt = []\n",
        "  len_s = 0\n",
        "  len_t = 0\n",
        "  for src,tgt in dataset:\n",
        "    s = s_en.encode(src, out_type=int)\n",
        "    s = np.append(s,[3])\n",
        "    s = np.append([2],np.pad(s,(0, src_max-len(s)), constant_values = 0))\n",
        "    padding_src.append(s)\n",
        "    # if len(s)!= len_s:\n",
        "    #   len_s = len(s)\n",
        "    #   print(len(s))\n",
        "\n",
        "    t = s_zh.encode(tgt, out_type=int)\n",
        "    t = np.append(t,[3])\n",
        "    t = np.append([2],np.pad(t,(0, tgt_max-len(t)), constant_values = 0))\n",
        "    padding_tgt.append(t)\n",
        "    # if len(t)!= len_t:\n",
        "    #   len_t = len(t)\n",
        "    #   print(len(t))\n",
        "  return(list(zip(padding_src,padding_tgt)))"
      ],
      "metadata": {
        "id": "ByrUmAvFkKk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bos_eos_padding([(\"hello world\",\"_哈囉\")],5,10)"
      ],
      "metadata": {
        "id": "s7NABiz1nWfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e046c60-a6a5-46d3-d495-44865b211349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(array([   2, 7227,  271,    3,    0,    0]),\n",
              "  array([   2, 2178,    1, 2998, 4387,    3,    0,    0,    0,    0,    0]))]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_en = spm.SentencePieceProcessor(model_file=\"/content/spm8000_en.model\")\n",
        "s_en.encode(\"hello world!\", out_type=int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJDiq6xoR0iT",
        "outputId": "e73f507a-ca83-4099-9fde-edb4c4a7d465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4993, 86, 349]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "def data_set_preparing():\n",
        "    src_set = []\n",
        "    tgt_set = []\n",
        "\n",
        "    with open(\"/content/train_dev/data.txt\",\"r\") as in_f :\n",
        "      for line in in_f:\n",
        "        src_set.append(line)\n",
        "    with open(\"/content/train_dev/label.txt\",\"r\") as in_f :\n",
        "      for line in in_f:\n",
        "        tgt_set.append(line)\n",
        "\n",
        "    dataset = list(zip(src_set,tgt_set))\n",
        "    dataset = bos_eos_padding(dataset,data_info[\"data\"][\"max_l\"],data_info[\"label\"][\"max_l\"])\n",
        "    train_set, valid_set = data.random_split(dataset,[0.9,0.1])\n",
        "    # print(train_set[0][0])\n",
        "\n",
        "    with open(\"/content/train_dev/tokenized_train_data.txt\", 'w') as out_f:\n",
        "      for line_pair in train_set:\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[0])+\"\\n\")\n",
        "    with open(\"/content/train_dev/tokenized_valid_data.txt\", 'w') as out_f:\n",
        "      for line_pair in valid_set:\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[0])+\"\\n\")\n",
        "    with open(\"/content/train_dev/tokenized_train_label.txt\", 'w') as out_f:\n",
        "      for line_pair in train_set:\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[1])+\"\\n\")\n",
        "    with open(\"/content/train_dev/tokenized_valid_label.txt\", 'w') as out_f:\n",
        "      for line_pair in valid_set:\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[1])+\"\\n\")\n",
        "data_set_preparing()"
      ],
      "metadata": {
        "id": "sX_7znJc170w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgMNmj4K6L4V",
        "outputId": "fb572646-5750-4a83-f37d-df4c91cdd4d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class myDataset(Dataset):\n",
        "  padding_id = 0\n",
        "  def __init__(self,mode):\n",
        "    if mode == \"train\":\n",
        "      self.data_path = \"/content/train_dev/tokenized_train_data.txt\"\n",
        "      self.label_path = \"/content/train_dev/tokenized_train_label.txt\"\n",
        "    elif mode == \"val\":\n",
        "      self.data_path = \"/content/train_dev/tokenized_valid_data.txt\"\n",
        "      self.label_path = \"/content/train_dev/tokenized_valid_label.txt\"\n",
        "\n",
        "    data = []\n",
        "    with open(self.data_path,\"r\") as f :\n",
        "      d_l = f.readlines()\n",
        "      for line in tqdm(d_l):\n",
        "        int_list = [int(i) for i in line.split()]\n",
        "        data.append(int_list)\n",
        "    self.data = torch.LongTensor(data)\n",
        "\n",
        "    label = []\n",
        "    with open(self.label_path,\"r\") as f :\n",
        "      l_l = f.readlines()\n",
        "      for line in tqdm(l_l):\n",
        "        int_list = [int(i) for i in line.split()]\n",
        "        label.append(int_list)\n",
        "    self.label = torch.LongTensor(np.array(label))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.data, self.label\n",
        "\n",
        "  @classmethod\n",
        "  def padding_mask_batch(cls,batch):\n",
        "    \"\"\"Collate a batch of data.\"\"\"\n",
        "    src, tgt = zip(*batch)\n",
        "\n",
        "    src_padding = (src == torch.full(src.size(),cls.padding_id))\n",
        "    tgt_padding = (tgt == torch.full(tgt.size(),cls.padding_id))\n",
        "\n",
        "    return src, tgt , src_padding, tgt_padding\n"
      ],
      "metadata": {
        "id": "dbfi-DvrlDhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "with open(\"/content/train_dev/tokenized_train_data.txt\",\"r\") as f :\n",
        "  d_l = f.readlines()\n",
        "  for line in tqdm(d_l):\n",
        "    int_list = [int(i) for i in line.split()]\n",
        "    data.append(int_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyjGbovh-zzX",
        "outputId": "9709f267-34ba-4a11-f064-f6064156a666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 349856/349856 [00:40<00:00, 8713.39it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data),len(data[0]),type(data[0][0]))\n",
        "data_np = np.array(data)\n",
        "print(data_np.shape)\n",
        "data = torch.LongTensor(data)\n",
        "print(data.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVuAbRgj8ulY",
        "outputId": "9e1bee6d-7029-4864-cd52-31da196e951e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "349856 402 <class 'int'>\n",
            "(349856, 402)\n",
            "torch.Size([349856, 402])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = myDataset(\"train\",padding_id = 0)\n",
        "valid_set = myDataset(\"val\",padding_id = 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBFKMSM45ppM",
        "outputId": "fd9c1834-eb49-4b1e-8bbe-26f6ba25aacf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 349856/349856 [00:33<00:00, 10415.55it/s]\n",
            "100%|██████████| 349856/349856 [00:13<00:00, 25089.58it/s]\n",
            "100%|██████████| 38872/38872 [00:03<00:00, 11511.21it/s]\n",
            "100%|██████████| 38872/38872 [00:01<00:00, 23368.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = torch.randint(0,3,(3,4,5))\n",
        "padding = torch.full(T.size(),3)\n",
        "(T == padding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRZBv02v7A1I",
        "outputId": "9e02742c-46e5-4d2d-e41f-c3110b19cdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False]],\n",
              "\n",
              "        [[False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False]],\n",
              "\n",
              "        [[False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False]]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 400\n",
        "\n",
        "train_loader = DataLoader(\n",
        "  train_set,\n",
        "  batch_size=batch_size,\n",
        "  shuffle=True,\n",
        "  num_workers=8,\n",
        "  pin_memory=True,\n",
        "  collate_fn=myDataset.padding_mask_batch\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "  valid_set,\n",
        "  batch_size=batch_size,\n",
        "  num_workers=8,\n",
        "  pin_memory=True,\n",
        "  collate_fn=myDataset.padding_mask_batch\n",
        ")\n"
      ],
      "metadata": {
        "id": "L5SsVH3Q6-Ie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bdbdc4d-018f-45b5-c2e6-d17e86e643f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFy1YrdJQgEe",
        "outputId": "614518fd-9b05-4f56-bd48-7cfb7391396e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchinfo import summary\n",
        "def attn_mask(input_dim):\n",
        "      return nn.Transformer.generate_square_subsequent_mask(input_dim)\n",
        "#attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)"
      ],
      "metadata": {
        "id": "EoKH9m1LznWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Positional_Encoding(nn.Module):\n",
        "    def __init__(self,max_sentence_length,embedding_dimension):\n",
        "      super().__init__()\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.encoding_values = nn.Parameter(nn.init.normal_(torch.empty(max_sentence_length,1, embedding_dimension)))\n",
        "    def forward(self, x):\n",
        "        # the shape of x : [batch,length,e_dim]\n",
        "        # the shape of self.encoding_values : [batch,length,e_dim]\n",
        "        x = x + self.encoding_values.unsqueeze(0)\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "CEoOzUwQCyg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConformerLayer(nn.Module):\n",
        "  \"\"\"\n",
        "    #------Conformer Encoder Part----------------------------------\n",
        "    # part 1 feed forward module\n",
        "    # part 2 multi heads self attention\n",
        "    # part 3 convolution module\n",
        "    # part 4 feed forward module\n",
        "    # part 5 layer norm\n",
        "    # multi heads self attention module\n",
        "    #   part 2-1 laryer norm\n",
        "    #   part 2-2 multi heads self attention\n",
        "    #   part 2-3 dropout\n",
        "    # convolution module\n",
        "    #   part 3-1 layer norm (do this outside the module)\n",
        "    #   part 3-2 pointwise conv\n",
        "    #   part 3-3 Glu activation\n",
        "    #   part 3-4 1D depthwise conv\n",
        "    #   part 3-5 batch norm\n",
        "    #   part 3-6 swish activation\n",
        "    #   part 3-7 pointwise conv\n",
        "    #   part 3-8 dropout\n",
        "    \"\"\"\n",
        "  def __init__(self,d_model,feedforword,dropout):\n",
        "    super().__init__()\n",
        "    self.feed_forward_layer_1 = nn.Sequential(\n",
        " #   nn.BatchNorm1d(d_model),\n",
        "    nn.Linear(d_model, feedforword),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(feedforword, d_model),\n",
        " #   nn.BatchNorm1d(d_model),\n",
        "    )\n",
        "    self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "    self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "    self.attention = torch.nn.MultiheadAttention(d_model, num_heads = 2, dropout=dropout)\n",
        "    self.DropoutMA = nn.Dropout(dropout)\n",
        "    self.cnn_module = nn.Sequential(\n",
        "      # part 3-2\n",
        "      nn.Conv1d(in_channels = d_model , out_channels = d_model*2 , kernel_size = 1),\n",
        "      # part 3-3\n",
        "      nn.GLU(dim = 1),\n",
        "      # part 3-4\n",
        "      nn.Conv1d(in_channels = d_model , out_channels = d_model , kernel_size = 3 ,padding = 1, groups = d_model),\n",
        "      # part 3-5\n",
        "      nn.BatchNorm1d(d_model),\n",
        "      # part 3-6\n",
        "      nn.SiLU(),\n",
        "      # part 3-7\n",
        "      nn.Conv1d(in_channels = d_model , out_channels = d_model , kernel_size = 1),\n",
        "      # part 3-8\n",
        "      nn.Dropout1d()\n",
        "    )\n",
        "    self.feed_forward_layer_2 = nn.Sequential(\n",
        " #   nn.BatchNorm1d(d_model),\n",
        "    nn.Linear(d_model, feedforword),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(feedforword, d_model),\n",
        " #   nn.BatchNorm1d(d_model),\n",
        "    )\n",
        "    self.layer_norm3 = nn.LayerNorm(d_model)\n",
        "  def forward(self ,x ):\n",
        "    # part 1\n",
        "    x = x + 0.5 * self.feed_forward_layer_1(x)\n",
        "    # part 2\n",
        "    x = x.permute(1, 0, 2)\n",
        "    x = self.layer_norm1(x)\n",
        "    x = x + self.attention(x,x,x, need_weights=False)[0]\n",
        "    x = self.DropoutMA(x)\n",
        "    # part 3\n",
        "    x = self.layer_norm2(x)\n",
        "    x = x.permute(1, 2, 0)\n",
        "    x = x + self.cnn_module(x)\n",
        "    x = x.permute(0, 2, 1)\n",
        "\n",
        "    # part 4\n",
        "    x = x + 0.5 * self.feed_forward_layer_2(x)\n",
        "    # part 5\n",
        "    x = self.layer_norm3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "5f1kh8okdGYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Apply_Padding_Mask(nn.Module):\n",
        "#   def __init__(self,data,mask_in_booling = None):\n",
        "#     super().__init__()\n",
        "#     torch.full\n",
        "#     self.mask = mask_in_booling\n",
        "#   def forward(self , x):\n",
        "#     if self.mask is not None:\n",
        "#       mask = (torch.zeros_like(self.mask,dtype = float)\\\n",
        "#           .masked_fill_(self.mask, float(\"-inf\")))\n",
        "#       return data+mask\n",
        "#     else:\n",
        "#       return data"
      ],
      "metadata": {
        "id": "eSpXjK6mGP1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.tensor([True,False,True,False])\n",
        "mask = (\n",
        "    torch.zeros_like(mask,dtype = float)\n",
        "    .masked_fill_(mask, float(\"-inf\"))\n",
        ")\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDWXNUBNNAXc",
        "outputId": "4b05c337-f4b7-4a59-ee12-0cd13d2cea17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-inf, 0., -inf, 0.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from logging import raiseExceptions\n",
        "from torch.nn.functional import scaled_dot_product_attention\n",
        "class My_MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, embedding_dimension, num_heads, dropout=0.1):\n",
        "        '''\n",
        "        embedding_dimension = input dimension\n",
        "        note that there are residual sublayers in MultiHeadedAttention\n",
        "        '''\n",
        "        super().__init__()\n",
        "        assert embedding_dimension % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.d = embedding_dimension\n",
        "        self.num_heads = num_heads\n",
        "        self.linear_for_qkv = nn.Linear(embedding_dimension, 3 * embedding_dimension)\n",
        "        self.linear_out_project = nn.Linear(embedding_dimension, embedding_dimension)\n",
        "    def forward(self, input_data , data_padding_mask):\n",
        "        x = self.linear_for_qkv(input_data)\n",
        "        # print(x.size())\n",
        "        query, key, value = x.split(self.d,dim = -1)\n",
        "        # print(query.size())\n",
        "        # print(query)\n",
        "        # query = query.view(x.size(0),x.size(1),self.num_heads,self.d//self.num_heads)\n",
        "        query,key,value = \\\n",
        "          map(lambda x : x.view(x.size(0),x.size(1),self.num_heads,self.d//self.num_heads),[query,key,value])\n",
        "\n",
        "        query,key,value = \\\n",
        "          map(lambda x : x.transpose(-2,-3),[query,key,value])\n",
        "\n",
        "        # print(query.size())\n",
        "        # print(query)\n",
        "        x = scaled_dot_product_attention(query,key,value,attn_mask = data_padding_mask, dropout_p =0)\n",
        "        print(x.size())\n",
        "        x = x.transpose(-2,-3)\n",
        "        x = x.view(x.size(0),x.size(1),self.d)\n",
        "        x = self.linear_out_project(x)\n",
        "        return x\n",
        "        # attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
        "        # attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
        "        # attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))"
      ],
      "metadata": {
        "id": "ufwt0X68ifSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = My_MultiHeadedAttention(6,2)\n",
        "# input = torch.rand(2,4,6)\n",
        "# print(input)\n",
        "# print(model(input))"
      ],
      "metadata": {
        "id": "jo9R1S8CynWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class My_Encoder_Layer(nn.Module):\n",
        "  def __init__(self,max_sentence_length,dictionary_length,embedding_dimension,feedforward_dimension):\n",
        "    super().__init__()\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    # self.apply_encoder_padding_mask = Apply_Padding_Mask()\n",
        "    self.encoder_embedding = nn.Embedding(dictionary_length,self.emb_dim,padding_idx=0)* math.sqrt(self.emb_dim)\n",
        "    self.positional_encoding = Positional_Encoding(max_sentence_length,self.emb_dim)\n",
        "\n",
        "    self.attention = My_MultiHeadedAttention(self.emb_dim, num_heads = 2, dropout=0)\n",
        "    self.layer_norm_attn = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_attn_layernorm = nn.Dropout(0)\n",
        "\n",
        "    self.feedforward = nn.Sequential({\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    })\n",
        "    self.layer_norm_feedforward = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_feedforward_layernorm = nn.Dropout(0)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    # x = self.apply_encoder_padding_mask(x)\n",
        "    x = self.encoder_embedding(x)\n",
        "    x = self.positional_encoding(x)\n",
        "    # print(x)\n",
        "    x = x + self.attention(x)\n",
        "    x = self.layer_norm_attn(x)\n",
        "    x = self.drop_out_attn_layernorm(x)\n",
        "\n",
        "    x = x + self.feedforward(x)\n",
        "    x = self.layer_norm_feedforward(x)\n",
        "    x = self.drop_out_feedforward_layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "PYhd7muASnrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = My_Encoder_Layer(400,8000,128)\n",
        "input = (torch.randint(0,7999,(32,400,1),dtype = torch.long),torch.randint(0,7999,(32,400,1),dtype = torch.long),myDataset.padding_mask_batch(),myDataset.padding_mask_batch())\n",
        "#print(summary(model,(32,400,128)))\n",
        "print(summary(model,input_data = input))\n",
        "print(model.state_dict().keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "Vv_Y1voAPzHB",
        "outputId": "835beb04-165e-424e-b376-94f4af07cb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-79d0c6d05ecc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_encoder_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_mask_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_mask_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(summary(model,(32,400,128)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: my_encoder_layer.__init__() missing 1 required positional argument: 'feedforward_dimension'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class My_Encoder(nn.Module):\n",
        "  def __init__(self,max_sentence_length,dictionary_length,embedding_dimension,feedforward_dimension,layer_num = 2):\n",
        "    super().__init__()\n",
        "    self.encoder = nn.ModuleList([My_Encoder_Layer(max_sentence_length,dictionary_length,\\\n",
        "                    embedding_dimension,feedforward_dimension) for i in range(layer_num)])\n",
        "  def forward(self,x):\n",
        "    x = self.encoder(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "6YT3B7Lv72la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class My_Decoder_Layer(nn.Module):\n",
        "  def __init__(self,max_sentence_length,dictionary_length,embedding_dimension,feedforward_dimension):\n",
        "    super().__init__()\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.apply_decoder_padding_mask = Apply_Padding_Mask()\n",
        "    self.encoder_embedding = nn.Embedding(dictionary_length,self.emb_dim,padding_idx=0)* math.sqrt(self.emb_dim)\n",
        "    self.positional_encoding = Positional_Encoding(max_sentence_length,self.emb_dim)\n",
        "\n",
        "    self.attention = My_MultiHeadedAttention(self.emb_dim, num_heads = 2, dropout=0)\n",
        "    self.layer_norm_attn = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_attn_layernorm = nn.Dropout(0)\n",
        "\n",
        "    self.feedforward = nn.Sequential({\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    })\n",
        "    self.layer_norm_feedforward = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_feedforward_layernorm = nn.Dropout(0)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.apply_encoder_padding_mask(x)\n",
        "    x = self.encoder_embedding(x)\n",
        "    x = self.positional_encoding(x)\n",
        "    # print(x)\n",
        "    x = x + self.attention(x)\n",
        "    x = self.layer_norm_attn(x)\n",
        "    x = self.drop_out_attn_layernorm(x)\n",
        "\n",
        "    x = x + self.feedforward(x)\n",
        "    x = self.layer_norm_feedforward(x)\n",
        "    x = self.drop_out_feedforward_layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "rEjaTbhyBmEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchinfo import summary\n",
        "# model = nn.TransformerDecoderLayer(d_model=40, nhead=1)\n",
        "# print(summary(model,((32,128,40),(32,128,40)),tgt_mask = attn_mask(32),tgt_key_padding_mask = padding_mask))"
      ],
      "metadata": {
        "id": "huMWrKUSNZAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"\"\"Implement label smoothing.\"\"\"\n",
        "\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "\n",
        "def clones(module, N):\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        # 复制N个encoder layer\n",
        "        self.layers = clones(layer, N)\n",
        "        # Layer Norm\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        使用循环连续decode N次(这里为6次)\n",
        "        这里的Decoderlayer会接收一个对于输入的attention mask处理\n",
        "        和一个对输出的attention mask + subsequent mask处理\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        # Self-Attention\n",
        "        self.self_attn = self_attn\n",
        "        # 与Encoder传入的Context进行Attention\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        # 用m来存放encoder的最终hidden表示结果\n",
        "        m = memory\n",
        "\n",
        "        # Self-Attention：注意self-attention的q，k和v均为decoder hidden\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        # Context-Attention：注意context-attention的q为decoder hidden，而k和v为encoder hidden\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        # encoder的结果作为decoder的memory参数传入，进行decode\n",
        "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    # vocab: tgt_vocab\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        # decode后的结果，先进入一个全连接层变为词典大小的向量\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 然后再进行log_softmax操作(在softmax结果上再做多一次log运算)\n",
        "        return F.log_softmax(self.proj(x), dim=-1)\n",
        "\n",
        "\n",
        "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    c = copy.deepcopy\n",
        "    # 实例化Attention对象\n",
        "    attn = MultiHeadedAttention(h, d_model).to(DEVICE)\n",
        "    # 实例化FeedForward对象\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout).to(DEVICE)\n",
        "    # 实例化PositionalEncoding对象\n",
        "    position = PositionalEncoding(d_model, dropout).to(DEVICE)\n",
        "    # 实例化Transformer模型对象\n",
        "    model = Transformer(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout).to(DEVICE), N).to(DEVICE),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout).to(DEVICE), N).to(DEVICE),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab).to(DEVICE), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab).to(DEVICE), c(position)),\n",
        "        Generator(d_model, tgt_vocab)).to(DEVICE)\n",
        "\n",
        "    # This was important from their code.\n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            # 这里初始化采用的是nn.init.xavier_uniform\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "\n",
        "def batch_greedy_decode(model, src, src_mask, max_len=64, start_symbol=2, end_symbol=3):\n",
        "    batch_size, src_seq_len = src.size()\n",
        "    results = [[] for _ in range(batch_size)]\n",
        "    stop_flag = [False for _ in range(batch_size)]\n",
        "    count = 0\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    tgt = torch.Tensor(batch_size, 1).fill_(start_symbol).type_as(src.data)\n",
        "\n",
        "    for s in range(max_len):\n",
        "        tgt_mask = subsequent_mask(tgt.size(1)).expand(batch_size, -1, -1).type_as(src.data)\n",
        "        out = model.decode(memory, src_mask, Variable(tgt), Variable(tgt_mask))\n",
        "\n",
        "        prob = model.generator(out[:, -1, :])\n",
        "        pred = torch.argmax(prob, dim=-1)\n",
        "\n",
        "        tgt = torch.cat((tgt, pred.unsqueeze(1)), dim=1)\n",
        "        pred = pred.cpu().numpy()\n",
        "        for i in range(batch_size):\n",
        "            # print(stop_flag[i])\n",
        "            if stop_flag[i] is False:\n",
        "                if pred[i] == end_symbol:\n",
        "                    count += 1\n",
        "                    stop_flag[i] = True\n",
        "                else:\n",
        "                    results[i].append(pred[i].item())\n",
        "            if count == batch_size:\n",
        "                break\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def greedy_decode(model, src, src_mask, max_len=64, start_symbol=2, end_symbol=3):\n",
        "    \"\"\"传入一个训练好的模型，对指定数据进行预测\"\"\"\n",
        "    # 先用encoder进行encode\n",
        "    memory = model.encode(src, src_mask)\n",
        "    # 初始化预测内容为1×1的tensor，填入开始符('BOS')的id，并将type设置为输入数据类型(LongTensor)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    # 遍历输出的长度下标\n",
        "    for i in range(max_len - 1):\n",
        "        # decode得到隐层表示\n",
        "        out = model.decode(memory,\n",
        "                           src_mask,\n",
        "                           Variable(ys),\n",
        "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
        "        # 将隐藏表示转为对词典各词的log_softmax概率分布表示\n",
        "        prob = model.generator(out[:, -1])\n",
        "        # 获取当前位置最大概率的预测词id\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.data[0]\n",
        "        if next_word == end_symbol:\n",
        "            break\n",
        "        # 将当前位置预测的字符id与之前的预测内容拼接起来\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return ys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYGJ9mR5GGhC",
        "outputId": "1c820b05-b7f0-43de-9f99-2cbf14f06f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134023434658160 134023434658160\n",
            "<function StaticMethod.set_string at 0x79e4c336e170> <function StaticMethod.set_string at 0x79e4c336e170>\n",
            "b b\n"
          ]
        }
      ]
    }
  ]
}