{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXjXZx9IsvrFxjJ8BV9dx8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW05_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "part 1 load file (finish)\n",
        "part 2 clean data (finish on 12/5)\n",
        "part 3 tokenize : sentencepiece (finish on 12/6)\n",
        "part 4 build model encoder decoder (finish on 12/7)\n",
        "part 5 beam search (finish on 12/8)\n",
        "part 6 label smooth (finish on 12/9)\n",
        "\n",
        "part 7 test (finish on 12/10)\n"
      ],
      "metadata": {
        "id": "WTv4XN2qB_fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "# !pip install --upgrade jupyter ipywidgets\n",
        "# !git clone https://github.com/pytorch/fairseq.git\n",
        "# !cd fairseq && git checkout 9a1c497\n",
        "# !pip install --upgrade ./fairseq/"
      ],
      "metadata": {
        "id": "hwhMkDjdvARr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# import pdb\n",
        "# import pprint\n",
        "# import logging\n",
        "# import os\n",
        "# import random\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.utils import data\n",
        "# import numpy as np\n",
        "# import tqdm.auto as tqdm\n",
        "# from pathlib import Path\n",
        "# from argparse import Namespace\n",
        "# #from fairseq import utils\n",
        "\n",
        "# import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FgIV_VBavJ2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVHTBvhfZyZY"
      },
      "outputs": [],
      "source": [
        "# seed = 1\n",
        "# random.seed(seed)\n",
        "# torch.manual_seed(seed)\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.manual_seed(seed)\n",
        "#     torch.cuda.manual_seed_all(seed)\n",
        "# np.random.seed(seed)\n",
        "# torch.backends.cudnn.benchmark = False\n",
        "# torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "shutil.copyfile('/content/drive/MyDrive/ted2020.tgz','/content/ted2020.tgz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "X23RaTWe9hoU",
        "outputId": "f4ea4b59-cc4c-436d-8b8e-58056d08dab7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/ted2020.tgz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "rawdata_file = \"/content/ted2020.tgz\"\n",
        "unzip_path = \"/content/train_dev/\"\n",
        "# open file\n",
        "file = tarfile.open(rawdata_file)\n",
        "\n",
        "# extracting file\n",
        "file.extractall(unzip_path)\n",
        "\n",
        "file.close()\n"
      ],
      "metadata": {
        "id": "2GhVMRgifeKi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_lang = \"raw.en\"\n",
        "tgt_lang = \"raw.zh\"\n",
        "\n",
        "src_path = f\"{unzip_path}{src_lang}\"\n",
        "tgt_path = f\"{unzip_path}{tgt_lang}\"\n",
        "print(tgt_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH2bCPFbvhK2",
        "outputId": "5ac96229-b105-426a-d8a5-53185652e00e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/train_dev/raw.zh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for path in [src_path,tgt_path]:\n",
        "  with open(path, \"r\") as f:\n",
        "  #  data = f.readlines()\n",
        "  #  print(type(data),data[0:5])#<class 'list'> ['Thank you so much, Chris.\\n', \"And it'......on.\\n']\n",
        "    data = f.read().splitlines()\n",
        "    print(type(data),data[0:5],len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjIUpASA076k",
        "outputId": "873f70f0-905a-4e85-fd7f-33191eab396e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> ['Thank you so much, Chris.', \"And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\", 'I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.', 'And I say that sincerely, partly because  I need that.', 'Put yourselves in my position.'] 394066\n",
            "<class 'list'> ['非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台', '真是一大榮幸。我非常感激。', '這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。', '我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!', '請你們設身處地為我想一想！'] 394066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "print(ord(\"，\"))\n",
        "print(chr(65292))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhEDa7SN6us_",
        "outputId": "0b69e44b-af47-4771-e0d8-0eadfd4a72b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65292\n",
            "，\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "def to_halfwidth(string):\n",
        "  return \"\".join(unicodedata.normalize('NFKC',letter) for letter in string)\n",
        "print(to_halfwidth(\"ＡＳＤＦＦＦＦＱ\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFgPn-1i1EZR",
        "outputId": "01c62286-4476-4e68-ec2c-1c8ef39a14bc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASDFFFFQ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "def clean_s_zh(s):\n",
        "    s = to_halfwidth(s)\n",
        "    # step 1 : delete — _\n",
        "    delete = \" _()[]\"\n",
        "    delete_rules = s.maketrans(\"\",\"\",delete)\n",
        "    s = s.translate(delete_rules)\n",
        "\n",
        "    # step 2 : replace “” with \"\"\n",
        "    to_be_replace = '“”'\n",
        "    replace = '\"\"'\n",
        "    replace_dict = dict(zip(to_be_replace,replace))\n",
        "    # print(replace_dict.items()) : dict_items([('“', '\"'), ('”', '\"')])\n",
        "\n",
        "    # step 3 : add **END** before and after punctuation\n",
        "\n",
        "    \"\"\"\n",
        "    The number of sentences in one line may be different\n",
        "    in line pairs of source and target set.\n",
        "    so I try to use \"。!?\" or \".!?\" to split sentences.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "#    punctuation = \"。,;!?()\\\"~「」\"\n",
        "    punctuation = \"。!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    zh_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return zh_list\n",
        "\n",
        "def clean_s_en(s):\n",
        "    s = to_halfwidth(s)\n",
        "\n",
        "    replace_dict = {}\n",
        "\n",
        "    delete = \"-()[]\"\n",
        "    for char in delete:\n",
        "      replace_dict[char] = \"\"\n",
        "\n",
        "    punctuation = \"!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "    s = pattern.sub(\"**END**\",s)\n",
        "\n",
        "    en_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return en_list"
      ],
      "metadata": {
        "id": "5dwWeVz-sPI2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "result = pattern.sub(\"**END**\",\"There are many people in U.S. w.r.t. in Taiwan.Thank you.\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt8FnBUBrSZb",
        "outputId": "d13c416c-53e1-44c0-92fe-a4779276fbc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are many people in U.S. w.r.t. in Taiwan**END**Thank you**END**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_file(path,fuction):\n",
        "  with open(path, \"r\") as f:\n",
        "    data = f.read()\n",
        "    return(fuction(data))\n",
        "src_list = load_file(src_path,clean_s_en)\n",
        "tgt_list = load_file(tgt_path,clean_s_zh)"
      ],
      "metadata": {
        "id": "yeSGu6mwmbzh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(tgt_path, \"r\") as f:\n",
        "    data = f.readlines()\n",
        "    print(data[17])\n",
        "print(tgt_list[17])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXOFN_BQN7ru",
        "outputId": "854aa5ac-1328-4a97-ecf9-c1cea5387f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我們下了交流道後就開始找餐廳 — 我們發現了一家 Shoney' s餐館。\n",
            "\n",
            "我們下了交流道後就開始找餐廳—我們發現了一家Shoney's餐館。**END**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # \"\"\"\n",
        "    # Please ensure that space char \" \" has bee remove in previous step ,\n",
        "    # it's the reason s must be translated twice.\n",
        "\n",
        "    # \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    # print(len(replace_dict.items())) : 13\n",
        "#    punctuation = \"(.,;!?()\\\"])\"\n"
      ],
      "metadata": {
        "id": "YH2tx-ShwhAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_by_END(s):\n",
        "    return(s.strip(\"**END**\").split(\"**END**\"))\n",
        "\n",
        "def devide_en_again(s,punctuation = \":;\"):\n",
        "    replace_dict = {}\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules_src = s.maketrans(replace_dict)\n",
        "    new_s = divide_by_END(s.translate(replace_rules_src))\n",
        "    return new_s"
      ],
      "metadata": {
        "id": "0RRwmoBw59gS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_data_pairs(src_list,tgt_list):\n",
        "    index = 0\n",
        "    new_src_list = []\n",
        "    new_tgt_list = []\n",
        "\n",
        "    same = 0\n",
        "    add_next = 0\n",
        "    split_again = 0\n",
        "    not_use = 0\n",
        "\n",
        "    while(index < len(src_list)):\n",
        "\n",
        "      src = divide_by_END(src_list[index])\n",
        "      tgt = divide_by_END(tgt_list[index])\n",
        "      # case 1 : src is as long as tgt , finished.\n",
        "      if len(src) == len(tgt):\n",
        "        new_src_list += src\n",
        "        new_tgt_list += tgt\n",
        "        same += 1\n",
        "        index += 1\n",
        "\n",
        "      else :\n",
        "        # if it is not the last one : both src and tgt add next sentence\n",
        "        if index != len(src_list)-1:\n",
        "          src_add_next = divide_by_END(src_list[index] + src_list[index+1])\n",
        "          tgt_add_next = divide_by_END(tgt_list[index] + tgt_list[index+1])\n",
        "          # case 2 : src_add_next is as long as tgt_add_next , finished.\n",
        "          if len(src_add_next) == len(tgt_add_next):\n",
        "            new_src_list += src_add_next\n",
        "            new_tgt_list += tgt_add_next\n",
        "            add_next += 2\n",
        "            index += 2\n",
        "\n",
        "          # using new punctuation to divide tgt (english) sentence.\n",
        "          else :\n",
        "            src_add_next = devide_en_again(src_list[index] + src_list[index+1])\n",
        "            # case 3 : src_add_next is as long as tgt_add_next , finished.\n",
        "            if len(src_add_next) == len(tgt_add_next):\n",
        "              new_src_list += src_add_next\n",
        "              new_tgt_list += tgt_add_next\n",
        "              split_again +=2\n",
        "              index += 2\n",
        "\n",
        "            # case 4 : sentence will not be used.\n",
        "            else :\n",
        "              not_use += 1\n",
        "              # if to_do == 1 :\n",
        "              #   print(index,src_add_next,tgt_add_next,len(src_add_next),len(tgt_add_next))\n",
        "              index += 1\n",
        "\n",
        "        # if it is the last one\n",
        "        else :\n",
        "          not_use += 1\n",
        "          index += 1\n",
        "\n",
        "    print(index,same,add_next,split_again,not_use)\n",
        "\n",
        "    return(new_src_list,new_tgt_list)\n",
        "dataset = check_data_pairs(src_list,tgt_list)\n",
        "print(dataset[0][0],dataset[1][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVYvYCpuDQDa",
        "outputId": "087ffb35-a14b-4c4c-b0d7-ee7a53f22c4d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "394066 350481 12188 2972 28425\n",
            "Thank you so much, Chris 非常謝謝你,克里斯。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/train_dev/data.txt\"\n",
        "label_path = \"/content/train_dev/label.txt\"\n",
        "with open(data_path, \"w\") as f:\n",
        "  f.write(\"\\n\".join(dataset[0]))\n",
        "with open(label_path, \"w\") as f:\n",
        "  f.write(\"\\n\".join(dataset[1]))"
      ],
      "metadata": {
        "id": "DNf0E5FDPsj8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_path, \"r\") as f:\n",
        "    data = f.readlines()\n",
        "    print(data[17])\n",
        "with open(label_path, \"r\") as f:\n",
        "    data = f.readlines()\n",
        "    print(data[17])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_68jlIE2RCzq",
        "outputId": "0c4fe7e4-8594-4ea0-b35e-3d371c80a8e4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We got off the exit, we found a Shoney's restaurant\n",
            "\n",
            "我們下了交流道後就開始找餐廳—我們發現了一家Shoney's餐館。\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ti-UGuHNhh",
        "outputId": "abcadac6-e05c-4790-9cbe-62ff720debeb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "src_lang = \"en\"\n",
        "tgt_lang = \"zh\"\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=label_path,\n",
        "    model_prefix=f'spm{vocab_size}',\n",
        "    vocab_size=vocab_size,\n",
        "    character_coverage=1,\n",
        "    model_type='unigram', # 'bpe' works as well\n",
        "    input_sentence_size=1e6,\n",
        "    shuffle_input_sentence=True,\n",
        "    normalization_rule_name='nmt_nfkc_cf',\n",
        ")"
      ],
      "metadata": {
        "id": "LoNi7CeA3P9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zZADzpO-HOLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> import sentencepiece as spm\n",
        ">>> s = spm.SentencePieceProcessor(model_file='spm.model')\n",
        ">>> for n in range(5):\n",
        "...     s.encode('New York', out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "sX_7znJc170w",
        "outputId": "8a382f03-19a3-42de-bf7e-73a1ea994444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a130a4d28667>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentencepiece\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spm.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'New York'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_sampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mInit\u001b[0;34m(self, model_file, model_proto, out_type, add_bos, add_eos, reverse, emit_unk_piece, enable_sampling, nbest_size, alpha, num_threads)\u001b[0m\n\u001b[1;32m    445\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    903\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_EncodeAsIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_sampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_bos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_eos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memit_unk_piece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Not found: \"spm.model\": No such file or directory Error #2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spm_model = spm.SentencePieceProcessor(model_file=f'spm{vocab_size}')\n",
        "in_tag = {\n",
        "    'train': 'train.clean',\n",
        "    'valid': 'valid.clean',\n",
        "    'test': 'test.raw.clean',\n",
        "}\n",
        "\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        out_path = prefix/f'{split}.{lang}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)"
      ],
      "metadata": {
        "id": "txsW_v8vHvoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
        "# !head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5\n",
        "# binpath = Path('./DATA/data-bin', dataset_name)\n",
        "# if binpath.exists():\n",
        "#     print(binpath, \"exists, will not overwrite!\")\n",
        "# else:\n",
        "#     !python -m fairseq_cli.preprocess \\\n",
        "#         --source-lang {src_lang}\\\n",
        "#         --target-lang {tgt_lang}\\\n",
        "#         --trainpref {prefix/'train'}\\\n",
        "#         --validpref {prefix/'valid'}\\\n",
        "#         --testpref {prefix/'test'}\\\n",
        "#         --destdir {binpath}\\\n",
        "#         --joined-dictionary\\\n",
        "#         --workers 2\n"
      ],
      "metadata": {
        "id": "xIWAdyf6h0bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "#     batch_iterator = task.get_batch_iterator(\n",
        "#         dataset=task.dataset(split),\n",
        "#         max_tokens=max_tokens,\n",
        "#         max_sentences=None,\n",
        "#         max_positions=utils.resolve_max_positions(\n",
        "#             task.max_positions(),\n",
        "#             max_tokens,\n",
        "#         ),\n",
        "#         ignore_invalid_inputs=True,\n",
        "#         seed=seed,\n",
        "#         num_workers=num_workers,\n",
        "#         epoch=epoch,\n",
        "#         disable_iterator_cache=not cached,\n",
        "#         # Set this to False to speed up. However, if set to False, changing max_tokens beyond\n",
        "#         # first call of this method has no effect.\n",
        "#     )\n",
        "#     return batch_iterator\n",
        "\n",
        "# demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "# demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "# sample = next(demo_iter)\n",
        "# sample"
      ],
      "metadata": {
        "id": "dbfi-DvrlDhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch = {\n",
        "#   \"id\": id, # 每個 example 的 id\n",
        "#   \"nsentences\": len(samples), # batch size 句子數\n",
        "#   \"ntokens\": ntokens, # batch size 字數\n",
        "#   \"net_input\": {\n",
        "#       \"src_tokens\": src_tokens, # 來源語言的序列\n",
        "#       \"src_lengths\": src_lengths, # 每句話沒有 pad 過的長度\n",
        "#       \"prev_output_tokens\": prev_output_tokens, # 上面提到右 shift 一格後的目標序列\n",
        "#   },\n",
        "#   \"target\": target, # 目標序列\n",
        "# }"
      ],
      "metadata": {
        "id": "0VgNDQMVlGLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cuda_env = utils.CudaEnvironment()\n",
        "# utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "cYgg7zwHk1jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # HINT: transformer 架構\n",
        "# # from fairseq.models.transformer import (\n",
        "# #     TransformerEncoder,\n",
        "# #     TransformerDecoder,\n",
        "# # )\n",
        "\n",
        "# def build_model(args, task):\n",
        "#     \"\"\" 按照參數設定建置模型 \"\"\"\n",
        "#     src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "#     # 詞嵌入\n",
        "#     encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "#     decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "\n",
        "#     # 編碼器與解碼器\n",
        "#     # TODO: 替換成 TransformerEncoder 和 TransformerDecoder\n",
        "#     encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
        "#     decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "#     # 序列到序列模型\n",
        "#     model = Seq2Seq(args, encoder, decoder)\n",
        "\n",
        "#     # 序列到序列模型的初始化很重要 需要特別處理\n",
        "#     def init_params(module):\n",
        "#         from fairseq.modules import MultiheadAttention\n",
        "#         if isinstance(module, nn.Linear):\n",
        "#             module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "#         if isinstance(module, nn.Embedding):\n",
        "#             module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             if module.padding_idx is not None:\n",
        "#                 module.weight.data[module.padding_idx].zero_()\n",
        "#         if isinstance(module, MultiheadAttention):\n",
        "#             module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#         if isinstance(module, nn.RNNBase):\n",
        "#             for name, param in module.named_parameters():\n",
        "#                 if \"weight\" in name or \"bias\" in name:\n",
        "#                     param.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "#     # 初始化模型\n",
        "#     model.apply(init_params)\n",
        "#     return model"
      ],
      "metadata": {
        "id": "-Zy7vAVXlTAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "#     def __init__(self, args, encoder, decoder):\n",
        "#         super().__init__(encoder, decoder)\n",
        "#         self.args = args\n",
        "\n",
        "#     def forward(\n",
        "#         self,\n",
        "#         src_tokens,\n",
        "#         src_lengths,\n",
        "#         prev_output_tokens,\n",
        "#         return_all_hiddens: bool = True,\n",
        "#     ):\n",
        "#         \"\"\"\n",
        "#         Run the forward pass for an encoder-decoder model.\n",
        "#         \"\"\"\n",
        "#         encoder_out = self.encoder(\n",
        "#             src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "#         )\n",
        "#         logits, extra = self.decoder(\n",
        "#             prev_output_tokens,\n",
        "#             encoder_out=encoder_out,\n",
        "#             src_lengths=src_lengths,\n",
        "#             return_all_hiddens=return_all_hiddens,\n",
        "#         )\n",
        "#         return logits, extra"
      ],
      "metadata": {
        "id": "hBNhGpgS08gI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}