{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQdj4ht9+vCZL2LZUBwVle",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW05_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "###Part 0 setting and installing package\n",
        "------\n",
        "###Part 1 preparing data set\n",
        "------\n",
        "######load data file\n",
        "######clean data\n",
        "######pick up line pairs\n",
        "######tokenize : using sentencepiece\n",
        "######make data set\n",
        "------\n",
        "###Part 2 make model\n",
        "------\n",
        "######positional encoding layer\n",
        "######multihead attention layer\n",
        "######encoder layer(s)\n",
        "######decoder layer(s)\n",
        "######transformer layer\n",
        "------\n",
        "###Part 3 training and validation\n",
        "------\n",
        "######label smoothing\n",
        "######beam search\n",
        "######bleu\n",
        "------\n"
      ],
      "metadata": {
        "id": "WTv4XN2qB_fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "setting\n",
        "======\n",
        ">Here are all parameters using in this project."
      ],
      "metadata": {
        "id": "iVQ2D_mLcx4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "setting = {\n",
        "# using in part 1\n",
        "\"data_info\" : {\n",
        "    \"document\":\"/content\",\n",
        "    \"raw_file_name\":\"/ted2020.tgz\",\n",
        "    \"unzip_path\":\"/train_dev/\",\n",
        "    \"source\":{\n",
        "        \"lang\":\"en\",\n",
        "        \"raw_data_path\":\"/train_dev/raw.en\",\n",
        "        \"clean_data_path\":\"/train_dev/clean_en.txt\",\n",
        "        \"tokenized_train_data\":\"/train_dev/tokenized_train_data.txt\",\n",
        "        \"tokenized_val_data\":\"/train_dev/tokenized_val_data.txt\"\n",
        "        },\n",
        "    \"target\":{\n",
        "        \"lang\":\"zh\",\n",
        "        \"raw_data_path\":\"/train_dev/raw.zh\",\n",
        "        \"clean_data_path\":\"/train_dev/clean_zh.txt\",\n",
        "        \"tokenized_train_data\":\"/train_dev/tokenized_train_data.txt\",\n",
        "        \"tokenized_val_data\":\"/train_dev/tokenized_val_data.txt\"\n",
        "        }\n",
        "},\n",
        "# using mainly in part 1\n",
        "# but \"vocab_size\",\"pad_id\",\"bos_id\",\"eos_id\",\"max_l\" used in other parts\n",
        "\"tokenized_setting\" : {\n",
        "    \"vocab_size\" : 8000,\n",
        "    \"character_coverage\" : 1,\n",
        "    \"model_type\" : \"unigram\", # \"bpe\",\n",
        "    \"input_sentence_size\" : 400000,\n",
        "    \"shuffle_input_sentence\" : True,\n",
        "    \"normalization_rule_name\" : \"nmt_nfkc_cf\",\n",
        "    \"pad_id\":0,\n",
        "    \"unk_id\":1,\n",
        "    \"bos_id\":2,\n",
        "    \"eos_id\":3,\n",
        "    \"max_l\":400\n",
        "},\n",
        "# using mainly in part 3\n",
        "\"training_hparas\" : {\n",
        "    \"batch_size\" : 400\n",
        "}\n",
        "}"
      ],
      "metadata": {
        "id": "-WoR-01STMor"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "installing package\n",
        "------"
      ],
      "metadata": {
        "id": "cdURO12Sntrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# used in part 1\n",
        "!pip install sentencepiece\n",
        "# used in part 1 and 3\n",
        "!pip install tqdm\n",
        "# used in part 2\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ti-UGuHNhh",
        "outputId": "1906bfcc-a9dd-4c9f-ddc0-a02b815472f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing data set\n",
        "=============\n",
        "\n",
        "load data file\n",
        "-------------\n",
        ">Here I load dataset from my drive,  \n",
        ">but it also can be download from the link below."
      ],
      "metadata": {
        "id": "mMkT3K4JXs60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 : download dataset from drive to google colab\n",
        "# original dataset is in \"https://mega.nz/#!vEcTCISJ!3Rw0eHTZWPpdHBTbQEqBDikDEdFPr7fI8WxaXK9yZ9U\"\n",
        "\n",
        "path_doc = setting[\"data_info\"][\"document\"]\n",
        "rawdata_file_name = setting[\"data_info\"][\"raw_file_name\"]\n",
        "rawdata_file_path = path_doc + rawdata_file_name\n",
        "unzip_path = path_doc + setting[\"data_info\"][\"unzip_path\"]\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive_path = path_doc + \"/drive\"\n",
        "drive_name = \"/MyDrive\"\n",
        "drive.mount(drive_path)\n",
        "\n",
        "# copy file from drive\n",
        "import shutil\n",
        "shutil.copyfile(drive_path + drive_name + rawdata_file_name, rawdata_file_path)\n",
        "\n",
        "# step 2 : unzip dataset\n",
        "import tarfile\n",
        "# open file\n",
        "file = tarfile.open(rawdata_file_path)\n",
        "# extracting file\n",
        "file.extractall(unzip_path)\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X23RaTWe9hoU",
        "outputId": "0fd2cd04-f889-425a-fdb7-adccb573e42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean data\n",
        "------\n",
        ">First each dataset (source or target) is clean  \n",
        ">seperately, change to halfwidth and remove/replace  \n",
        ">some kind of punctuations.\n",
        "\n",
        ">Also because the number of sentences in one line may be  \n",
        ">different in line pairs of source and target set (its an error),  \n",
        ">some special punctuations is add to the end of sentences  \n",
        ">for the next process dealing with these problem by  \n",
        ">using sentence pairs instead of lines pairs to form datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "-y54N2UNimor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "# convert fullwidth to halfwidth\n",
        "def to_halfwidth(string):\n",
        "  return \"\".join(unicodedata.normalize('NFKC',letter) for letter in string)\n",
        "def clean_s_zh(s):\n",
        "    s = to_halfwidth(s)\n",
        "    # step 1 : delete — _\n",
        "    delete = \" _()[]\"\n",
        "    delete_rules = s.maketrans(\"\",\"\",delete)\n",
        "    s = s.translate(delete_rules)\n",
        "\n",
        "    # step 2 : replace “” with \"\"\n",
        "    to_be_replace = '“”'\n",
        "    replace = '\"\"'\n",
        "    replace_dict = dict(zip(to_be_replace,replace))\n",
        "\n",
        "    # step 3 : add **END** before and after punctuation\n",
        "\n",
        "    \"\"\"\n",
        "    The number of sentences in one line may be different\n",
        "    in line pairs of source and target set.\n",
        "    \"**END**\" is add after \"。!?\" and \".!?\", which can be used\n",
        "    to check if the number of sentence in the pair are equal\n",
        "    in the next process.\n",
        "    also in english, \".\" may be use in abbreviation,\n",
        "    these different use must be identified.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    punctuation = \"。!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    zh_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return zh_list\n",
        "\n",
        "def clean_s_en(s):\n",
        "    s = to_halfwidth(s)\n",
        "\n",
        "    replace_dict = {}\n",
        "\n",
        "    delete = \"-()[]\"\n",
        "    for char in delete:\n",
        "      replace_dict[char] = \"\"\n",
        "\n",
        "    punctuation = \"!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    # Identify if \".\" is used in abbreviation,\n",
        "    # if not, add \"**END**\" after it.\n",
        "    pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "    s = pattern.sub(\"**END**\",s)\n",
        "\n",
        "    # test pattern\n",
        "    # pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "    # result = pattern.sub(\"**END**\",\"There are many people in U.S. w.r.t. in Taiwan.Thank you.\")\n",
        "\n",
        "    en_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return en_list"
      ],
      "metadata": {
        "id": "5dwWeVz-sPI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pick up line pairs\n",
        "------\n",
        ">pick up line pairs has equal number of sentences and  \n",
        ">split them into sentences to form sourse/target dataset.  \n",
        ">Remove sentences with too many words for training and validation."
      ],
      "metadata": {
        "id": "M2EfnpIXV91l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using \"**END**\" to split line pairs to check if they have equal sentence\n",
        "def divide_by_END(s):\n",
        "    list_s = []\n",
        "    for line_string in s.strip(\"**END**\").split(\"**END**\"):\n",
        "      if line_string not in [\"\",\" \"]:\n",
        "         list_s.append(line_string)\n",
        "    return(list_s)\n",
        "'''\n",
        "warning : devide_en_again function is apply just beacause\n",
        "in \"this\" dataset english sentences end with \":\" or \";\"\n",
        "sometimes not splited well.\n",
        "If the dataset is change, this part may need to be\n",
        "eliminated or modified.\n",
        "'''\n",
        "def devide_en_again(s,punctuation = \":;\"):\n",
        "    replace_dict = {}\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules_src = s.maketrans(replace_dict)\n",
        "    new_s = divide_by_END(s.translate(replace_rules_src))\n",
        "    return new_s\n",
        "\n",
        "# remove \"sentence\" if it is too long.\n",
        "def remove_too_long(src_list,tgt_list,threshold = setting[\"tokenized_setting\"][\"max_l\"]):\n",
        "    too_long_src = 0\n",
        "    too_long_tgt = 0\n",
        "    remove = False\n",
        "    new_s = []\n",
        "    new_t = []\n",
        "    for i in range(len(src_list)):\n",
        "      if ((len(src_list[i])>threshold)):\n",
        "        remove = True\n",
        "        too_long_src += 1\n",
        "      if (len(tgt_list[i])>threshold):\n",
        "        remove = True\n",
        "        too_long_tgt += 1\n",
        "      if remove == False:\n",
        "        new_s.append(src_list[i])\n",
        "        new_t.append(tgt_list[i])\n",
        "      else :\n",
        "        remove = False\n",
        "    return(new_s,new_t,too_long_src,too_long_tgt)\n",
        "\n",
        "# pick up good line pairs for traning and validation model\n",
        "def check_data_pairs(src_list,tgt_list):\n",
        "    index = 0\n",
        "    new_src_list = []\n",
        "    new_tgt_list = []\n",
        "\n",
        "    same = 0\n",
        "    add_next = 0\n",
        "    split_again = 0\n",
        "    not_use = 0\n",
        "\n",
        "    while(index < len(src_list)):\n",
        "\n",
        "      src = divide_by_END(src_list[index])\n",
        "      tgt = divide_by_END(tgt_list[index])\n",
        "\n",
        "      # case 1 : src is as long as tgt , finished.\n",
        "      if len(src) == len(tgt):\n",
        "        new_src_list += src\n",
        "        new_tgt_list += tgt\n",
        "        same += 1\n",
        "        index += 1\n",
        "\n",
        "      else :\n",
        "        # if it is not the last one : both src and tgt add next sentence\n",
        "        if index != len(src_list)-1:\n",
        "          src_add_next = divide_by_END(src_list[index] + src_list[index+1])\n",
        "          tgt_add_next = divide_by_END(tgt_list[index] + tgt_list[index+1])\n",
        "          # case 2 : src_add_next is as long as tgt_add_next , finished.\n",
        "          if len(src_add_next) == len(tgt_add_next):\n",
        "            new_src_list += src_add_next\n",
        "            new_tgt_list += tgt_add_next\n",
        "            add_next += 2\n",
        "            index += 2\n",
        "\n",
        "          # using new punctuation to divide tgt (english) sentence.\n",
        "          # note that this part could cause negative effects if the dataset is change.\n",
        "          else :\n",
        "            src_add_next = devide_en_again(src_list[index] + src_list[index+1])\n",
        "            # case 3 : src_add_next is as long as tgt_add_next , finished.\n",
        "            if len(src_add_next) == len(tgt_add_next):\n",
        "              new_src_list += src_add_next\n",
        "              new_tgt_list += tgt_add_next\n",
        "              split_again +=2\n",
        "              index += 2\n",
        "\n",
        "            # case 4 : sentence will not be used.\n",
        "            else :\n",
        "              not_use += 1\n",
        "              # if to_do == 1 :\n",
        "              #   print(index,src_add_next,tgt_add_next,len(src_add_next),len(tgt_add_next))\n",
        "              index += 1\n",
        "\n",
        "        # if it is the last one\n",
        "        else :\n",
        "          not_use += 1\n",
        "          index += 1\n",
        "    # print information\n",
        "    print(f\"The original total number of line is {index}.\")\n",
        "    print(f\"The number of line pairs have the equal sentences is {same}.\")\n",
        "    print(f\"The number of line pairs have the equal sentences after combine the next lines is {add_next}.\")\n",
        "    print(f\"The number of line pairs have the equal sentences after combine the next lines\"+\\\n",
        "       f\"and resplit english lines using :; is {split_again}.\")\n",
        "    print(f\"The number of line we don't use is {not_use}.\")\n",
        "    print(f\"Note that {index} = {same}+{add_next}+{split_again}+{not_use}.\")\n",
        "\n",
        "    # remove long lines\n",
        "    print(f\"The total number of sentence pairs before remove long sentences is {len(new_src_list)}.\")\n",
        "    new_src_list,new_tgt_list,too_long_src,too_long_tgt = remove_too_long(new_src_list,new_tgt_list)\n",
        "    print(f\"The finally total number of sentence pairs using is {len(new_src_list)}.\")\n",
        "    print(f\"Note that {len(new_src_list)} are the number of sentence pairs, not line pairs\")\n",
        "\n",
        "    return(new_src_list,new_tgt_list)"
      ],
      "metadata": {
        "id": "UVYvYCpuDQDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load and clean data\n",
        "------"
      ],
      "metadata": {
        "id": "1KPMwJwck437"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load and clean data\n",
        "def load_file(path,function):\n",
        "    with open(path, \"r\") as f:\n",
        "      data = f.read()\n",
        "      return function(data)\n",
        "# saving to new path\n",
        "def clean_data_and_save(\n",
        "    path_doc = setting[\"data_info\"][\"document\"],\n",
        "    raw_src_path = setting[\"data_info\"][\"source\"][\"raw_data_path\"],\n",
        "    raw_tgt_path = setting[\"data_info\"][\"target\"][\"raw_data_path\"],\n",
        "    clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "    clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"]\n",
        "    ):\n",
        "    raw_src_path = path_doc + raw_src_path\n",
        "    raw_tgt_path = path_doc + raw_tgt_path\n",
        "    src = load_file(raw_src_path,clean_s_en),\n",
        "    tgt = load_file(raw_tgt_path,clean_s_zh),\n",
        "    # src , tgt are tuples with only one term : src_list, tgt_list\n",
        "    src_list = src[0]\n",
        "    tgt_list = tgt[0]\n",
        "    clean_src_list, clean_tgt_list = check_data_pairs(src_list,tgt_list)\n",
        "    with open(path_doc + clean_src_path, \"w\") as f:\n",
        "      f.write(\"\\n\".join(clean_src_list))\n",
        "    with open(path_doc + clean_tgt_path, \"w\") as f:\n",
        "      f.write(\"\\n\".join(clean_tgt_list))\n",
        "# clean_data_and_save()"
      ],
      "metadata": {
        "id": "DhVI2xylk1tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenize\n",
        "------\n",
        ">using sentencepiece to tokenize sentences,  \n",
        ">first make the english/chinese dictionary separately,  \n",
        ">then use these dict to encode sentence pair in dataset,  \n",
        ">including add bos/eos/padding to tokenized sentences.  \n",
        ">Finally split then into train/val set and save."
      ],
      "metadata": {
        "id": "S_-APGe5okQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "def tokenized(clean_data_path,\n",
        "       vocab_size,\n",
        "       lang,\n",
        "       tokenized_setting\n",
        "       ):\n",
        "  model_prefix = f\"spm_{vocab_size}_{lang}\"\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      input=clean_data_path,\n",
        "      **tokenized_setting,\n",
        "      model_prefix=model_prefix,\n",
        "  )\n",
        "  return(model_prefix)\n",
        "def bos_eos_padding(dataset,\n",
        "          max_l,\n",
        "          vocab_size,\n",
        "          src_lang,\n",
        "          tgt_lang\n",
        "          ):\n",
        "\n",
        "  s_src = spm.SentencePieceProcessor(model_file = path_doc + f\"/spm_{vocab_size}_{src_lang}\" +\".model\")\n",
        "  s_tgt = spm.SentencePieceProcessor(model_file = path_doc + f\"/spm_{vocab_size}_{tgt_lang}\" +\".model\")\n",
        "  padding_src = []\n",
        "  padding_tgt = []\n",
        "  len_s = 0\n",
        "  len_t = 0\n",
        "  for src,tgt in dataset:\n",
        "    s = s_src.encode(src, out_type=int)\n",
        "    s = np.append(s,[3])\n",
        "    s = np.append([2],np.pad(s,(0, max_l-len(s)), constant_values = 0))\n",
        "    padding_src.append(s)\n",
        "\n",
        "    t = s_tgt.encode(tgt, out_type=int)\n",
        "    t = np.append(t,[3])\n",
        "    t = np.append([2],np.pad(t,(0, max_l-len(t)), constant_values = 0))\n",
        "    padding_tgt.append(t)\n",
        "\n",
        "  return(list(zip(padding_src,padding_tgt)))\n",
        "# test SentencePieceProcessor and bos_eos_padding\n",
        "# s_src = spm.SentencePieceProcessor(model_file=\"/content/spm8000_en.model\")\n",
        "# s_src.encode(\"hello world!\", out_type=int)\n",
        "# bos_eos_padding([(\"hello world\",\"_哈囉\")],5,10)\n",
        "\n",
        "def data_set_preparing(path_doc,\n",
        "            clean_src_path,\n",
        "            clean_tgt_path,\n",
        "            max_l,\n",
        "            vocab_size,\n",
        "            src_lang,\n",
        "            tgt_lang,\n",
        "            st_train_path,\n",
        "            st_val_path,\n",
        "            tt_train_path,\n",
        "            tt_val_path,\n",
        "            ):\n",
        "    src_set = []\n",
        "    tgt_set = []\n",
        "\n",
        "    with open(path_doc+clean_src_path,\"r\") as in_f :\n",
        "      for line in in_f:\n",
        "        src_set.append(line)\n",
        "    with open(path_doc+clean_tgt_path,\"r\") as in_f :\n",
        "      for line in in_f:\n",
        "        tgt_set.append(line)\n",
        "\n",
        "    dataset = list(zip(src_set,tgt_set))\n",
        "    dataset = bos_eos_padding(dataset,max_l,vocab_size,src_lang,tgt_lang)\n",
        "    train_set, valid_set = data.random_split(dataset,[0.9,0.1])\n",
        "    # print(train_set[0][0])\n",
        "\n",
        "    with open(path_doc + st_train_path, 'w') as out_f:\n",
        "      for line_pair in train_set:\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[0])+\"\\n\")\n",
        "    with open(path_doc + st_val_path, 'w') as out_f:\n",
        "      for line_pair in valid_set:\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[0])+\"\\n\")\n",
        "    with open(path_doc + tt_train_path, 'w') as out_f:\n",
        "      for line_pair in train_set:\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[1])+\"\\n\")\n",
        "    with open(path_doc + tt_val_path, 'w') as out_f:\n",
        "      for line_pair in valid_set:\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[1])+\"\\n\")"
      ],
      "metadata": {
        "id": "ByrUmAvFkKk9"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenized_data(vocab_size = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "    tokenized_setting = {k:setting[\"tokenized_setting\"][k] for k in \\\n",
        "               set(list(setting[\"tokenized_setting\"].keys()))-{\"vocab_size\",\"max_l\"}},\n",
        "    max_l = setting[\"tokenized_setting\"][\"max_l\"],\n",
        "    path_doc = setting[\"data_info\"][\"document\"],\n",
        "    clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "    clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"],\n",
        "    src_lang = setting[\"data_info\"][\"source\"][\"lang\"],\n",
        "    tgt_lang = setting[\"data_info\"][\"target\"][\"lang\"],\n",
        "    st_train_path = setting[\"data_info\"][\"source\"][\"tokenized_train_data\"],\n",
        "    st_val_path = setting[\"data_info\"][\"source\"][\"tokenized_val_data\"],\n",
        "    tt_train_path = setting[\"data_info\"][\"target\"][\"tokenized_train_data\"],\n",
        "    tt_val_path = setting[\"data_info\"][\"target\"][\"tokenized_val_data\"],\n",
        "    ):\n",
        "  tokenized(path_doc + clean_src_path,vocab_size,src_lang,tokenized_setting)\n",
        "  tokenized(path_doc + clean_tgt_path,vocab_size,tgt_lang,tokenized_setting)\n",
        "  data_set_preparing(path_doc,clean_src_path,clean_tgt_path,max_l,vocab_size,src_lang,\n",
        "            tgt_lang,st_train_path,st_val_path,tt_train_path,tt_val_path,)\n",
        "tokenized_data()"
      ],
      "metadata": {
        "id": "RkIHpc6qkRqh"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make data set\n",
        "------\n",
        "> Using tokenized data to make dataset.  \n",
        "> Classmethod : padding_mask_batch which  \n",
        "> where the key padding mask is constucted  \n",
        "> also defined here."
      ],
      "metadata": {
        "id": "FV3kHGqlggdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class myDataset(Dataset):\n",
        "  def __init__(self,src_path,tgt_path):\n",
        "\n",
        "    self.src_path = src_path\n",
        "    self.tgt_path = tgt_path\n",
        "\n",
        "    src_list = []\n",
        "    with open(self.src_path,\"r\") as f :\n",
        "      d_l = f.readlines()\n",
        "      for line in tqdm(d_l):\n",
        "        int_list = [int(i) for i in line.split()]\n",
        "        src_list.append(int_list)\n",
        "    self.src = torch.LongTensor(src_list)\n",
        "\n",
        "    tgt_list = []\n",
        "    with open(self.tgt_path,\"r\") as f :\n",
        "      l_l = f.readlines()\n",
        "      for line in tqdm(l_l):\n",
        "        int_list = [int(i) for i in line.split()]\n",
        "        tgt_list.append(int_list)\n",
        "    self.tgt = torch.LongTensor(np.array(tgt_list))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.src)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.src, self.tgt\n",
        "\n",
        "  # make key padding mask\n",
        "  @classmethod\n",
        "  def padding_mask_batch(cls,batch,pad_id = setting[\"tokenized_setting\"][\"pad_id\"]):\n",
        "    \"\"\"Collate a batch of data.\"\"\"\n",
        "    src, tgt = zip(*batch)\n",
        "    src = torch.stack(src)\n",
        "    tgt = torch.stack(tgt)\n",
        "    src_padding = (src == pad_id)\n",
        "    tgt_padding = (tgt == pad_id)\n",
        "\n",
        "    return src, tgt , src_padding, tgt_padding\n",
        "# test myDataset\n",
        "# data = []\n",
        "# with open(\"/content/train_dev/tokenized_train_data.txt\",\"r\") as f :\n",
        "#   d_l = f.readlines()\n",
        "#   for line in tqdm(d_l):\n",
        "#     int_list = [int(i) for i in line.split()]\n",
        "#     data.append(int_list)"
      ],
      "metadata": {
        "id": "dbfi-DvrlDhI"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "def get_data_set(batch_size = setting[\"training_hparas\"][\"batch_size\"],\n",
        "         path_doc = setting[\"data_info\"][\"document\"],\n",
        "         st_train_path = setting[\"data_info\"][\"source\"][\"tokenized_train_data\"],\n",
        "         st_val_path = setting[\"data_info\"][\"source\"][\"tokenized_val_data\"],\n",
        "         tt_train_path = setting[\"data_info\"][\"target\"][\"tokenized_train_data\"],\n",
        "         tt_val_path = setting[\"data_info\"][\"target\"][\"tokenized_val_data\"],\n",
        "         ):\n",
        "\n",
        "  train_set = myDataset(src_path = path_doc + st_train_path,\n",
        "              tgt_path = path_doc + tt_train_path,\n",
        "              )\n",
        "  valid_set = myDataset(src_path = path_doc + st_val_path,\n",
        "              tgt_path = path_doc + tt_val_path,\n",
        "              )\n",
        "  train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=8,\n",
        "    pin_memory=True,\n",
        "    collate_fn=myDataset.padding_mask_batch\n",
        "  )\n",
        "  valid_loader = DataLoader(\n",
        "    valid_set,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=8,\n",
        "    pin_memory=True,\n",
        "    collate_fn=myDataset.padding_mask_batch\n",
        "  )\n",
        "  return train_loader,valid_loader\n",
        "get_data_set()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBFKMSM45ppM",
        "outputId": "411f1e7c-0fd5-43d6-9eb2-7a755733cbbd"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 349149/349149 [00:38<00:00, 9155.13it/s] \n",
            "100%|██████████| 349149/349149 [00:36<00:00, 9642.77it/s] \n",
            "100%|██████████| 38794/38794 [00:03<00:00, 10204.98it/s]\n",
            "100%|██████████| 38794/38794 [00:03<00:00, 12823.07it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7be67e044280>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7be67e045f30>)"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "make model\n",
        "======\n",
        "positional encoding layer\n",
        "------\n",
        ">The first layer is embedding layer, where each integers  \n",
        ">in encoder sentence will be represent by a vector.   \n",
        ">I use build-in class in pytorch to finish these part,    \n",
        ">and combine it with encoder layers to form my encoder.\n",
        "\n",
        ">The layer below is the second layer :positional encoding layer  \n",
        ">in this layer the position infomation is add to each \"word\"  \n",
        ">in the sentence.\n",
        ">Here I use parameters instead of constant as  \n",
        ">position infomation so they will change during training process."
      ],
      "metadata": {
        "id": "RZAzqY3bwds8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Positional_Encoding(nn.Module):\n",
        "    def __init__(self,max_sentence_length,embedding_dimension):\n",
        "      super().__init__()\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.encoding_values = nn.Parameter(nn.init.normal_(torch.empty(max_sentence_length,1, embedding_dimension)))\n",
        "    def forward(self, x):\n",
        "        # the shape of x : [batch,length,e_dim]\n",
        "        # the shape of self.encoding_values : [batch,length,e_dim]\n",
        "        x = x + self.encoding_values.unsqueeze(0)\n",
        "        x = x.squeeze(-2)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "EoKH9m1LznWO"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "multihead attention layer\n",
        "------\n"
      ],
      "metadata": {
        "id": "OURB2Fg-4lE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import math\n",
        "from torchinfo import summary\n",
        "\n",
        "def attn_mask(input_dim):\n",
        "    return nn.Transformer.generate_square_subsequent_mask(input_dim)\n",
        "#attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
        "\n",
        "\n",
        "# This part is modify from pytorch : torch.nn.functional.scaled_dot_product_attention\n",
        "# Efficient implementation equivalent to the following:\n",
        "def scaled_dot_product_attention(query, key, value, padding_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> torch.Tensor:\n",
        "    # Efficient implementation equivalent to the following:\n",
        "    L, S = query.size(-2), key.size(-2)\n",
        "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
        "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
        "\n",
        "    if is_causal:\n",
        "      # assert attn_mask is None\n",
        "      temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
        "      attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
        "      attn_bias.to(query.dtype)\n",
        "\n",
        "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
        "    attn_weight += attn_bias\n",
        "\n",
        "    if padding_mask is not None:\n",
        "        if padding_mask.dtype == torch.bool:\n",
        "          padding_mask = torch.zeros_like(padding_mask,dtype = float).masked_fill_(padding_mask, (float(\"-inf\")))\n",
        "\n",
        "        padding_mask = padding_mask.unsqueeze(0).unsqueeze(0)\n",
        "        padding_mask.to(query.dtype)\n",
        "\n",
        "        attn_weight = attn_weight.transpose(-4,-2)\n",
        "        attn_weight += padding_mask\n",
        "        attn_weight = attn_weight.transpose(-4,-2)\n",
        "\n",
        "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
        "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
        "    return attn_weight @ value\n",
        "# test scaled_dot_product_attention\n",
        "# t = torch.rand([2,3,4,5])\n",
        "# mask = torch.tensor([[False,False,True,True],[False,True,False,True]],dtype = torch.bool)\n",
        "# print(scaled_dot_product_attention(t,t,t,padding_mask= mask, is_causal=True))\n",
        "# from torch.nn.functional import scaled_dot_product_attention\n",
        "class My_MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, kv_input_dimension, embedding_dimension, num_heads, dropout=0.1, if_decoder = False):\n",
        "        '''\n",
        "        embedding_dimension = input dimension\n",
        "        note that there are residual sublayers in MultiHeadedAttention\n",
        "        '''\n",
        "        super().__init__()\n",
        "        assert embedding_dimension % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.kv_d = kv_input_dimension\n",
        "        self.d = embedding_dimension\n",
        "        self.num_heads = num_heads\n",
        "        self.is_causal = if_decoder\n",
        "        self.linear_for_q = nn.Linear(self.d, self.d)\n",
        "        self.linear_for_kv = nn.Linear(self.kv_d, 2 * self.d)\n",
        "        self.linear_out_project = nn.Linear(self.d, self.d)\n",
        "\n",
        "    def forward(self, q_input_data, kv_input_data , padding_mask = None):\n",
        "\n",
        "        query = self.linear_for_q(q_input_data)\n",
        "        key, value = self.linear_for_kv(kv_input_data).split(self.d,dim = -1)\n",
        "\n",
        "        query,key,value = \\\n",
        "          map(lambda x : x.view(x.size(0),x.size(1),self.num_heads,self.d//self.num_heads),[query,key,value])\n",
        "        query,key,value = \\\n",
        "          map(lambda x : x.transpose(-2,-3),[query,key,value])\n",
        "\n",
        "        x = scaled_dot_product_attention(query,key,value,padding_mask = padding_mask, dropout_p =0, is_causal = self.is_causal)\n",
        "        x = x.transpose(-2,-3).contiguous()\n",
        "        x = x.view(x.size(0),x.size(1),self.d)\n",
        "        x = self.linear_out_project(x)\n",
        "\n",
        "        return x\n",
        "# test My_MultiHeadedAttention\n",
        "# model = My_MultiHeadedAttention(64,128,2)\n",
        "# q_input = torch.rand(32,400,128)\n",
        "# kv_input = torch.rand(32,400,64)\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(q_input,kv_input,mask).size())\n",
        "# print(summary(model,q_input_data = q_input, kv_input_data = kv_input,padding_mask = mask))"
      ],
      "metadata": {
        "id": "PLWLkr9UaKFD"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "encoder layer(s)\n",
        "------"
      ],
      "metadata": {
        "id": "T0IjI-zC5fqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class My_Encoder_Layer(nn.Module):\n",
        "  def __init__(self,embedding_dimension,feedforward_dimension):\n",
        "    super().__init__()\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "\n",
        "    self.attention = My_MultiHeadedAttention(self.emb_dim, self.emb_dim ,num_heads = 2, dropout=0)\n",
        "    self.layer_norm_attn = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_attn_layernorm = nn.Dropout(0)\n",
        "\n",
        "    self.feedforward = nn.Sequential(\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    )\n",
        "    self.layer_norm_feedforward = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_feedforward_layernorm = nn.Dropout(0)\n",
        "\n",
        "\n",
        "  def forward(self,x,padding_mask):\n",
        "    x = x + self.attention(x,x,padding_mask)\n",
        "    x = self.layer_norm_attn(x)\n",
        "\n",
        "    x = self.drop_out_attn_layernorm(x)\n",
        "\n",
        "    x = x + self.feedforward(x)\n",
        "    x = self.layer_norm_feedforward(x)\n",
        "    x = self.drop_out_feedforward_layernorm(x)\n",
        "\n",
        "    return x\n",
        "# test My_Encoder_Layer\n",
        "# model = My_Encoder_Layer(128,256)\n",
        "# input = torch.rand((32,400,128))\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(input,mask).size())\n",
        "# print(summary(model,input_data = input,padding_mask = mask))\n",
        "# print(model.state_dict().keys())\n",
        "class My_Encoder(nn.Module):\n",
        "  def __init__(self,max_sentence_length,dictionary_length,embedding_dimension,feedforward_dimension,layer_num = 2):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.dict_l = dictionary_length\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.encoder_embedding = nn.Embedding(self.dict_l,self.emb_dim,padding_idx=0)\n",
        "    self.positional_encoding = Positional_Encoding(self.max_l,self.emb_dim)\n",
        "    self.encoder = nn.ModuleList([My_Encoder_Layer(self.emb_dim,self.fwd_dim) for i in range(layer_num)])\n",
        "\n",
        "  def forward(self,x,padding_mask):\n",
        "    x = self.encoder_embedding(x)* math.sqrt(self.emb_dim)\n",
        "    x = self.positional_encoding(x)\n",
        "\n",
        "    for index,module in enumerate(self.encoder):\n",
        "      if index == 0:\n",
        "        x = module(x,padding_mask)\n",
        "      else:\n",
        "        x = module(x,None)\n",
        "    return x\n",
        "# test My_Encoder\n",
        "# model = My_Encoder(400,8000,128,256)\n",
        "# input = torch.randint(0,7999,(32,400,1),dtype = torch.long)\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(input,mask).size())\n",
        "# print(summary(model,input_data = input,padding_mask = mask))\n",
        "# print(model.state_dict().keys())"
      ],
      "metadata": {
        "id": "PYhd7muASnrY"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "decoder layer(s)\n",
        "------"
      ],
      "metadata": {
        "id": "4O_rI3QV55Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class My_Decoder_Layer(nn.Module):\n",
        "  def __init__(self,encoder_embedding_dimension,embedding_dimension,feedforward_dimension):\n",
        "    super().__init__()\n",
        "    self.encoder_dim = encoder_embedding_dimension\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "\n",
        "    self.self_attention = My_MultiHeadedAttention \\\n",
        "     (self.emb_dim,self.emb_dim, num_heads = 2, dropout=0, if_decoder = True)\n",
        "    self.layer_norm_sa = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_sa = nn.Dropout(0)\n",
        "\n",
        "    self.feedforward_sa = nn.Sequential(\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    )\n",
        "    self.layer_norm_sa_fw = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_sa_fw = nn.Dropout(0)\n",
        "\n",
        "    self.cross_attention = My_MultiHeadedAttention \\\n",
        "     (self.encoder_dim, self.emb_dim, num_heads = 2, dropout=0, if_decoder = True)\n",
        "    self.layer_norm_ca = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_ca = nn.Dropout(0)\n",
        "\n",
        "    self.feedforward_ca = nn.Sequential(\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    )\n",
        "    self.layer_norm_ca_fw = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_ca_fw = nn.Dropout(0)\n",
        "\n",
        "  def forward(self,encoder_input,input,padding_mask):\n",
        "\n",
        "    x = input + self.self_attention(input,input,padding_mask)\n",
        "    x = self.layer_norm_sa(x)\n",
        "    x = self.drop_out_sa(x)\n",
        "\n",
        "    x = x + self.feedforward_sa(x)\n",
        "    x = self.layer_norm_sa_fw(x)\n",
        "    x = self.drop_out_sa_fw(x)\n",
        "\n",
        "    x = x + self.cross_attention(x,encoder_input,padding_mask)\n",
        "    x = self.layer_norm_ca(x)\n",
        "    x = self.drop_out_ca(x)\n",
        "\n",
        "    x = x + self.feedforward_ca(x)\n",
        "    x = self.layer_norm_ca_fw(x)\n",
        "    x = self.drop_out_ca_fw(x)\n",
        "\n",
        "    return x\n",
        "class My_Decoder(nn.Module):\n",
        "  def __init__(self,max_sentence_length,dictionary_length,encoder_embedding_dimension,\\\n",
        "               embedding_dimension,feedforward_dimension,layer_num = 2):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.dict_l = dictionary_length\n",
        "    self.encoder_dim = encoder_embedding_dimension\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.decoder_embedding = nn.Embedding(self.dict_l,self.emb_dim,padding_idx=0)\n",
        "    self.positional_encoding = Positional_Encoding(self.max_l,self.emb_dim)\n",
        "    self.decoder = nn.ModuleList([My_Decoder_Layer(self.encoder_dim,self.emb_dim,self.fwd_dim) for i in range(layer_num)])\n",
        "    # self.encoder = My_Encoder_Layer(self.emb_dim,self.fwd_dim)\n",
        "\n",
        "    self.generator = nn.Linear(self.emb_dim,self.dict_l)\n",
        "\n",
        "  def forward(self,encoder_input,input,padding_mask):\n",
        "    x = self.decoder_embedding(input)* math.sqrt(self.emb_dim)\n",
        "    x = self.positional_encoding(x)\n",
        "    # x = self.encoder(x,padding_mask)\n",
        "    for index,module in enumerate(self.decoder):\n",
        "      if index == 0:\n",
        "        x = module(encoder_input,x,padding_mask)\n",
        "      else:\n",
        "        x = module(encoder_input,x,None)\n",
        "    x = self.generator(x)\n",
        "    x = F.log_softmax(x,dim = -1)\n",
        "    return x\n",
        "# test My_Decoder\n",
        "# model = My_Decoder(400,8000,128,64,256)\n",
        "# encoder_input = torch.rand(32,400,128)\n",
        "# input = torch.randint(0,7999,(32,400,1),dtype = torch.long)\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(encoder_input = encoder_input,input = input, padding_mask = mask).size())\n",
        "# print(summary(model,encoder_input = encoder_input,input = input, padding_mask = mask))\n",
        "# print(model.state_dict().keys())"
      ],
      "metadata": {
        "id": "rEjaTbhyBmEV"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer layer\n",
        "------"
      ],
      "metadata": {
        "id": "1gcz18nz6QTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class My_Transformer(nn.Module):\n",
        "  def __init__(self,max_sentence_length,dictionary_length,\\\n",
        "               encoder_embedding_dimension,decoder_embedding_dimension,feedforward_dimension,layer_num = 2):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.dict_l = dictionary_length\n",
        "    self.en_dim = encoder_embedding_dimension\n",
        "    self.de_dim = decoder_embedding_dimension\n",
        "    self.fw_dim = feedforward_dimension\n",
        "    self.layer_num = layer_num\n",
        "    self.encoder = My_Encoder \\\n",
        "     (self.max_l,self.dict_l,self.en_dim,self.fw_dim,self.layer_num)\n",
        "    self.decoder = My_Decoder \\\n",
        "     (self.max_l,self.dict_l,self.en_dim,self.de_dim,self.fw_dim,self.layer_num)\n",
        "\n",
        "  def forward(self,src,tgt,src_mask,tgt_mask):\n",
        "    memory = self.encoder(src,src_mask)\n",
        "    outputs = self.decoder(memory,tgt,tgt_mask)\n",
        "    return outputs\n",
        "# test My_Transformer\n",
        "# model = My_Transformer(400,8000,128,64,256,3)\n",
        "# src = torch.randint(0,8000,(32,400,1),dtype = torch.long)\n",
        "# tgt = torch.randint(0,8000,(32,400,1),dtype = torch.long)\n",
        "# src_mask = torch.cat(((torch.FloatTensor(32,200).uniform_() > 1),(torch.FloatTensor(32,200).uniform_() > 0.15)),dim =1)\n",
        "# tgt_mask = torch.cat(((torch.FloatTensor(32,100).uniform_() > 1),(torch.FloatTensor(32,300).uniform_() > 0.15)),dim =1)\n",
        "# out = model(src,tgt,src_mask,tgt_mask)\n",
        "# print(out.size(),out.dim(),out[0][0])\n",
        "# print(summary(model,src = src,tgt = tgt,src_mask = src_mask,tgt_mask = tgt_mask))\n",
        "# print(model.state_dict().keys())"
      ],
      "metadata": {
        "id": "PBDq_h1jKCo7"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training and validation\n",
        "======\n",
        "label smoothing\n",
        "------"
      ],
      "metadata": {
        "id": "sPbMfCre6cSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "  def __init__(self,dictionary_length,smoothing,padding_id):\n",
        "        super().__init__()\n",
        "        self.dict_len = dictionary_length\n",
        "        self.smoothing = smoothing\n",
        "        self.padding_id = padding_id\n",
        "  def forward(self, outputs , label):\n",
        "\n",
        "    label_onehot = F.one_hot(label,self.dict_len).float().squeeze()\n",
        "    add = self.smoothing / (self.dict_len)\n",
        "    label_onehot += add\n",
        "\n",
        "    label_smoothed = label_onehot.masked_fill_((label_onehot > 1),float(1-self.smoothing+add))\n",
        "    loss = -1*torch.sum((outputs*label_smoothed),dim = -1)\n",
        "\n",
        "    label_padding_mask = (label == self.padding_id).squeeze()\n",
        "    mask_loss = loss.masked_fill_(label_padding_mask,0)\n",
        "\n",
        "    avg_loss = torch.mean(mask_loss)\n",
        "\n",
        "    del label,label_onehot,label_smoothed,label_padding_mask,loss,mask_loss\n",
        "    gc.collect()\n",
        "    return(avg_loss)\n",
        "# ignore_index not work correctly\n",
        "# def LabelSmoothedCrossEntropy(outputs , label,dictionary_length,smooth,padding_id):\n",
        "#   print(outputs.shape)\n",
        "#   print(label.shape)\n",
        "#   label_onehot = label.transpose(-1,-2).squeeze()\n",
        "#   outputs = outputs.transpose(-1,-2)\n",
        "#   cal_loss = nn.CrossEntropyLoss(ignore_index = padding_id,reduction = \"mean\", label_smoothing=smooth)\n",
        "#   return cal_loss(outputs,label_onehot)\n",
        "\n",
        "# test LabelSmoothedCrossEntropyCriterion\n",
        "# cal1 = LabelSmoothedCrossEntropyCriterion(8000,0.1,0)\n",
        "# print(list(iter(cal1.state_dict())))\n",
        "# cal2 = LabelSmoothedCrossEntropy(out,tgt,8000,0.1,0)\n",
        "# print(cal1(out,tgt),cal2)"
      ],
      "metadata": {
        "id": "hdS8eh_WFtNs"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://arxiv.org/pdf/1512.00567.pdf page 7\n",
        "\n",
        "#Ref 1 : Hong-Yi Li ML2021 HW5\n",
        "\n",
        "# class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "#     def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "#         super().__init__()\n",
        "#         self.smoothing = smoothing\n",
        "#         self.ignore_index = ignore_index\n",
        "#         self.reduce = reduce\n",
        "\n",
        "#     def forward(self, lprobs, target):\n",
        "#         if target.dim() == lprobs.dim() - 1:\n",
        "#             target = target.unsqueeze(-1)\n",
        "#         # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
        "#         nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "#         #  reserve some probability for other labels. thus when calculating cross-entropy,\n",
        "#         # equivalent to summing the log probs of all labels\n",
        "#         smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "#         if self.ignore_index is not None:\n",
        "#             pad_mask = target.eq(self.ignore_index)\n",
        "#             nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "#             smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "#         else:\n",
        "#             nll_loss = nll_loss.squeeze(-1)\n",
        "#             smooth_loss = smooth_loss.squeeze(-1)\n",
        "#         if self.reduce:\n",
        "#             nll_loss = nll_loss.sum()\n",
        "#             smooth_loss = smooth_loss.sum()\n",
        "#         # when calculating cross-entropy, add the loss of other labels\n",
        "#         eps_i = self.smoothing / lprobs.size(-1)\n",
        "#         loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "#         return loss\n",
        "\n",
        "#Ref 2 : By hemingkx : https://github.com/hemingkx/ChineseNMT\n",
        "\n",
        "# class LabelSmoothing(nn.Module):\n",
        "#     \"\"\"Implement label smoothing.\"\"\"\n",
        "\n",
        "#     def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "#         super(LabelSmoothing, self).__init__()\n",
        "#         self.criterion = nn.KLDivLoss(size_average=False)\n",
        "#         self.padding_idx = padding_idx\n",
        "#         self.confidence = 1.0 - smoothing\n",
        "#         self.smoothing = smoothing\n",
        "#         self.size = size\n",
        "#         self.true_dist = None\n",
        "\n",
        "\n",
        "#     def forward(self, x, target):\n",
        "#         assert x.size(1) == self.size\n",
        "#         true_dist = x.data.clone()\n",
        "#         true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "#         true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "#         true_dist[:, self.padding_idx] = 0\n",
        "#         mask = torch.nonzero(target.data == self.padding_idx)\n",
        "#         if mask.dim() > 0:\n",
        "#             true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "#         self.true_dist = true_dist\n",
        "#         return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "metadata": {
        "id": "uaE0-tA9Q9cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "beam search\n",
        "------"
      ],
      "metadata": {
        "id": "33m5daxZ7Cpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "def beam_search_one_step(sentences,p_sentences,n_beam_output):\n",
        "    # sentences : {type : tensor , shape : (batch X beam_num) X now_sentences_length X 1 value : int}\n",
        "    # p_sentences : {type : tensor , shape : batch X beam_num X 1 value : log_softmax probability}\n",
        "    # n_beam_output : {type : tensor , shape : batch X beam_num X dictionary_length,\n",
        "    # value : [P1,P2,P3...] X beam_num times (Pk in [0,1])}\n",
        "\n",
        "    '''\n",
        "    TO DO : (set beam num = K)\n",
        "    for every batch:\n",
        "    expand sentences(total number = K) K times (so there are K-square sentences),then concat with\n",
        "    the index of top K consequence of each beam(total K beams) in n_beam_output (so there are also K-square values).\n",
        "    '''\n",
        "    batch = n_beam_output.size(0)\n",
        "    beam_num = n_beam_output.size(1)\n",
        "    # sentences : {type : tensor , shape : batch X beam_num X now_sentences_length value : int}\n",
        "    sentences = sentences.view(batch,beam_num,-1)\n",
        "    # repeat : {type : tensor , shape : beam_num X 1 ,value : beam_num}\n",
        "    # each row repeat beam_num times before concatenate\n",
        "    repeat = torch.full([beam_num],fill_value = beam_num)\n",
        "    # sentences_expand : {type : tensor , shape : batch X (beam_num X beam_num) X now_sentences_length ,\n",
        "    # value : [[[A,B...] X beam_num times,[C,D...] X beam_num times}...] A,B,C,D...are int}\n",
        "    sentences_expand = torch.repeat_interleave(sentences,repeat,dim=1)\n",
        "    del repeat\n",
        "    # topk_prob : {type : tensor , shape : batch X beam_num X beam_num, value : log_softmax probability}\n",
        "    # topk_index : {type : tensor , shape : batch X beam_num X beam_num, value : int}\n",
        "    topk_prob, topk_index = torch.topk(n_beam_output,dim = -1,k = beam_num)\n",
        "\n",
        "    # topk_index : {type : tensor , shape : batch X (beam_num X beam_num) X 1, value : int}\n",
        "    topk_index = topk_index.view(batch,-1,1)\n",
        "    # sentences : {type : tensor , shape : batch X (beam_num X beam_num) X (now_sentences_length+1), value : int}\n",
        "    sentences_expand = torch.cat((sentences_expand,topk_index),dim = -1)\n",
        "    '''\n",
        "    TO DO :\n",
        "    multipies p_sentences with the probability of top K consequence of each beam(total K beams) in n_beam_output\n",
        "    (so there are also K-square values).\n",
        "\n",
        "    The final step is to choose Top K consequence from K-square sentences by using p_sentences.\n",
        "    '''\n",
        "\n",
        "    # p_sentences : {type : tensor , shape : batch X (beam_num X beam_num),\n",
        "    # value : [P1,P2,P3...] X beam_num times (Pk is log_softmax probability)}\n",
        "    p_sentences = (p_sentences+topk_prob).view(batch,-1)\n",
        "    # p_sentences : {type : tensor , shape : batch X beam_num, value : log_softmax probability}\n",
        "    # p_index : {type : tensor , shape : batch X beam_num, value : int}\n",
        "    p_sentences, p_index = torch.topk(p_sentences, dim = 1, k = beam_num)\n",
        "    p_sentences = p_sentences.unsqueeze(-1)\n",
        "    # row : {type : tensor , shape : batch X 1, value : [[0],[1],[2],...]}\n",
        "    row = torch.tensor(range(batch)).unsqueeze(1)\n",
        "    # sentences : {type : tensor , shape : batch X beam_num X (now_sentences_length+1), value : log_softmax probability}\n",
        "    new_sentences = sentences_expand[row, p_index].view(batch*beam_num,-1)\n",
        "    sentences.data = new_sentences.data\n",
        "\n",
        "    del topk_prob,topk_index,sentences_expand,row,new_sentences\n",
        "    gc.collect()\n",
        "    return sentences,p_sentences\n",
        "\n",
        "def get_next_word(model,memory,out,out_probability,id,batch,beam_num,max_sentence_length,dictionary_length,padding_id):\n",
        "    # padding : {type : tensor , shape : (Batch X beam_num) X (max_sentence_length-(id+1)) ,value : int}\n",
        "    padding = torch.full(size = (batch*beam_num,max_sentence_length-(id+1)),fill_value = padding_id)\n",
        "    # out_padding : {type : tensor , shape : (Batch X beam_num) X max_sentence_length X 1,\n",
        "    # value : [[bos_id],[any_id],...[padding_id],....] X Batch}\n",
        "    out_padding = torch.cat((out,padding),dim = 1).unsqueeze(-1)\n",
        "    # tgt_padding : {type : tensor , shape : (Batch X beam_num) X max_sentence_length ,value: bool}\n",
        "    tgt_padding = (out_padding == padding_id).squeeze(-1)\n",
        "    # out_add : {type : tensor , shape : Batch X beam_num X dictionary_length ,value : int}\n",
        "    out_add = model.decoder(memory,out_padding,tgt_padding)[:,id,:].view(batch,beam_num,dictionary_length)\n",
        "\n",
        "    # out_n_beam : {type : tensor , shape : (Batch X beam_num) X (id+1) ,value : int}\n",
        "    # out_probability {type : tensor , shape : Batch X beam_num X 1 , value : log_softmax probability}\n",
        "    out , out_probability = beam_search_one_step(out,out_probability,out_add)\n",
        "\n",
        "    del padding,out_padding,tgt_padding,out_add\n",
        "    gc.collect()\n",
        "    return(out , out_probability)\n",
        "\n",
        "def apply_beam_search(model,src,src_mask,beam_num,\n",
        "               max_sentence_length,\n",
        "               dictionary_length,\n",
        "               bos_id = setting[\"tokenized_setting\"][\"bos_id\"],\n",
        "               padding_id = setting[\"tokenized_setting\"][\"pad_id\"]):\n",
        "    with torch.no_grad():\n",
        "      batch = src.size(0)\n",
        "      # out : {type : tensor , shape : Batch X 1 , value : bos_id}\n",
        "      out = torch.full(size = (batch,1),fill_value = bos_id)\n",
        "      # repeat : {type : tensor , shape : Batch X 1 ,value : beam_num}\n",
        "      # each row repeat beam_num times before concatenate\n",
        "      repeat = torch.full([batch],fill_value = beam_num)\n",
        "      # out_beam_expand : {type : tensor , shape : (Batch X beam_num) X 1 ,value : bos_id}\n",
        "      out_beam_expand = torch.repeat_interleave(out,repeat,dim=0)\n",
        "\n",
        "      del out\n",
        "\n",
        "      # out_probability {type : tensor , shape : Batch X beam_num X 1, value : 0.1}\n",
        "      out_probability = torch.full(size = (batch,beam_num,1),fill_value = 0.0)\n",
        "\n",
        "      # memory : {type : tensor , shape : Batch X max_sentence_length X encoder_output_dim ,value : arbitary float}\n",
        "      memory = model.encoder(src,src_mask)\n",
        "      # print(f\"memor.shape={memory.shape}\")\n",
        "      # memory_beam_expand : {type : tensor , shape : (Batch X n_beam) X max_sentence_length X encoder_output_dim ,value : float}\n",
        "      memory_beam_expand = torch.repeat_interleave(memory,repeat,dim=0)\n",
        "\n",
        "      del memory,src,src_mask,repeat\n",
        "\n",
        "      gc.collect()\n",
        "\n",
        "      for id in range(max_sentence_length-1):\n",
        "\n",
        "        # out_n_beam : {type : tensor , shape : (Batch X beam_num) X (id+1) ,value : int}\n",
        "        # out_probability {type : tensor , shape : Batch X beam_num X 1 , value : log_softmax probability}\n",
        "        new_out_beam_expand , new_out_probability = \\\n",
        "        get_next_word(model,memory_beam_expand,out_beam_expand,\n",
        "               out_probability,id,batch,beam_num,max_sentence_length,dictionary_length,padding_id)\n",
        "\n",
        "        del out_beam_expand,out_probability\n",
        "\n",
        "        out_beam_expand,out_probability = new_out_beam_expand , new_out_probability\n",
        "\n",
        "        del new_out_beam_expand , new_out_probability\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "      # out_beam_expand : {type : tensor , shape : Batch X beam_num X (max_sentence_length) ,value : 0 or 1}\n",
        "      print(out_beam_expand.shape)\n",
        "      out_beam_expand = out_beam_expand.view(batch,beam_num,max_sentence_length)\n",
        "      # max_probability : {type : tensor , shape :  Batch  X 1 ,value : int(max prob index)}\n",
        "      max_probability = torch.argmax(input = out_probability,dim = 1)\n",
        "      # max_probability_expand : {type : tensor , shape :  Batch  X 1 X max_sentence_length ,\n",
        "      # value : [[A,A,A....],[B,B,B...],...](A,B are int)}\n",
        "      max_probability_expand = max_probability.expand(batch, max_sentence_length).unsqueeze(1)\n",
        "      print(max_probability_expand.shape)\n",
        "      print(out_beam_expand[0])\n",
        "      # out : {type : tensor , shape :  Batch X max_sentence_length ,value : [[int,int,...],[int,int...],...]}\n",
        "      out =  torch.gather(input = out_beam_expand ,dim = 1, index = max_probability_expand)\n",
        "      print(out.shape)\n",
        "    return out"
      ],
      "metadata": {
        "id": "sz_zI5fDh9UU"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #test\n",
        "# batch = 3\n",
        "# beam_num = 2\n",
        "# sentences = torch.randint(0,8000,(batch*beam_num,5))\n",
        "# p_sentences = torch.log(torch.rand((batch , beam_num , 1)))\n",
        "# n_beam_output = torch.rand((batch , beam_num , 8000))\n",
        "# print(sentences,p_sentences,n_beam_output)\n",
        "# print(beam_search_one_step(sentences,p_sentences,n_beam_output))\n",
        "# repeat = torch.full([beam_num],fill_value = beam_num)\n",
        "# sentences_expand = torch.repeat_interleave(sentences.view(batch,beam_num,-1),repeat,dim=1)\n",
        "# print(sentences_expand,sentences_expand.shape)\n",
        "\n",
        "# topk_prob = torch.rand((batch , beam_num , beam_num))\n",
        "# print(p_sentences)\n",
        "# print(topk_prob)\n",
        "# prob = (p_sentences*topk_prob).view(batch,-1)\n",
        "# print(prob,prob.shape)\n",
        "# row = torch.tensor(range(batch))\n",
        "# id = torch.tensor([[[0],[1]],[[1],[0]],[[0],[1]]]).unsqueeze(1)\n",
        "\n",
        "# print(row)\n",
        "# print(id.shape)"
      ],
      "metadata": {
        "id": "HPvbrGxuCiUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = apply_beam_search(model,src,src_mask,2,\n",
        "               400,\n",
        "               8000,\n",
        "              )\n",
        "print(test)"
      ],
      "metadata": {
        "id": "4p2rZAaRWi1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee65a76e-2132-49fb-c5cc-aa6ae0add8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "40\n",
            "80\n",
            "120\n",
            "160\n",
            "200\n",
            "240\n",
            "280\n",
            "320\n",
            "360\n",
            "torch.Size([64, 400])\n",
            "torch.Size([32, 1, 400])\n",
            "tensor([[   2, 2163, 5610,  711, 6892, 2027,  421, 3564,  484, 5813, 4662, 5009,\n",
            "         6038,  593, 4666, 1090, 2486, 7320, 1225, 1674, 5813, 5233,  833, 3135,\n",
            "         7320, 1225,  407, 6046, 3405,  593, 3662, 2358, 4690, 4455, 6470, 1714,\n",
            "          760, 1621, 6107, 5813, 5233, 3904, 1094, 6973, 5745, 5080, 3255,  889,\n",
            "         7816, 5495, 5612, 4476, 3075,  833, 7635, 5073, 6318, 4940, 3793,  629,\n",
            "         4571, 4576, 3387, 5934, 3865, 3037, 2772, 7263,  325, 1713, 7114, 5249,\n",
            "         3386, 2769, 4914, 1237, 5156, 5392, 1318,  593, 5088, 6283, 6383, 3807,\n",
            "         3055, 3231, 4020, 2741,   48, 7682, 2409, 1491, 7816, 3765, 4998, 3476,\n",
            "          787,  593, 1980, 3795, 5267, 5053, 2754, 1621, 2650, 3045, 3184, 3494,\n",
            "         3694, 6659, 4690, 5892, 5228,  919, 4780, 4327, 1999, 7540, 2607, 4820,\n",
            "          665, 3347, 7635, 7704,  469,  884,  686, 1209, 1733,  117, 4144, 6973,\n",
            "         7320, 1458, 2835, 7881, 3075,  177, 4868, 4144, 3956, 3410, 2994, 6272,\n",
            "         6267, 5872, 4563, 1997, 1382, 6551, 7970, 3005, 5009, 6038,  456, 6809,\n",
            "         3452,  593,  117, 2441, 1382, 6222, 2542,   80, 5872, 5544, 2792, 5050,\n",
            "         6794, 2196, 5799, 2905, 4917, 5662, 3452, 1383, 6827, 2367,  117, 5268,\n",
            "          760, 4443, 7207, 3452,  593, 5109, 2062, 1759, 5469, 5799,  628, 4609,\n",
            "         4649, 5601, 4554, 2891, 7753, 1050, 5892, 1842, 6816, 4887,  593, 5532,\n",
            "         4609, 4649, 7616, 1594, 7874, 7879, 1094, 6973, 5745, 7635, 5842, 3795,\n",
            "         7164, 1463, 7468,  547,  593, 4651, 4957, 7682, 5982, 6587, 1594, 5593,\n",
            "         6502, 7705, 5088, 5540, 7443, 6939, 6390, 1237, 2033,  628, 4609, 4649,\n",
            "         1844,  628, 5233, 3904, 1094, 6973, 1603, 5766, 1179, 5813, 5233, 3904,\n",
            "         7879, 1311, 7957, 2470, 6963, 1584, 1274, 7042, 7605, 1844,  628, 4609,\n",
            "         4649, 1594, 1503, 7968,  550, 2131, 2853, 1596, 6150, 1759, 4036, 4371,\n",
            "         2511, 2745, 1409, 6809, 3452,  593, 5532, 4609, 3662, 1137, 7962, 3037,\n",
            "         3255, 5130,  136, 6533, 7665, 6935, 3083, 7661, 4554, 7350, 4609, 4649,\n",
            "         6107, 5813, 5233,  833, 4780, 4327, 5796, 4064, 7665, 1997, 5214, 5115,\n",
            "          299, 1463,  558, 1896, 5745, 7635, 5842, 5745,   71, 1371, 4410, 6489,\n",
            "         4949, 3183, 5765, 2712, 1705, 1990,  593, 5532, 4609, 5591, 1708, 4977,\n",
            "         4020, 2596,  142,  986, 2825, 5407, 4562, 5742, 7858, 1382, 1295, 7913,\n",
            "         2625, 6013,  646, 3096, 7506, 5813, 5233, 3904, 7879, 1094, 6973, 5745,\n",
            "         2905, 3772, 5228, 4595, 4400,  833, 4780, 2483, 3349, 6268, 1209, 3963,\n",
            "         4562, 5742, 7665, 1997, 5646, 4983,  248, 4917, 5662, 3452,  593, 5532,\n",
            "         4609, 4649, 3561, 4313, 3255,  437, 7665, 1997, 3156,  633, 2185, 3386,\n",
            "         1254, 2637, 6044, 6787],\n",
            "        [   2, 2163, 5610,  711, 6892, 2027,  421, 3564,  484, 5813, 4662, 5009,\n",
            "         6038,  593, 4666, 1090, 2486, 7320, 1225, 1674, 5813, 5233,  833, 3135,\n",
            "         7320, 1225,  407, 6046, 3405,  593, 3662, 2358, 4690, 4455, 6470, 1714,\n",
            "          760, 1621, 6107, 5813, 5233, 3904, 1094, 6973, 5745, 5080, 3255,  889,\n",
            "         7816, 5495, 5612, 4476, 3075,  833, 7635, 5073, 6318, 4940, 3793,  629,\n",
            "         4571, 4576, 3387, 5934, 3865, 3037, 2772, 7263,  325, 1713, 7114, 5249,\n",
            "         3386, 2769, 4914, 1237, 5156, 5392, 1318,  593, 5088, 6283, 6383, 3807,\n",
            "         3055, 3231, 4020, 2741,   48, 7682, 2409, 1491, 7816, 3765, 4998, 3476,\n",
            "          787,  593, 1980, 3795, 5267, 5053, 2754, 1621, 2650, 3045, 3184, 3494,\n",
            "         3694, 6659, 4690, 5892, 5228,  919, 4780, 4327, 1999, 7540, 2607, 4820,\n",
            "          665, 3347, 7635, 7704,  469,  884,  686, 1209, 1733,  117, 4144, 6973,\n",
            "         7320, 1458, 2835, 7881, 3075,  177, 4868, 4144, 3956, 3410, 2994, 6272,\n",
            "         6267, 5872, 4563, 1997, 1382, 6551, 7970, 3005, 5009, 6038,  456, 6809,\n",
            "         3452,  593,  117, 2441, 1382, 6222, 2542,   80, 5872, 5544, 2792, 5050,\n",
            "         6794, 2196, 5799, 2905, 4917, 5662, 3452, 1383, 6827, 2367,  117, 5268,\n",
            "          760, 4443, 7207, 3452,  593, 5109, 2062, 1759, 5469, 5799,  628, 4609,\n",
            "         4649, 5601, 4554, 2891, 7753, 1050, 5892, 1842, 6816, 4887,  593, 5532,\n",
            "         4609, 4649, 7616, 1594, 7874, 7879, 1094, 6973, 5745, 7635, 5842, 3795,\n",
            "         7164, 1463, 7468,  547,  593, 4651, 4957, 7682, 5982, 6587, 1594, 5593,\n",
            "         6502, 7705, 5088, 5540, 7443, 6939, 6390, 1237, 2033,  628, 4609, 4649,\n",
            "         1844,  628, 5233, 3904, 1094, 6973, 1603, 5766, 1179, 5813, 5233, 3904,\n",
            "         7879, 1311, 7957, 2470, 6963, 1584, 1274, 7042, 7605, 1844,  628, 4609,\n",
            "         4649, 1594, 1503, 7968,  550, 2131, 2853, 1596, 6150, 1759, 4036, 4371,\n",
            "         2511, 2745, 1409, 6809, 3452,  593, 5532, 4609, 3662, 1137, 7962, 3037,\n",
            "         3255, 5130,  136, 6533, 7665, 6935, 3083, 7661, 4554, 7350, 4609, 4649,\n",
            "         6107, 5813, 5233,  833, 4780, 4327, 5796, 4064, 7665, 1997, 5214, 5115,\n",
            "          299, 1463,  558, 1896, 5745, 7635, 5842, 5745,   71, 1371, 4410, 6489,\n",
            "         4949, 3183, 5765, 2712, 1705, 1990,  593, 5532, 4609, 5591, 1708, 4977,\n",
            "         4020, 2596,  142,  986, 2825, 5407, 4562, 5742, 7858, 1382, 1295, 7913,\n",
            "         2625, 6013,  646, 3096, 7506, 5813, 5233, 3904, 7879, 1094, 6973, 5745,\n",
            "         2905, 3772, 5228, 4595, 4400,  833, 4780, 2483, 3349, 6268, 1209, 3963,\n",
            "         4562, 5742, 7665, 1997, 5646, 4983,  248, 4917, 5662, 3452,  593, 5532,\n",
            "         4609, 4649, 3561, 4313, 3255,  437, 7665, 1997, 3156,  633, 2185, 3386,\n",
            "         1254, 2637, 6044, 2895]])\n",
            "torch.Size([32, 1, 400])\n",
            "tensor([[[   2, 2163, 5610,  ..., 2637, 6044, 6787]],\n",
            "\n",
            "        [[   2, 6280, 6963,  ..., 5495, 4629, 2860]],\n",
            "\n",
            "        [[   2, 1272, 7547,  ..., 5842, 3795, 5353]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[   2, 6242, 4649,  ...,  986, 1728, 6398]],\n",
            "\n",
            "        [[   2, 6175,  593,  ..., 6787, 1877, 6780]],\n",
            "\n",
            "        [[   2, 6175, 4063,  ...,  986, 7023, 2905]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.tensor([[1, 2, 3],[4,5,6]])\n",
        "# y = torch.full([2],fill_value = 3)\n",
        "\n",
        "# print(y)\n",
        "# torch.repeat_interleave(x, y, dim=0)\n",
        "\n",
        "\n",
        "x = torch.tensor([[2,5,7,3],[3,4,6,5],[5,2,1,8],[6,4,3,9],[1,2,9,7],[3,4,8,2]])\n",
        "p = torch.tensor([[0.9],[0.8],[0.4],[0.2],[0.9],[0.7]])\n",
        "x = x.view(3,2,4)\n",
        "p = p.view(3,2,1)\n",
        "print(p.shape)\n",
        "max_p = torch.argmax(input = p,dim = 1)\n",
        "print(max_p.shape)\n",
        "max_p_expand = max_p.expand(max_p.size(0), x.size(-1)).unsqueeze(1)\n",
        "print(max_p_expand.shape,x.shape)\n",
        "print(max_p_expand,x)\n",
        "out =  torch.gather(input = x ,dim = 1, index = max_p_expand)\n",
        "out\n",
        "# t = torch.tensor([[1, 2], [3, 4]])\n",
        "# torch.gather(t, 1, torch.tensor([[0, 0, 0], [1, 0, 0]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hia5PH9fNsE",
        "outputId": "503c9d90-eb0d-4ad4-e3cf-309ce5910e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2, 1])\n",
            "torch.Size([3, 1])\n",
            "torch.Size([3, 1, 4]) torch.Size([3, 2, 4])\n",
            "tensor([[[0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0]]]) tensor([[[2, 5, 7, 3],\n",
            "         [3, 4, 6, 5]],\n",
            "\n",
            "        [[5, 2, 1, 8],\n",
            "         [6, 4, 3, 9]],\n",
            "\n",
            "        [[1, 2, 9, 7],\n",
            "         [3, 4, 8, 2]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[2, 5, 7, 3]],\n",
              "\n",
              "        [[5, 2, 1, 8]],\n",
              "\n",
              "        [[1, 2, 9, 7]]])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.FloatTensor([[[1,1,1],[2,2,2]],[[9,9,9],[5,5,5]]])\n",
        "b = torch.LongTensor([1,0])\n",
        "print(a.shape)\n",
        "R = a.shape[0]\n",
        "C = a.shape[2]\n",
        "\n",
        "idx = b.unsqueeze(dim=1).repeat(1, C).view(R, 1, C)\n",
        "print(idx)\n",
        "print(idx.shape)\n",
        "torch.gather(a, 1, idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LKopScu639x",
        "outputId": "9aa4da24-5fe5-442c-b896-ad9619bf8cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 3])\n",
            "tensor([[[1, 1, 1]],\n",
            "\n",
            "        [[0, 0, 0]]])\n",
            "torch.Size([2, 1, 3])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[2., 2., 2.]],\n",
              "\n",
              "        [[9., 9., 9.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label_onehot = F.one_hot(tgt,8000).float().squeeze()\n",
        "# print((tgt<4000).expand(32,400,2)[0])"
      ],
      "metadata": {
        "id": "3iDQuN99J7IV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}