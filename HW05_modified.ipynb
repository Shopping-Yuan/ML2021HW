{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB/UjZyRTqobOqsjLlOX2V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW05_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "part 1 load file (finish)\n",
        "part 2 clean data (finish on 12/5)\n",
        "part 3 tokenize : sentencepiece (finish on 12/6)\n",
        "part 4 build model encoder decoder (finish on 12/7)\n",
        "part 5 beam search (finish on 12/8)\n",
        "part 6 label smooth (finish on 12/9)\n",
        "\n",
        "part 7 test (finish on 12/10)\n"
      ],
      "metadata": {
        "id": "WTv4XN2qB_fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "# !pip install --upgrade jupyter ipywidgets\n",
        "# !git clone https://github.com/pytorch/fairseq.git\n",
        "# !cd fairseq && git checkout 9a1c497\n",
        "# !pip install --upgrade ./fairseq/"
      ],
      "metadata": {
        "id": "hwhMkDjdvARr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# import pdb\n",
        "# import pprint\n",
        "# import logging\n",
        "# import os\n",
        "# import random\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.utils import data\n",
        "# import numpy as np\n",
        "# import tqdm.auto as tqdm\n",
        "# from pathlib import Path\n",
        "# from argparse import Namespace\n",
        "# #from fairseq import utils\n",
        "\n",
        "# import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FgIV_VBavJ2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVHTBvhfZyZY"
      },
      "outputs": [],
      "source": [
        "# seed = 1\n",
        "# random.seed(seed)\n",
        "# torch.manual_seed(seed)\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.manual_seed(seed)\n",
        "#     torch.cuda.manual_seed_all(seed)\n",
        "# np.random.seed(seed)\n",
        "# torch.backends.cudnn.benchmark = False\n",
        "# torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "shutil.copyfile('/content/drive/MyDrive/ted2020.tgz','/content/ted2020.tgz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "X23RaTWe9hoU",
        "outputId": "e2e9f156-a6a4-40f4-9982-d0dff954248c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/ted2020.tgz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "rawdata_file = \"/content/ted2020.tgz\"\n",
        "unzip_path = \"/content/train_dev/\"\n",
        "# open file\n",
        "file = tarfile.open(rawdata_file)\n",
        "\n",
        "# extracting file\n",
        "file.extractall(unzip_path)\n",
        "\n",
        "file.close()\n"
      ],
      "metadata": {
        "id": "2GhVMRgifeKi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_lang = \"raw.en\"\n",
        "tgt_lang = \"raw.zh\"\n",
        "\n",
        "src_path = f\"{unzip_path}{src_lang}\"\n",
        "tgt_path = f\"{unzip_path}{tgt_lang}\"\n",
        "print(tgt_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH2bCPFbvhK2",
        "outputId": "84e01c18-d498-4ae1-d94b-c26398eaee74"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/train_dev/raw.zh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for path in [src_path,tgt_path]:\n",
        "  with open(path, \"r\") as f:\n",
        "  #  data = f.readlines()\n",
        "  #  print(type(data),data[0:5])#<class 'list'> ['Thank you so much, Chris.\\n', \"And it'......on.\\n']\n",
        "    data = f.read().splitlines()\n",
        "    print(type(data),data[0:5],len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjIUpASA076k",
        "outputId": "d9abdaf0-5ef6-4ea5-95b3-ca10b35aa781"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> ['Thank you so much, Chris.', \"And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\", 'I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.', 'And I say that sincerely, partly because  I need that.', 'Put yourselves in my position.'] 394066\n",
            "<class 'list'> ['非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台', '真是一大榮幸。我非常感激。', '這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。', '我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!', '請你們設身處地為我想一想！'] 394066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "print(ord(\"，\"))\n",
        "print(chr(65292))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhEDa7SN6us_",
        "outputId": "0b69e44b-af47-4771-e0d8-0eadfd4a72b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65292\n",
            "，\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "def to_halfwidth(string):\n",
        "  return \"\".join(unicodedata.normalize('NFKC',letter) for letter in string)\n",
        "print(to_halfwidth(\"ＡＳＤＦＦＦＦＱ\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFgPn-1i1EZR",
        "outputId": "44bea53c-3682-4572-e76d-c4a3a78c2fa9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASDFFFFQ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "def clean_s_zh(s):\n",
        "    s = to_halfwidth(s)\n",
        "    # step 1 : delete — _\n",
        "    delete = \" _()[]\"\n",
        "    delete_rules = s.maketrans(\"\",\"\",delete)\n",
        "    s = s.translate(delete_rules)\n",
        "\n",
        "    # step 2 : replace “” with \"\"\n",
        "    to_be_replace = '“”'\n",
        "    replace = '\"\"'\n",
        "    replace_dict = dict(zip(to_be_replace,replace))\n",
        "    # print(replace_dict.items()) : dict_items([('“', '\"'), ('”', '\"')])\n",
        "\n",
        "    # step 3 : add **END** before and after punctuation\n",
        "\n",
        "    \"\"\"\n",
        "    The number of sentences in one line may be different\n",
        "    in line pairs of source and target set.\n",
        "    so I try to use \"。!?\" or \".!?\" to split sentences.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "#    punctuation = \"。,;!?()\\\"~「」\"\n",
        "    punctuation = \"。!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    zh_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return zh_list\n",
        "\n",
        "def clean_s_en(s):\n",
        "    s = to_halfwidth(s)\n",
        "\n",
        "    replace_dict = {}\n",
        "\n",
        "    delete = \"-()[]\"\n",
        "    for char in delete:\n",
        "      replace_dict[char] = \"\"\n",
        "\n",
        "    punctuation = \"!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "    s = pattern.sub(\"**END**\",s)\n",
        "\n",
        "    en_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return en_list"
      ],
      "metadata": {
        "id": "5dwWeVz-sPI2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "result = pattern.sub(\"**END**\",\"There are many people in U.S. w.r.t. in Taiwan.Thank you.\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt8FnBUBrSZb",
        "outputId": "161b4c84-c39b-4f6d-a792-3eac1122e100"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are many people in U.S. w.r.t. in Taiwan**END**Thank you**END**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_file(path,fuction):\n",
        "  with open(path, \"r\") as f:\n",
        "    data = f.read()\n",
        "    return(fuction(data))\n",
        "src_list = load_file(src_path,clean_s_en)\n",
        "tgt_list = load_file(tgt_path,clean_s_zh)"
      ],
      "metadata": {
        "id": "yeSGu6mwmbzh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(tgt_path, \"r\") as f:\n",
        "    data = f.readlines()\n",
        "    print(data[17])\n",
        "print(tgt_list[17])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXOFN_BQN7ru",
        "outputId": "901d04bf-4bd2-4a95-fcdc-d29cd54b85b0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我們下了交流道後就開始找餐廳 — 我們發現了一家 Shoney' s餐館。\n",
            "\n",
            "我們下了交流道後就開始找餐廳—我們發現了一家Shoney's餐館。**END**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # \"\"\"\n",
        "    # Please ensure that space char \" \" has bee remove in previous step ,\n",
        "    # it's the reason s must be translated twice.\n",
        "\n",
        "    # \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    # print(len(replace_dict.items())) : 13\n",
        "#    punctuation = \"(.,;!?()\\\"])\"\n"
      ],
      "metadata": {
        "id": "YH2tx-ShwhAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_by_END(s):\n",
        "    return(s.strip(\"**END**\").split(\"**END**\"))\n",
        "\n",
        "def devide_en_again(s,punctuation = \":;\"):\n",
        "    replace_dict = {}\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules_src = s.maketrans(replace_dict)\n",
        "    new_s = divide_by_END(s.translate(replace_rules_src))\n",
        "    return new_s"
      ],
      "metadata": {
        "id": "0RRwmoBw59gS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_data_pairs(src_list,tgt_list):\n",
        "    index = 0\n",
        "    new_src_list = []\n",
        "    new_tgt_list = []\n",
        "\n",
        "    same = 0\n",
        "    add_next = 0\n",
        "    split_again = 0\n",
        "    not_use = 0\n",
        "\n",
        "    while(index < len(src_list)):\n",
        "\n",
        "      src = divide_by_END(src_list[index])\n",
        "      tgt = divide_by_END(tgt_list[index])\n",
        "      # case 1 : src is as long as tgt , finished.\n",
        "      if len(src) == len(tgt):\n",
        "        new_src_list += src\n",
        "        new_tgt_list += tgt\n",
        "        same += 1\n",
        "        index += 1\n",
        "\n",
        "      else :\n",
        "        # if it is not the last one : both src and tgt add next sentence\n",
        "        if index != len(src_list)-1:\n",
        "          src_add_next = divide_by_END(src_list[index] + src_list[index+1])\n",
        "          tgt_add_next = divide_by_END(tgt_list[index] + tgt_list[index+1])\n",
        "          # case 2 : src_add_next is as long as tgt_add_next , finished.\n",
        "          if len(src_add_next) == len(tgt_add_next):\n",
        "            new_src_list += src_add_next\n",
        "            new_tgt_list += tgt_add_next\n",
        "            add_next += 2\n",
        "            index += 2\n",
        "\n",
        "          # using new punctuation to divide tgt (english) sentence.\n",
        "          else :\n",
        "            src_add_next = devide_en_again(src_list[index] + src_list[index+1])\n",
        "            # case 3 : src_add_next is as long as tgt_add_next , finished.\n",
        "            if len(src_add_next) == len(tgt_add_next):\n",
        "              new_src_list += src_add_next\n",
        "              new_tgt_list += tgt_add_next\n",
        "              split_again +=2\n",
        "              index += 2\n",
        "\n",
        "            # case 4 : sentence will not be used.\n",
        "            else :\n",
        "              not_use += 1\n",
        "              # if to_do == 1 :\n",
        "              #   print(index,src_add_next,tgt_add_next,len(src_add_next),len(tgt_add_next))\n",
        "              index += 1\n",
        "\n",
        "        # if it is the last one\n",
        "        else :\n",
        "          not_use += 1\n",
        "          index += 1\n",
        "\n",
        "    print(index,same,add_next,split_again,not_use)\n",
        "\n",
        "    return(new_src_list,new_tgt_list)\n",
        "dataset = check_data_pairs(src_list,tgt_list)\n",
        "print(dataset[0][0],dataset[1][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVYvYCpuDQDa",
        "outputId": "1abb7694-8a02-4613-b56b-a150d4728f03"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "394066 350481 12188 2972 28425\n",
            "Thank you so much, Chris 非常謝謝你,克里斯。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/train_dev/data.txt\"\n",
        "label_path = \"/content/train_dev/label.txt\"\n",
        "with open(data_path, \"w\") as f:\n",
        "  f.write(\"\\n\".join(dataset[0]))\n",
        "with open(label_path, \"w\") as f:\n",
        "  f.write(\"\\n\".join(dataset[1]))"
      ],
      "metadata": {
        "id": "DNf0E5FDPsj8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_path, \"r\") as f:\n",
        "    data = f.readlines()\n",
        "    print(data[17])\n",
        "with open(label_path, \"r\") as f:\n",
        "    data = f.readlines()\n",
        "    print(data[17])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_68jlIE2RCzq",
        "outputId": "dcae1ec4-f00f-4f0c-9351-307007f722a0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We got off the exit, we found a Shoney's restaurant\n",
            "\n",
            "我們下了交流道後就開始找餐廳—我們發現了一家Shoney's餐館。\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ti-UGuHNhh",
        "outputId": "a54d86ff-a103-4d96-8f21-77b5dff7f0e7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "data_info = {\n",
        "    \"data\":{\"path\":data_path,\"lang\":\"en\"},\n",
        "    \"label\":{\"path\":label_path,\"lang\":\"zh\"},\n",
        "}\n",
        "vocab_size = 8000\n",
        "def tokenized(data_or_label,data_info=data_info):\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      input=data_info[data_or_label][\"path\"],\n",
        "      model_prefix=f'spm{vocab_size}_{data_info[data_or_label][\"lang\"]}',\n",
        "      vocab_size=vocab_size,\n",
        "      character_coverage=1,\n",
        "      model_type='unigram', # 'bpe' works as well\n",
        "      input_sentence_size=1e6,\n",
        "      shuffle_input_sentence=True,\n",
        "      normalization_rule_name='nmt_nfkc_cf',\n",
        "  )"
      ],
      "metadata": {
        "id": "LoNi7CeA3P9G"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_info[\"data\"][\"path\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AgJQjk7JgTNi",
        "outputId": "9f3087e7-c7fc-4ade-a927-ce6e6bb201c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/train_dev/data.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized(\"data\")\n",
        "tokenized(\"label\")"
      ],
      "metadata": {
        "id": "dJPM2p4QfG33"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --model_type=%s --character_coverage=%s ' \\\n",
        "#                      '--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 '\n",
        "# cmd = input_argument % (\"A\", \"B\", 8000, \"C\", 1)\n",
        "# print(cmd)\n",
        "#spm.SentencePieceTrainer(cmd)"
      ],
      "metadata": {
        "id": "EViFhIw_ciUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import torch.utils.data as data\n",
        "def data_set_preparing():\n",
        "    src_set = []\n",
        "    tgt_set = []\n",
        "    s_en = spm.SentencePieceProcessor(model_file=\"/content/spm8000_en.model\")\n",
        "    s_zh = spm.SentencePieceProcessor(model_file=\"/content/spm8000_zh.model\")\n",
        "    with open(\"/content/train_dev/data.txt\",\"r\") as in_f :\n",
        "      for line in in_f:\n",
        "        src_set.append(line)\n",
        "    with open(\"/content/train_dev/label.txt\",\"r\") as in_f :\n",
        "      for line in in_f:\n",
        "        tgt_set.append(line)\n",
        "    train_set, valid_set = data.random_split(list(zip(src_set,tgt_set)),[0.9,0.1])\n",
        "    # print(\" \".join(str(x) for x in s_en.encode(train_set[0][0], out_type=int))+\"\\n\")\n",
        "\n",
        "    with open(\"/content/train_dev/tokenized_train_data.txt\", 'w') as out_f:\n",
        "      for line_pair in train_set:\n",
        "        out_f.write(\" \".join(str(x) for x in s_en.encode(line_pair[0], out_type=int))+\"\\n\")\n",
        "    with open(\"/content/train_dev/tokenized_valid_data.txt\", 'w') as out_f:\n",
        "      for line_pair in valid_set:\n",
        "        out_f.write(\" \".join(str(x) for x in s_en.encode(line_pair[0], out_type=int))+\"\\n\")\n",
        "    with open(\"/content/train_dev/tokenized_train_label.txt\", 'w') as out_f:\n",
        "      for line_pair in train_set:\n",
        "        out_f.write(\" \".join(str(x) for x in s_zh.encode(line_pair[1], out_type=int))+\"\\n\")\n",
        "    with open(\"/content/train_dev/tokenized_valid_label.txt\", 'w') as out_f:\n",
        "      for line_pair in valid_set:\n",
        "        out_f.write(\" \".join(str(x) for x in s_zh.encode(line_pair[1], out_type=int))+\"\\n\")"
      ],
      "metadata": {
        "id": "sX_7znJc170w"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set_preparing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAhVfoAh4a-Y",
        "outputId": "b11a865f-f628-4197-9fdf-9afd092f2b85"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25 397 3 97 240 14 210 35\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spm_model = spm.SentencePieceProcessor(model_file=f'spm{vocab_size}')\n",
        "in_tag = {\n",
        "    'train': 'train.clean',\n",
        "    'valid': 'valid.clean',\n",
        "    'test': 'test.raw.clean',\n",
        "}\n",
        "\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        out_path = prefix/f'{split}.{lang}'\n",
        "\n",
        "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)"
      ],
      "metadata": {
        "id": "txsW_v8vHvoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
        "# !head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5\n",
        "# binpath = Path('./DATA/data-bin', dataset_name)\n",
        "# if binpath.exists():\n",
        "#     print(binpath, \"exists, will not overwrite!\")\n",
        "# else:\n",
        "#     !python -m fairseq_cli.preprocess \\\n",
        "#         --source-lang {src_lang}\\\n",
        "#         --target-lang {tgt_lang}\\\n",
        "#         --trainpref {prefix/'train'}\\\n",
        "#         --validpref {prefix/'valid'}\\\n",
        "#         --testpref {prefix/'test'}\\\n",
        "#         --destdir {binpath}\\\n",
        "#         --joined-dictionary\\\n",
        "#         --workers 2\n"
      ],
      "metadata": {
        "id": "xIWAdyf6h0bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "#     batch_iterator = task.get_batch_iterator(\n",
        "#         dataset=task.dataset(split),\n",
        "#         max_tokens=max_tokens,\n",
        "#         max_sentences=None,\n",
        "#         max_positions=utils.resolve_max_positions(\n",
        "#             task.max_positions(),\n",
        "#             max_tokens,\n",
        "#         ),\n",
        "#         ignore_invalid_inputs=True,\n",
        "#         seed=seed,\n",
        "#         num_workers=num_workers,\n",
        "#         epoch=epoch,\n",
        "#         disable_iterator_cache=not cached,\n",
        "#         # Set this to False to speed up. However, if set to False, changing max_tokens beyond\n",
        "#         # first call of this method has no effect.\n",
        "#     )\n",
        "#     return batch_iterator\n",
        "\n",
        "# demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "# demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "# sample = next(demo_iter)\n",
        "# sample"
      ],
      "metadata": {
        "id": "dbfi-DvrlDhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch = {\n",
        "#   \"id\": id, # 每個 example 的 id\n",
        "#   \"nsentences\": len(samples), # batch size 句子數\n",
        "#   \"ntokens\": ntokens, # batch size 字數\n",
        "#   \"net_input\": {\n",
        "#       \"src_tokens\": src_tokens, # 來源語言的序列\n",
        "#       \"src_lengths\": src_lengths, # 每句話沒有 pad 過的長度\n",
        "#       \"prev_output_tokens\": prev_output_tokens, # 上面提到右 shift 一格後的目標序列\n",
        "#   },\n",
        "#   \"target\": target, # 目標序列\n",
        "# }"
      ],
      "metadata": {
        "id": "0VgNDQMVlGLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cuda_env = utils.CudaEnvironment()\n",
        "# utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "cYgg7zwHk1jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # HINT: transformer 架構\n",
        "# # from fairseq.models.transformer import (\n",
        "# #     TransformerEncoder,\n",
        "# #     TransformerDecoder,\n",
        "# # )\n",
        "\n",
        "# def build_model(args, task):\n",
        "#     \"\"\" 按照參數設定建置模型 \"\"\"\n",
        "#     src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "#     # 詞嵌入\n",
        "#     encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "#     decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "\n",
        "#     # 編碼器與解碼器\n",
        "#     # TODO: 替換成 TransformerEncoder 和 TransformerDecoder\n",
        "#     encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
        "#     decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "#     # 序列到序列模型\n",
        "#     model = Seq2Seq(args, encoder, decoder)\n",
        "\n",
        "#     # 序列到序列模型的初始化很重要 需要特別處理\n",
        "#     def init_params(module):\n",
        "#         from fairseq.modules import MultiheadAttention\n",
        "#         if isinstance(module, nn.Linear):\n",
        "#             module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "#         if isinstance(module, nn.Embedding):\n",
        "#             module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             if module.padding_idx is not None:\n",
        "#                 module.weight.data[module.padding_idx].zero_()\n",
        "#         if isinstance(module, MultiheadAttention):\n",
        "#             module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#         if isinstance(module, nn.RNNBase):\n",
        "#             for name, param in module.named_parameters():\n",
        "#                 if \"weight\" in name or \"bias\" in name:\n",
        "#                     param.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "#     # 初始化模型\n",
        "#     model.apply(init_params)\n",
        "#     return model"
      ],
      "metadata": {
        "id": "-Zy7vAVXlTAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "#     def __init__(self, args, encoder, decoder):\n",
        "#         super().__init__(encoder, decoder)\n",
        "#         self.args = args\n",
        "\n",
        "#     def forward(\n",
        "#         self,\n",
        "#         src_tokens,\n",
        "#         src_lengths,\n",
        "#         prev_output_tokens,\n",
        "#         return_all_hiddens: bool = True,\n",
        "#     ):\n",
        "#         \"\"\"\n",
        "#         Run the forward pass for an encoder-decoder model.\n",
        "#         \"\"\"\n",
        "#         encoder_out = self.encoder(\n",
        "#             src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "#         )\n",
        "#         logits, extra = self.decoder(\n",
        "#             prev_output_tokens,\n",
        "#             encoder_out=encoder_out,\n",
        "#             src_lengths=src_lengths,\n",
        "#             return_all_hiddens=return_all_hiddens,\n",
        "#         )\n",
        "#         return logits, extra"
      ],
      "metadata": {
        "id": "hBNhGpgS08gI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}