{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# You may choose where to download the data.\n",
    "\n",
    "# Google Drive\n",
    "#!gdown --id '1awF7pZ9Dz7X1jn1_QAiKN-_v56veCEKy' --output food-11.zip\n",
    "\n",
    "# Dropbox\n",
    "# !wget https://www.dropbox.com/s/m9q6273jl3djall/food-11.zip -O food-11.zip\n",
    "\n",
    "# MEGA\n",
    "# !sudo apt install megatools\n",
    "# !megadl \"https://mega.nz/#!zt1TTIhK!ZuMbg5ZjGWzWX1I6nEUbfjMZgCmAgeqJlwDkqdIryfg\"\n",
    "\n",
    "# Unzip the dataset.\n",
    "# This may take some time.\n",
    "#!unzip -q food-11.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#import pytorch\n",
    "import torch\n",
    "\n",
    "# torch.backends.cudnn: set CNN algorithmtorch.backends.cudnn\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# get the current available device ('cpu' or 'cuda')\n",
    "def get_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = get_device()\n",
    "print(torch.cuda.is_available())\n",
    "#set random variable\n",
    "import numpy as np\n",
    "myseed = 1\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# m = nn.Conv2d(1, 1, 2 , stride=2, padding=(9,9),bias = False)\n",
    "# test = torch.tensor([[[1.0,2.0,3.0],[4.0,5.0,6.0],[7.0,8.0,9.0]]])\n",
    "# print(test)\n",
    "# print(m(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# with Image.open(\"C:/Users/user/Pictures\\Screenshots/資格證.jpg\") as im:\n",
    "#     im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# x = torch.FloatTensor([[2,7,1],[3,4,9],[5,6,8]])\n",
    "# y = x[x> 2]\n",
    "# print(x,y)\n",
    "# print(x.argmax(dim = 0))\n",
    "# print(x.argmax(dim = 1))\n",
    "# print(x.argmax(dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from PIL import Image\n",
    "def get_tfm(mode):\n",
    "    if mode in [\"val\",\"test\"]:\n",
    "      tfm = transforms.Compose([\n",
    "      transforms.Resize((128, 128)),\n",
    "      transforms.ToTensor(),\n",
    "      ])\n",
    "    elif mode in [\"train_unlabeled\",\"train\"]:\n",
    "       tfm = transforms.Compose([\n",
    "      transforms.Resize((128, 128)),\n",
    "      transforms.ToTensor(),\n",
    "      ])\n",
    "    return tfm\n",
    "def food_f(mode):\n",
    "    dataset = DatasetFolder(data_info[mode][\"path\"], loader=lambda x: Image.open(x), extensions=\"jpg\", transform=get_tfm(mode))\n",
    "    print('Size of {} data: {}'.format(mode,len(list(dataset))))\n",
    "    return dataset\n",
    "\n",
    "#create a dict of functions and path w.r.t. different mode\n",
    "data_info = {\n",
    "    \"train_origin\":{\"path\":\"./food-11/training/labeled\"},\n",
    "    \"train\":{\"path\":\"./food-11/training/unlabeled\"},\n",
    "    \"val\":{\"path\":\"./food-11/validation\"},\n",
    "    \"test\":{\"path\":\"./food-11/testing\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.transforms.transforms.Compose'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfm = transforms.Compose([\n",
    "      transforms.Resize((128, 128)),\n",
    "      transforms.ToTensor(),\n",
    "      ])\n",
    "print(type(tfm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of val data: 660\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "x = 500# x = int(max(prep))\n",
    "print(list(food_f(\"val\"))[x][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_paras = {\n",
    "    # maximum number of epochs\n",
    "    'n_epochs': 80,\n",
    "    # mini-batch size for dataloader\n",
    "    'batch_size': 128,\n",
    "    # optimization algorithm (optimizer in torch.optim)\n",
    "    'optimizer': 'Adam',\n",
    "    # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
    "    'optim_hparas': {\n",
    "        # learning rate of Adam\n",
    "        'lr': 0.0003,\n",
    "        \"weight_decay\" : 1e-5\n",
    "    },\n",
    "    # your model will be saved here\n",
    "    'save_path': './model.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data: 6786\n",
      "Size of val data: 660\n",
      "Size of test data: 3347\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Construct data loaders.\n",
    "train_set = DataLoader(food_f(\"train\"), batch_size=h_paras[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_set = DataLoader(food_f(\"val\"), batch_size=h_paras[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_set = DataLoader(food_f(\"test\"), batch_size=h_paras[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 128, 128])\n",
      "torch.Size([64, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for i , t in enumerate(valid_set):\n",
    "    if i ==0:\n",
    "        test_t = t[0]\n",
    "#        print(type(t[0]),t[0].size())\n",
    "        print(test_t.size())\n",
    "        x = test_t[list(range(64))]\n",
    "        print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # The arguments for commonly used modules:\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "        # input image size: [3, 128, 128]\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4, 4, 0),\n",
    "        )\n",
    "        self.flat_layer = nn.Flatten()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(256 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input (x): [batch_size, 3, 128, 128]\n",
    "        # output: [batch_size, 11]\n",
    "\n",
    "        # Extract features by convolutional layers.\n",
    "        x = self.cnn_layers(x)\n",
    "\n",
    "        # The extracted feature map must be flatten before going to fully-connected layers.\n",
    "        x = self.flat_layer(x)\n",
    "\n",
    "        # The features are transformed by fully-connected layers to obtain the final logits.\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\ML2021_python_3_11_5\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import ConcatDataset\n",
    "def get_pseudo_labels(model, threshold=0.65):\n",
    "    # This functions generates pseudo-labels of a dataset using given model.\n",
    "    # It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.\n",
    "    # You are NOT allowed to use any models trained on external data for pseudo-labeling.\n",
    "\n",
    "    # Construct a data loader.\n",
    "    train_unlabeled_set = \\\n",
    "    DataLoader(food_f(\"train_unlabeled\"), batch_size=h_paras[\"batch_size\"], shuffle=True, num_workers=8, pin_memory=True)\n",
    "    # Make sure the model is in eval mode.\n",
    "    model.eval()\n",
    "    # Define softmax function.\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    # Iterate over the dataset by batches.\n",
    "    datas = []\n",
    "    labels = []\n",
    "    for batch in tqdm(train_unlabeled_set):\n",
    "        img = batch[0]\n",
    "        # Forward the data\n",
    "        # Using torch.no_grad() accelerates the forward process.\n",
    "        with torch.no_grad():\n",
    "            img_d = img.to(device)\n",
    "            logits = model(img_d)\n",
    "\n",
    "        # Obtain the probability distributions by applying softmax on logits.\n",
    "        predicts = softmax(logits)\n",
    "        for  row_index , probs in enumerate(predicts):\n",
    "            max_prob , max_column_index = probs.max(dim = -1)\n",
    "            if max_prob > threshold:\n",
    "                datas.append(img[row_index].tolist())\n",
    "                labels += (max_column_index.tolist())\n",
    "    new_train_set = torch.FloatTensor(datas)\n",
    "    new_train_label = torch.LongTensor(labels)\n",
    "    model.train()\n",
    "    return new_train_set,new_train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a model, and put it on the device specified.\n",
    "model = Classifier().to(device)\n",
    "model.device = device\n",
    "optimizer = torch.optim.Adam(model.parameters(), **h_paras[\"optim_hparas\"])\n",
    "\n",
    "# The number of training epochs.\n",
    "n_epochs = 80\n",
    "\n",
    "# Whether to do semi-supervised learning.\n",
    "do_semi = False\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    if do_semi:\n",
    "        # Obtain pseudo-labels for unlabeled data using trained model.\n",
    "        pseudo_set = get_pseudo_labels(model)\n",
    "        concat_dataset = ConcatDataset([train_set, pseudo_set])\n",
    "        train_set = pseudo_set\n",
    "        train_loader = DataLoader(concat_dataset, batch_size=128, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    # ---------- Training ----------\n",
    "    # Make sure the model is in train mode before training.\n",
    "    model.train()\n",
    "\n",
    "    # These are used to record information in training.\n",
    "    train_loss = []\n",
    "    train_accs = []\n",
    "\n",
    "    # Iterate the training set by batches.\n",
    "    for batch in tqdm(train_loader):\n",
    "\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "\n",
    "        # Forward the data. (Make sure data and model are on the same device.)\n",
    "        logits = model(imgs.to(device))\n",
    "\n",
    "        # Calculate the cross-entropy loss.\n",
    "        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n",
    "        loss = h_paras[](logits, labels.to(device))\n",
    "\n",
    "        # Gradients stored in the parameters in the previous step should be cleared out first.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the gradients for parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient norms for stable training.\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "        # Update the parameters with computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        train_loss.append(loss.item())\n",
    "        train_accs.append(acc)\n",
    "\n",
    "    # The average loss and accuracy of the training set is the average of the recorded values.\n",
    "    train_loss = sum(train_loss) / len(train_loss)\n",
    "    train_acc = sum(train_accs) / len(train_accs)\n",
    "\n",
    "    # Print the information.\n",
    "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "    model.eval()\n",
    "\n",
    "    # These are used to record information in validation.\n",
    "    valid_loss = []\n",
    "    valid_accs = []\n",
    "\n",
    "    # Iterate the validation set by batches.\n",
    "    for batch in tqdm(valid_loader):\n",
    "\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "\n",
    "        # We don't need gradient in validation.\n",
    "        # Using torch.no_grad() accelerates the forward process.\n",
    "        with torch.no_grad():\n",
    "          logits = model(imgs.to(device))\n",
    "\n",
    "        # We can still compute the loss (but not the gradient).\n",
    "        loss = criterion(logits, labels.to(device))\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        valid_loss.append(loss.item())\n",
    "        valid_accs.append(acc)\n",
    "\n",
    "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "\n",
    "    # Print the information.\n",
    "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model is in eval mode.\n",
    "# Some modules like Dropout or BatchNorm affect if the model is in training mode.\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store the predictions.\n",
    "predictions = []\n",
    "\n",
    "# Iterate the testing set by batches.\n",
    "for batch in tqdm(test_loader):\n",
    "    # A batch consists of image data and corresponding labels.\n",
    "    # But here the variable \"labels\" is useless since we do not have the ground-truth.\n",
    "    # If printing out the labels, you will find that it is always 0.\n",
    "    # This is because the wrapper (DatasetFolder) returns images and labels for each batch,\n",
    "    # so we have to create fake labels to make it work normally.\n",
    "    imgs, labels = batch\n",
    "\n",
    "    # We don't need gradient in testing, and we don't even have labels to compute loss.\n",
    "    # Using torch.no_grad() accelerates the forward process.\n",
    "    with torch.no_grad():\n",
    "        logits = model(imgs.to(device))\n",
    "\n",
    "    # Take the class with greatest logit as prediction and record it.\n",
    "    predictions.extend(logits.argmax(dim=-1).cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions into the file.\n",
    "with open(\"predict.csv\", \"w\") as f:\n",
    "\n",
    "    # The first row must be \"Id, Category\"\n",
    "    f.write(\"Id,Category\\n\")\n",
    "\n",
    "    # For the rest of the rows, each image id corresponds to a predicted class.\n",
    "    for i, pred in  enumerate(predictions):\n",
    "         f.write(f\"{i},{pred}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2021_python_3_11_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
