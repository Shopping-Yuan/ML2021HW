{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# You may choose where to download the data.\n",
    "\n",
    "# Google Drive\n",
    "#!gdown --id '1awF7pZ9Dz7X1jn1_QAiKN-_v56veCEKy' --output food-11.zip\n",
    "\n",
    "# Dropbox\n",
    "# !wget https://www.dropbox.com/s/m9q6273jl3djall/food-11.zip -O food-11.zip\n",
    "\n",
    "# MEGA\n",
    "# !sudo apt install megatools\n",
    "# !megadl \"https://mega.nz/#!zt1TTIhK!ZuMbg5ZjGWzWX1I6nEUbfjMZgCmAgeqJlwDkqdIryfg\"\n",
    "\n",
    "# Unzip the dataset.\n",
    "# This may take some time.\n",
    "#!unzip -q food-11.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#import pytorch\n",
    "import torch\n",
    "\n",
    "# torch.backends.cudnn: set CNN algorithmtorch.backends.cudnn\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# get the current available device ('cpu' or 'cuda')\n",
    "def get_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = get_device()\n",
    "print(torch.cuda.is_available())\n",
    "#set random variable\n",
    "import numpy as np\n",
    "myseed = 1\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# m = nn.Conv2d(1, 1, 2 , stride=2, padding=(9,9),bias = False)\n",
    "# test = torch.tensor([[[1.0,2.0,3.0],[4.0,5.0,6.0],[7.0,8.0,9.0]]])\n",
    "# print(test)\n",
    "# print(m(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# with Image.open(\"C:/Users/user/Pictures\\Screenshots/資格證.jpg\") as im:\n",
    "#     im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision.datasets import DatasetFolder\n",
    "def food_train_val_f(mode,d_l = \"both\"):\n",
    "  train_tfm = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "  print('Size of training data: {}'.format(data.shape))\n",
    "  print('Size of training label: {}'.format(label.shape))\n",
    "  VAL_RATIO = 0.2\n",
    "  percent = int(data.shape[0] * (1 - VAL_RATIO))\n",
    "  train_data, train_label, val_data, val_label = \\\n",
    "  data[:percent], label[:percent].astype(np.int_), data[percent:], label[percent:].astype(np.int_)\n",
    "\n",
    "  if mode == \"train\":\n",
    "    return{\"data\" : torch.FloatTensor(train_data) , \"label\" : torch.LongTensor(train_label)}\n",
    "  elif mode == \"val\":\n",
    "    return{\"data\" : torch.FloatTensor(val_data) , \"label\" : torch.LongTensor(val_label)}\n",
    "\n",
    "def food_test_f(mode,d_l = \"data\"):\n",
    "  test_tfm = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "  test_set = DatasetFolder(\"food-11/testing\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)\n",
    "  return torch.FloatTensor(test_set)\n",
    "\n",
    "#create a dict of functions and path w.r.t. different mode\n",
    "data_info = {\n",
    "    \"train\":{\"function\":food_train_val_f,\"data_path\":\"./timit_11/train_11.npy\",\"label_path\":\"./timit_11/train_label_11.npy\",\"data_or_label\":\"both\"},\n",
    "    \"val\":{\"function\":food_train_val_f,\"data_path\":\"./timit_11/train_11.npy\",\"label_path\":\"./timit_11/train_label_11.npy\",\"data_or_label\":\"both\"},\n",
    "    \"test\":{\"function\":food_test_f,\"path\":\"./timit_11/test_11.npy\",\"data_or_label\":\"data\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# Construct datasets.\n",
    "# The argument \"loader\" tells how torchvision reads the data.\n",
    "train_set = DatasetFolder(\"food-11/training/labeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_tfm)\n",
    "valid_set = DatasetFolder(\"food-11/validation\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)\n",
    "unlabeled_set = DatasetFolder(\"food-11/training/unlabeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_tfm)\n",
    "\n",
    "\n",
    "# Construct data loaders.\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # The arguments for commonly used modules:\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "        # input image size: [3, 128, 128]\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4, 4, 0),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(256 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input (x): [batch_size, 3, 128, 128]\n",
    "        # output: [batch_size, 11]\n",
    "\n",
    "        # Extract features by convolutional layers.\n",
    "        x = self.cnn_layers(x)\n",
    "\n",
    "        # The extracted feature map must be flatten before going to fully-connected layers.\n",
    "        x = nn.Flatten()(x)\n",
    "\n",
    "        # The features are transformed by fully-connected layers to obtain the final logits.\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2021_python_3_11_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
