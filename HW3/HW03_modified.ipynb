{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW2/ML2021HW2_modified.ipynb\" target=\"_parent\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# You may choose where to download the data.\n",
    "\n",
    "# Google Drive\n",
    "#!gdown --id '1awF7pZ9Dz7X1jn1_QAiKN-_v56veCEKy' --output food-11.zip\n",
    "\n",
    "# Dropbox\n",
    "# !wget https://www.dropbox.com/s/m9q6273jl3djall/food-11.zip -O food-11.zip\n",
    "\n",
    "# MEGA\n",
    "# !sudo apt install megatools\n",
    "# !megadl \"https://mega.nz/#!zt1TTIhK!ZuMbg5ZjGWzWX1I6nEUbfjMZgCmAgeqJlwDkqdIryfg\"\n",
    "\n",
    "# Unzip the dataset.\n",
    "# This may take some time.\n",
    "#!unzip -q food-11.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#import pytorch\n",
    "import torch\n",
    "\n",
    "# torch.backends.cudnn: set CNN algorithmtorch.backends.cudnn\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# get the current available device ('cpu' or 'cuda')\n",
    "def get_device():\n",
    "#    return 'cpu'\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = get_device()\n",
    "print(torch.cuda.is_available())\n",
    "#set random variable\n",
    "import numpy as np\n",
    "myseed = 1\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# m = nn.Conv2d(1, 1, 2 , stride=2, padding=(9,9),bias = False)\n",
    "# test = torch.tensor([[[1.0,2.0,3.0],[4.0,5.0,6.0],[7.0,8.0,9.0]]])\n",
    "# print(test)\n",
    "# print(m(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# with Image.open(\"C:/Users/user/Pictures\\Screenshots/資格證.jpg\") as im:\n",
    "#     im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# x = torch.FloatTensor([[2,7,1],[3,4,9],[5,6,8]])\n",
    "# y = x[x> 2]\n",
    "# print(x,y)\n",
    "# print(x.argmax(dim = 0))\n",
    "# print(x.argmax(dim = 1))\n",
    "# print(x.argmax(dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from PIL import Image\n",
    "def get_tfm(mode):\n",
    "    if mode in [\"val\",\"test\"]:\n",
    "      tfm = transforms.Compose([\n",
    "      transforms.Resize(data_info[\"size\"]),\n",
    "      transforms.ToTensor(),\n",
    "      ])\n",
    "    elif mode in [\"train_unlabeled\",\"train\"]:\n",
    "       tfm = transforms.Compose([\n",
    "      transforms.Resize(data_info[\"size\"]),\n",
    "      transforms.ToTensor(),\n",
    "      ])\n",
    "    return tfm\n",
    "def food_f(mode):\n",
    "    dataset = DatasetFolder(data_info[mode][\"path\"], loader=lambda x: Image.open(x), extensions=\"jpg\", transform=get_tfm(mode))\n",
    "    print('Size of {} data: {}'.format(mode,len(list(dataset))))\n",
    "    return dataset\n",
    "\n",
    "#create a dict of functions and path w.r.t. different mode\n",
    "data_info = {\n",
    "    \"train_origin\":{\"path\":\"./food-11/training/labeled\"},\n",
    "    \"train\":{\"path\":\"./food-11/training/unlabeled\"},\n",
    "    \"val\":{\"path\":\"./food-11/validation\"},\n",
    "    \"test\":{\"path\":\"./food-11/testing\"},\n",
    "    \"size\" :(128,128)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.transforms.transforms.Compose'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfm = transforms.Compose([\n",
    "      transforms.Resize(data_info[\"size\"]),\n",
    "      transforms.ToTensor(),\n",
    "      ])\n",
    "print(type(tfm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of val data: 660\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "x = 500# x = int(max(prep))\n",
    "print(list(food_f(\"val\"))[x][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_paras = {\n",
    "    # maximum number of epochs\n",
    "    'n_epochs': 2,\n",
    "    # mini-batch size for dataloader\n",
    "    'batch_size': 1024,\n",
    "    # optimization algorithm (optimizer in torch.optim)\n",
    "    'optimizer': 'Adam',\n",
    "    # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
    "    'optim_hparas': {\n",
    "        # learning rate of Adam\n",
    "        'lr': 0.0003,\n",
    "        \"weight_decay\" : 1e-5\n",
    "    },\n",
    "    # your model will be saved here\n",
    "    'save_path': './model.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data: 6786\n",
      "Size of val data: 660\n",
      "Size of test data: 3347\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Construct data loaders.\n",
    "train_set = DataLoader(food_f(\"train\"), batch_size=h_paras[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_set = DataLoader(food_f(\"val\"), batch_size=h_paras[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_set = DataLoader(food_f(\"test\"), batch_size=h_paras[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([16, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for i , t in enumerate(valid_set):\n",
    "    if i ==0:\n",
    "        test_t = t[0]\n",
    "#        print(type(t[0]),t[0].size())\n",
    "        print(test_t.size())\n",
    "        x = test_t[list(range(16))]\n",
    "        print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # The arguments for commonly used modules:\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "        # input image size: [3, 32, 32]\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "        )\n",
    "        self.flat_layer = nn.Flatten()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(int(256 * data_info[\"size\"][0]/8 * data_info[\"size\"][1]/8), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 11)\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    def forward(self, x):\n",
    "        # input (x): [batch_size, 3, 128, 128]\n",
    "        # output: [batch_size, 11]\n",
    "\n",
    "        # Extract features by convolutional layers.\n",
    "        x = self.cnn_layers(x)\n",
    "\n",
    "        # The extracted feature map must be flatten before going to fully-connected layers.\n",
    "        x = self.flat_layer(x)\n",
    "\n",
    "        # The features are transformed by fully-connected layers to obtain the final logits.\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    def cal_loss(self, pred, target):\n",
    "        return self.criterion(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import ConcatDataset\n",
    "def get_pseudo_labels(model, threshold=0.65):\n",
    "    # This functions generates pseudo-labels of a dataset using given model.\n",
    "    # It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.\n",
    "    # You are NOT allowed to use any models trained on external data for pseudo-labeling.\n",
    "\n",
    "    # Construct a data loader.\n",
    "    train_unlabeled_set = \\\n",
    "    DataLoader(food_f(\"train_unlabeled\"), batch_size=h_paras[\"batch_size\"], shuffle=True, num_workers=8, pin_memory=True)\n",
    "    # Make sure the model is in eval mode.\n",
    "    model.eval()\n",
    "    # Define softmax function.\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    # Iterate over the dataset by batches.\n",
    "    datas = []\n",
    "    labels = []\n",
    "    for batch in tqdm(train_unlabeled_set):\n",
    "        img = batch[0]\n",
    "        # Forward the data\n",
    "        # Using torch.no_grad() accelerates the forward process.\n",
    "        with torch.no_grad():\n",
    "            img_d = img.to(device)\n",
    "            logits = model(img_d)\n",
    "\n",
    "        # Obtain the probability distributions by applying softmax on logits.\n",
    "        predicts = softmax(logits)\n",
    "        for  row_index , probs in enumerate(predicts):\n",
    "            max_prob , max_column_index = probs.max(dim = -1)\n",
    "            if max_prob > threshold:\n",
    "                datas.append(img[row_index].tolist())\n",
    "                labels += (max_column_index.tolist())\n",
    "    new_train_set = torch.FloatTensor(datas)\n",
    "    new_train_label = torch.LongTensor(labels)\n",
    "    model.train()\n",
    "    return new_train_set,new_train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_set,model_d,optimizer,device):\n",
    "    # set model to training mode\n",
    "    model_d.train()\n",
    "    total_correct_number = 0\n",
    "    train_loss_list = []\n",
    "    # iterate through the dataloader\n",
    "    for data , label in train_set:\n",
    "      # move data to device (cpu/cuda)\n",
    "      data_d , label_d = data.to(device), label.to(device)\n",
    "      # forward pass (compute output tensor)\n",
    "      pred = (model_d(data_d))\n",
    "      # get the index of the class with the highest probability\n",
    "      max_prob_values, max_prob_indexs = torch.max(pred, dim = 1)\n",
    "      correct_number = (max_prob_indexs.cpu() == label_d.cpu()).sum().item()\n",
    "#      print(correct_number)\n",
    "      total_correct_number += correct_number\n",
    "      # compute loss\n",
    "      loss = model_d.cal_loss(pred , label_d)\n",
    "      # compute gradient (backpropagation)\n",
    "      loss.backward()\n",
    "      # Clip the gradient norms for stable training.\n",
    "      nn.utils.clip_grad_norm_(model_d.parameters(), max_norm=10)\n",
    "      # update model with optimizer\n",
    "      optimizer.step()\n",
    "      # set optimizer gradient to zero\n",
    "      optimizer.zero_grad()\n",
    "      train_loss_list.append(loss.detach().cpu().item())\n",
    "    acc = total_correct_number/len(train_set.dataset) \n",
    "#    print(acc , train_loss_list)   \n",
    "    return acc , train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(val_set,model_d,device):\n",
    "    # set model to evalutation mode\n",
    "    model_d.eval()\n",
    "    total_correct_number = 0\n",
    "    total_loss = 0\n",
    "    # iterate through the dataloader\n",
    "    for data , label in val_set:\n",
    "    # move data to device (cpu/cuda)\n",
    "      data_d, label_d = data.to(device), label.to(device)\n",
    "      # disable gradient calculation\n",
    "      with torch.no_grad():\n",
    "        # forward pass (compute output)\n",
    "        pred = model_d(data_d)\n",
    "        # get the index of the class with the highest probability\n",
    "        max_prob_values, max_prob_indexs = torch.max(pred, dim = 1)\n",
    "        total_correct_number += (max_prob_indexs.cpu() == label_d.cpu()).sum().item()\n",
    "        # compute loss\n",
    "        mse_loss = model_d.cal_loss(pred, label_d)\n",
    "      # accumulate loss\n",
    "      batch_size = len(data_d)\n",
    "      total_loss += mse_loss.detach().cpu().item() * batch_size\n",
    "\n",
    "    # compute averaged loss\n",
    "    totol_size = len(val_set.dataset)\n",
    "    acc = total_correct_number/totol_size\n",
    "    avg_loss =  total_loss/totol_size\n",
    "    return acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_process(training_set, validation_set, model, h_paras, device):\n",
    "    # Initialize a model, and put it on the device specified.\n",
    "    model_d = model.to(device)\n",
    "    model_d.device = device\n",
    "    optimizer = torch.optim.Adam(model_d.parameters(), **h_paras[\"optim_hparas\"])\n",
    "\n",
    "    # The number of training epochs.\n",
    "    n_epochs = h_paras[\"n_epochs\"]\n",
    "\n",
    "    # record training accuracy\n",
    "    acc_record = {'train': [], \"val\": []}\n",
    "    # record training loss\n",
    "    loss_record = {'train': [], \"val\": []}\n",
    "\n",
    "    # Whether to do semi-supervised learning.\n",
    "    do_semi = False\n",
    "    train_set_origin = food_f(\"train\")\n",
    "    for epoch in range(n_epochs):\n",
    "        # ---------- Training ----------\n",
    "        train_acc , train_loss_list = train(training_set,model_d,optimizer,device)\n",
    "        # save accuracy to acc_record['train']\n",
    "        acc_record['train'].append(train_acc)\n",
    "      # save loss to loss_record['train']\n",
    "        loss_record['train'].append(train_loss_list)\n",
    "\n",
    "        if do_semi:\n",
    "            # Obtain pseudo-labels for unlabeled data using trained model.\n",
    "            pseudo_set = get_pseudo_labels(model_d)\n",
    "            concat_dataset = ConcatDataset([train_set_origin, pseudo_set])\n",
    "            train_set_origin = pseudo_set\n",
    "            training_set = DataLoader(concat_dataset, batch_size=128, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "        # ---------- Validation ----------\n",
    "        val_acc , val_loss = val(validation_set,model_d,device)\n",
    "        # Print the information.\n",
    "        acc_record[\"val\"].append(val_acc)\n",
    "        # save loss to loss_record[\"val\"]\n",
    "        loss_record[\"val\"].append(val_loss)\n",
    "        \n",
    "        print(' model epoch = {:4d}, train_loss = {:.4f} , val_loss = {:.4f})'\\\n",
    "        .format(epoch , train_loss_list[-1] , val_loss))\n",
    "        print('train set accuracy = {:.3f}'.format(train_acc))\n",
    "        torch.save(model.state_dict(), h_paras[\"save_path\"])\n",
    "        print('saving model with acc {:.3f}'.format(val_acc))\n",
    "    return acc_record , loss_record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data: 6786\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 2.00 GiB of which 0 bytes is free. Of the allocated memory 4.99 GiB is allocated by PyTorch, and 312.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32md:\\轉職資料\\VSCODEPYTHON\\ML2021\\HW3\\HW03_modified.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Construct model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m Classifier()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model_acc_record, model_loss_record \u001b[39m=\u001b[39m train_val_process(train_set, valid_set, model, h_paras, device)\n",
      "\u001b[1;32md:\\轉職資料\\VSCODEPYTHON\\ML2021\\HW3\\HW03_modified.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train_set_origin \u001b[39m=\u001b[39m food_f(\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# ---------- Training ----------\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     train_acc , train_loss_list \u001b[39m=\u001b[39m train(training_set,model_d,optimizer,device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# save accuracy to acc_record['train']\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     acc_record[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(train_acc)\n",
      "\u001b[1;32md:\\轉職資料\\VSCODEPYTHON\\ML2021\\HW3\\HW03_modified.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m model_d\u001b[39m.\u001b[39mcal_loss(pred , label_d)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# compute gradient (backpropagation)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Clip the gradient norms for stable training.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E8%BD%89%E8%81%B7%E8%B3%87%E6%96%99/VSCODEPYTHON/ML2021/HW3/HW03_modified.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model_d\u001b[39m.\u001b[39mparameters(), max_norm\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML2021_python_3_11_5\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML2021_python_3_11_5\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 2.00 GiB of which 0 bytes is free. Of the allocated memory 4.99 GiB is allocated by PyTorch, and 312.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Construct model\n",
    "model = Classifier()\n",
    "model_acc_record, model_loss_record = train_val_process(train_set, valid_set, model, h_paras, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model is in eval mode.\n",
    "# Some modules like Dropout or BatchNorm affect if the model is in training mode.\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store the predictions.\n",
    "predictions = []\n",
    "\n",
    "# Iterate the testing set by batches.\n",
    "for batch in tqdm(test_set):\n",
    "    # A batch consists of image data and corresponding labels.\n",
    "    # But here the variable \"labels\" is useless since we do not have the ground-truth.\n",
    "    # If printing out the labels, you will find that it is always 0.\n",
    "    # This is because the wrapper (DatasetFolder) returns images and labels for each batch,\n",
    "    # so we have to create fake labels to make it work normally.\n",
    "    imgs, labels = batch\n",
    "\n",
    "    # We don't need gradient in testing, and we don't even have labels to compute loss.\n",
    "    # Using torch.no_grad() accelerates the forward process.\n",
    "    with torch.no_grad():\n",
    "        pred = model(imgs.to(device))\n",
    "\n",
    "    # Take the class with greatest logit as prediction and record it.\n",
    "    predictions.extend(pred.argmax(dim=-1).cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions into the file.\n",
    "with open(\"predict.csv\", \"w\") as f:\n",
    "\n",
    "    # The first row must be \"Id, Category\"\n",
    "    f.write(\"Id,Category\\n\")\n",
    "\n",
    "    # For the rest of the rows, each image id corresponds to a predicted class.\n",
    "    for i, pred in  enumerate(predictions):\n",
    "         f.write(f\"{i},{pred}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2021_python_3_11_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
