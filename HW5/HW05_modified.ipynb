{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW5/HW05_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "###Part 0 setting and installing package\n",
        "------\n",
        "###Part 1 preparing data set\n",
        "------\n",
        "######load data file\n",
        "######clean data\n",
        "######pick up line pairs\n",
        "######tokenize : using sentencepiece\n",
        "######make data set\n",
        "------\n",
        "###Part 2 make model\n",
        "------\n",
        "######positional encoding layer\n",
        "######multihead attention layer\n",
        "######encoder layer(s)\n",
        "######decoder layer(s)\n",
        "######transformer layer\n",
        "------\n",
        "###Part 3 training and validation process\n",
        "------\n",
        "######Noam optimizer\n",
        "######label smoothing\n",
        "######beam search\n",
        "######bleu\n",
        "######training and validation function\n",
        "######main function\n",
        "------\n"
      ],
      "metadata": {
        "id": "WTv4XN2qB_fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "setting\n",
        "======\n",
        ">Here are all parameters using in this project."
      ],
      "metadata": {
        "id": "iVQ2D_mLcx4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "setting = {\n",
        "# information of the path of dataset\n",
        "\"data_info\" : {\n",
        "    \"document\":\"/content\",\n",
        "    \"raw_file_name\":\"/ted2020.tgz\",\n",
        "    \"unzip_path\":\"/train_dev/\",\n",
        "    \"source\":{\n",
        "        \"lang\":\"en\",\n",
        "        \"raw_data_path\":\"/train_dev/raw.en\",\n",
        "        \"clean_data_path\":\"/train_dev/clean_en.txt\",\n",
        "        \"tokenized_train_data\":\"/train_dev/tokenized_train_data_en.txt\",\n",
        "        \"tokenized_val_data\":\"/train_dev/tokenized_val_data_en.txt\"\n",
        "        },\n",
        "    \"target\":{\n",
        "        \"lang\":\"zh\",\n",
        "        \"raw_data_path\":\"/train_dev/raw.zh\",\n",
        "        \"clean_data_path\":\"/train_dev/clean_zh.txt\",\n",
        "        \"tokenized_train_data\":\"/train_dev/tokenized_train_data_zh.txt\",\n",
        "        \"tokenized_val_data\":\"/train_dev/tokenized_val_data_zh.txt\"\n",
        "        }\n",
        "},\n",
        "# tokenized setting for spm\n",
        "\"tokenized_setting\" : {\n",
        "    \"vocab_size\" : 8000,\n",
        "    \"character_coverage\" : 1,\n",
        "    \"model_type\" : \"bpe\", # \"unigram\",\n",
        "    \"input_sentence_size\" : 400000,\n",
        "    \"shuffle_input_sentence\" : True,\n",
        "    \"normalization_rule_name\" : \"nmt_nfkc_cf\",\n",
        "    \"pad_id\":0,\n",
        "    \"unk_id\":1,\n",
        "    \"bos_id\":2,\n",
        "    \"eos_id\":3,\n",
        "    \"max_l\":400\n",
        "},\n",
        "# model structure setting\n",
        "\"model\" : {\n",
        "      \"encoder_embedding_dimension\" : 256,\n",
        "      \"decoder_embedding_dimension\" : 256,\n",
        "      \"feedforward_dimension\" : 1024,\n",
        "      \"num_heads\" : 2,\n",
        "      \"dropout_p\" : 0.0,\n",
        "      \"layer_num\" : 1\n",
        "},\n",
        "\n",
        "# setting in training and validation process ,\n",
        "# including optimization setting.\n",
        "\"training_hparas\" : {\n",
        "    \"total_step\" : 20000,\n",
        "    \"do_valid_step\" : 1000,\n",
        "    \"early_stop_step\" : 2,\n",
        "    \"train_batch_size\" : 100,\n",
        "    \"valid_batch_size\" : 100,\n",
        "    \"workers\" : 0,\n",
        "    \"label_smoothing\" : 0.1,\n",
        "    \"beam_num\" : 2,\n",
        "    \"optimization\":{\n",
        "        \"factor\" : 2,\n",
        "        \"warmup\"  : 4000,\n",
        "        \"optimizer\" : {\n",
        "                \"lr\" : 0,\n",
        "                \"betas\" : (0.9, 0.98),\n",
        "                \"eps\" : 1e-9,\n",
        "                \"weight_decay\" : 0.0001\n",
        "                }\n",
        "            },\n",
        "    \"model_saving_path\" : \"/content/model.pth\"\n",
        "}\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "-WoR-01STMor"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "installing package\n",
        "------"
      ],
      "metadata": {
        "id": "cdURO12Sntrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# used in part 1\n",
        "!pip install sentencepiece\n",
        "# used in part 1 and 3\n",
        "!pip install tqdm\n",
        "# used in part 2\n",
        "!pip install torchinfo\n",
        "# used in part 3\n",
        "!pip install torcheval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ti-UGuHNhh",
        "outputId": "780d89b3-4439-4681-83dc-acb85aed761a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\n",
            "Installing collected packages: torcheval\n",
            "Successfully installed torcheval-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing data set\n",
        "=============\n",
        "\n",
        "load data file\n",
        "-------------\n",
        ">Here I load dataset from my drive,  \n",
        ">but it also can be download from the link below."
      ],
      "metadata": {
        "id": "mMkT3K4JXs60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 : download dataset from drive to google colab\n",
        "# original dataset is in \"https://mega.nz/#!vEcTCISJ!3Rw0eHTZWPpdHBTbQEqBDikDEdFPr7fI8WxaXK9yZ9U\"\n",
        "\n",
        "path_doc = setting[\"data_info\"][\"document\"]\n",
        "rawdata_file_name = setting[\"data_info\"][\"raw_file_name\"]\n",
        "rawdata_file_path = path_doc + rawdata_file_name\n",
        "unzip_path = path_doc + setting[\"data_info\"][\"unzip_path\"]\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive_path = path_doc + \"/drive\"\n",
        "drive_name = \"/MyDrive\"\n",
        "drive.mount(drive_path)\n",
        "\n",
        "# copy file from drive\n",
        "import shutil\n",
        "shutil.copyfile(drive_path + drive_name + rawdata_file_name, rawdata_file_path)\n",
        "\n",
        "# step 2 : unzip dataset\n",
        "import tarfile\n",
        "# open file\n",
        "file = tarfile.open(rawdata_file_path)\n",
        "# extracting file\n",
        "file.extractall(unzip_path)\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X23RaTWe9hoU",
        "outputId": "617a6169-bcd6-4362-ed74-348faaa5a662"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean data\n",
        "------\n",
        ">First each dataset (source or target) is clean  \n",
        ">seperately, change to halfwidth and remove/replace  \n",
        ">some kind of punctuations.\n",
        "\n",
        ">Also because the number of sentences in one line may be  \n",
        ">different in line pairs of source and target set (its an error),  \n",
        ">some special punctuations is add to the end of sentences  \n",
        ">for the next process dealing with these problem by  \n",
        ">using sentence pairs instead of lines pairs to form datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "-y54N2UNimor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "# convert fullwidth to halfwidth\n",
        "def to_halfwidth(string):\n",
        "  return \"\".join(unicodedata.normalize('NFKC',letter) for letter in string)\n",
        "def clean_s_zh(s):\n",
        "    s = to_halfwidth(s)\n",
        "    # step 1 : delete — _\n",
        "    delete = \" _()[]\"\n",
        "    delete_rules = s.maketrans(\"\",\"\",delete)\n",
        "    s = s.translate(delete_rules)\n",
        "\n",
        "    # step 2 : replace “” with \"\"\n",
        "    to_be_replace = '“”'\n",
        "    replace = '\"\"'\n",
        "    replace_dict = dict(zip(to_be_replace,replace))\n",
        "\n",
        "    # step 3 : add **END** before and after punctuation\n",
        "\n",
        "    \"\"\"\n",
        "    The number of sentences in one line may be different\n",
        "    in line pairs of source and target set.\n",
        "    \"**END**\" is add after \"。!?\" and \".!?\", which can be used\n",
        "    to check if the number of sentence in the pair are equal\n",
        "    in the next process.\n",
        "    also in english, \".\" may be use in abbreviation,\n",
        "    these different use must be identified.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    punctuation = \"。!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    zh_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return zh_list\n",
        "\n",
        "def clean_s_en(s):\n",
        "    s = to_halfwidth(s)\n",
        "\n",
        "    replace_dict = {}\n",
        "\n",
        "    delete = \"-()[]\"\n",
        "    for char in delete:\n",
        "      replace_dict[char] = \"\"\n",
        "\n",
        "    punctuation = \"!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    # Identify if \".\" is used in abbreviation,\n",
        "    # if not, add \"**END**\" after it.\n",
        "    pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "    s = pattern.sub(\"**END**\",s)\n",
        "\n",
        "    # test pattern\n",
        "    # pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "    # result = pattern.sub(\"**END**\",\"There are many people in U.S. w.r.t. in Taiwan.Thank you.\")\n",
        "\n",
        "    en_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return en_list"
      ],
      "metadata": {
        "id": "5dwWeVz-sPI2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pick up line pairs\n",
        "------\n",
        ">pick up line pairs has equal number of sentences and  \n",
        ">split them into sentences to form sourse/target dataset.  \n",
        ">Remove sentences with too many words for training and validation."
      ],
      "metadata": {
        "id": "M2EfnpIXV91l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using \"**END**\" to split line pairs to check if they have equal sentence\n",
        "def divide_by_END(s):\n",
        "    list_s = []\n",
        "    for line_string in s.strip(\"**END**\").split(\"**END**\"):\n",
        "      if line_string not in [\"\",\" \"]:\n",
        "         list_s.append(line_string)\n",
        "    return(list_s)\n",
        "'''\n",
        "warning : devide_en_again function is apply just beacause\n",
        "in \"this\" dataset english sentences end with \":\" or \";\"\n",
        "sometimes not splited well.\n",
        "If the dataset is change, this part may need to be\n",
        "eliminated or modified.\n",
        "'''\n",
        "def devide_en_again(s,punctuation = \":;\"):\n",
        "    replace_dict = {}\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules_src = s.maketrans(replace_dict)\n",
        "    new_s = divide_by_END(s.translate(replace_rules_src))\n",
        "    return new_s\n",
        "\n",
        "# remove \"sentence\" if it is too long.\n",
        "def remove_too_long(src_list,tgt_list,threshold):\n",
        "    too_long_src = 0\n",
        "    too_long_tgt = 0\n",
        "    remove = False\n",
        "    new_s = []\n",
        "    new_t = []\n",
        "    for i in range(len(src_list)):\n",
        "      if ((len(src_list[i])>threshold)):\n",
        "        remove = True\n",
        "        too_long_src += 1\n",
        "      if (len(tgt_list[i])>threshold):\n",
        "        remove = True\n",
        "        too_long_tgt += 1\n",
        "      if remove == False:\n",
        "        new_s.append(src_list[i])\n",
        "        new_t.append(tgt_list[i])\n",
        "      else :\n",
        "        remove = False\n",
        "    return(new_s,new_t,too_long_src,too_long_tgt)\n",
        "\n",
        "# pick up good line pairs for traning and validation model\n",
        "def check_data_pairs(src_list,tgt_list,threshold):\n",
        "    index = 0\n",
        "    new_src_list = []\n",
        "    new_tgt_list = []\n",
        "\n",
        "    same = 0\n",
        "    add_next = 0\n",
        "    split_again = 0\n",
        "    not_use = 0\n",
        "\n",
        "    while(index < len(src_list)):\n",
        "\n",
        "      src = divide_by_END(src_list[index])\n",
        "      tgt = divide_by_END(tgt_list[index])\n",
        "\n",
        "      # case 1 : src is as long as tgt , finished.\n",
        "      if len(src) == len(tgt):\n",
        "        new_src_list += src\n",
        "        new_tgt_list += tgt\n",
        "        same += 1\n",
        "        index += 1\n",
        "\n",
        "      else :\n",
        "        # if it is not the last one : both src and tgt add next sentence\n",
        "        if index != len(src_list)-1:\n",
        "          src_add_next = divide_by_END(src_list[index] + src_list[index+1])\n",
        "          tgt_add_next = divide_by_END(tgt_list[index] + tgt_list[index+1])\n",
        "          # case 2 : src_add_next is as long as tgt_add_next , finished.\n",
        "          if len(src_add_next) == len(tgt_add_next):\n",
        "            new_src_list += src_add_next\n",
        "            new_tgt_list += tgt_add_next\n",
        "            add_next += 2\n",
        "            index += 2\n",
        "\n",
        "          # using new punctuation to divide tgt (english) sentence.\n",
        "          # note that this part could cause negative effects if the dataset is change.\n",
        "          else :\n",
        "            src_add_next = devide_en_again(src_list[index] + src_list[index+1])\n",
        "            # case 3 : src_add_next is as long as tgt_add_next , finished.\n",
        "            if len(src_add_next) == len(tgt_add_next):\n",
        "              new_src_list += src_add_next\n",
        "              new_tgt_list += tgt_add_next\n",
        "              split_again +=2\n",
        "              index += 2\n",
        "\n",
        "            # case 4 : sentence will not be used.\n",
        "            else :\n",
        "              not_use += 1\n",
        "              # if to_do == 1 :\n",
        "              #   print(index,src_add_next,tgt_add_next,len(src_add_next),len(tgt_add_next))\n",
        "              index += 1\n",
        "\n",
        "        # if it is the last one\n",
        "        else :\n",
        "          not_use += 1\n",
        "          index += 1\n",
        "    # print information\n",
        "    print(f\"The original total number of line is {index}.\")\n",
        "    print(f\"The number of line pairs have the equal sentences is {same}.\")\n",
        "    print(f\"The number of line pairs have the equal sentences after combine the next lines is {add_next}.\")\n",
        "    print(f\"The number of line pairs have the equal sentences after combine the next lines\"+\\\n",
        "       f\"and resplit english lines using :; is {split_again}.\")\n",
        "    print(f\"The number of line we don't use is {not_use}.\")\n",
        "    print(f\"Note that {index} = {same}+{add_next}+{split_again}+{not_use}.\")\n",
        "\n",
        "    # remove long lines\n",
        "    print(f\"The total number of sentence pairs before remove long sentences is {len(new_src_list)}.\")\n",
        "    new_src_list,new_tgt_list,too_long_src,too_long_tgt = remove_too_long(new_src_list,new_tgt_list,threshold)\n",
        "    print(f\"The finally total number of sentence pairs using is {len(new_src_list)}.\")\n",
        "    print(f\"Note that {len(new_src_list)} are the number of sentence pairs, not line pairs\")\n",
        "\n",
        "    return(new_src_list,new_tgt_list)"
      ],
      "metadata": {
        "id": "UVYvYCpuDQDa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load and clean data\n",
        "------"
      ],
      "metadata": {
        "id": "1KPMwJwck437"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load and clean data\n",
        "def load_file(path,function):\n",
        "    with open(path, \"r\") as f:\n",
        "      data = f.read()\n",
        "      return function(data)\n",
        "# saving to new path\n",
        "def clean_data_and_save(\n",
        "    path_doc,raw_src_path,raw_tgt_path,\n",
        "    clean_src_path,clean_tgt_path,threshold\n",
        "    ):\n",
        "    raw_src_path = path_doc + raw_src_path\n",
        "    raw_tgt_path = path_doc + raw_tgt_path\n",
        "    src = load_file(raw_src_path,clean_s_en),\n",
        "    tgt = load_file(raw_tgt_path,clean_s_zh),\n",
        "    # src , tgt are tuples with only one term : src_list, tgt_list\n",
        "    src_list = src[0]\n",
        "    tgt_list = tgt[0]\n",
        "    clean_src_list, clean_tgt_list = check_data_pairs(src_list,tgt_list,threshold)\n",
        "    with open(path_doc + clean_src_path, \"w\") as f:\n",
        "      f.write(\"\\n\".join(clean_src_list))\n",
        "    with open(path_doc + clean_tgt_path, \"w\") as f:\n",
        "      f.write(\"\\n\".join(clean_tgt_list))\n",
        "# test clean_data_and_save\n",
        "# clean_data_and_save(\n",
        "#     path_doc = setting[\"data_info\"][\"document\"],\n",
        "#     raw_src_path = setting[\"data_info\"][\"source\"][\"raw_data_path\"],\n",
        "#     raw_tgt_path = setting[\"data_info\"][\"target\"][\"raw_data_path\"],\n",
        "#     clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "#     clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"],\n",
        "#     threshold = setting[\"tokenized_setting\"][\"max_l\"]\n",
        "# )"
      ],
      "metadata": {
        "id": "DhVI2xylk1tr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenize\n",
        "------\n",
        ">using sentencepiece to tokenize sentences,  \n",
        ">first make the english/chinese dictionary separately,  \n",
        ">then use these dict to encode sentence pair in dataset,  \n",
        ">including add bos/eos/padding to tokenized sentences.  \n",
        ">Finally split then into train/val set and save."
      ],
      "metadata": {
        "id": "S_-APGe5okQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.utils.data as data\n",
        "def tokenized(clean_data_path,\n",
        "       vocab_size,\n",
        "       lang,\n",
        "       tokenized_setting\n",
        "       ):\n",
        "  model_prefix = f\"spm_{vocab_size}_{lang}\"\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      input=clean_data_path,\n",
        "      **tokenized_setting,\n",
        "      model_prefix=model_prefix,\n",
        "  )\n",
        "  return(model_prefix)\n",
        "\n",
        "def get_tokenizers(path_doc,vocab_size,src_lang,tgt_lang):\n",
        "  src_tokenizer = spm.SentencePieceProcessor(model_file = path_doc + f\"/spm_{vocab_size}_{src_lang}\" +\".model\")\n",
        "  tgt_tokenizer = spm.SentencePieceProcessor(model_file = path_doc + f\"/spm_{vocab_size}_{tgt_lang}\" +\".model\")\n",
        "  return src_tokenizer,tgt_tokenizer\n",
        "\n",
        "def bos_eos_padding(dataset,\n",
        "          max_l,\n",
        "          src_tokenizer,\n",
        "          tgt_tokenizer\n",
        "          ):\n",
        "\n",
        "\n",
        "  padding_src = []\n",
        "  padding_tgt = []\n",
        "  len_s = 0\n",
        "  len_t = 0\n",
        "  for src,tgt in dataset:\n",
        "    s = src_tokenizer.encode(src, out_type=int)\n",
        "    s = np.append(s,[3])\n",
        "    s = np.append([2],np.pad(s,(0, max_l-len(s)-1), constant_values = 0))\n",
        "    padding_src.append(s)\n",
        "\n",
        "    t = tgt_tokenizer.encode(tgt, out_type=int)\n",
        "    t = np.append(t,[3])\n",
        "    t = np.append([2],np.pad(t,(0, max_l-len(t)-1), constant_values = 0))\n",
        "    padding_tgt.append(t)\n",
        "\n",
        "  return(list(zip(padding_src,padding_tgt)))\n",
        "# test SentencePieceProcessor and bos_eos_padding\n",
        "# s_src = spm.SentencePieceProcessor(model_file=\"/content/spm8000_en.model\")\n",
        "# s_src.encode(\"hello world!\", out_type=int)\n",
        "# bos_eos_padding([(\"hello world\",\"_哈囉\")],5,10)\n",
        "\n",
        "def data_set_preparing(path_doc,\n",
        "            clean_src_path,\n",
        "            clean_tgt_path,\n",
        "            max_l,\n",
        "            src_tokenizer,\n",
        "            tgt_tokenizer,\n",
        "            st_train_path,\n",
        "            st_val_path,\n",
        "            tt_train_path,\n",
        "            tt_val_path,\n",
        "            ):\n",
        "    src_set = []\n",
        "    tgt_set = []\n",
        "\n",
        "    with open(path_doc+clean_src_path,\"r\") as in_f :\n",
        "      for line in tqdm(in_f):\n",
        "        src_set.append(line)\n",
        "    with open(path_doc+clean_tgt_path,\"r\") as in_f :\n",
        "      for line in tqdm(in_f):\n",
        "        tgt_set.append(line)\n",
        "\n",
        "    dataset = list(zip(src_set,tgt_set))\n",
        "    dataset = bos_eos_padding(dataset,max_l,src_tokenizer,tgt_tokenizer)\n",
        "    train_set, valid_set = data.random_split(dataset,[0.99,0.01])\n",
        "    # print(train_set[0][0])\n",
        "\n",
        "    with open(path_doc + st_train_path, 'w') as out_f:\n",
        "      for line_pair in tqdm(train_set):\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[0])+\"\\n\")\n",
        "    with open(path_doc + st_val_path, 'w') as out_f:\n",
        "      for line_pair in tqdm(valid_set):\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[0])+\"\\n\")\n",
        "    with open(path_doc + tt_train_path, 'w') as out_f:\n",
        "      for line_pair in tqdm(train_set):\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[1])+\"\\n\")\n",
        "    with open(path_doc + tt_val_path, 'w') as out_f:\n",
        "      for line_pair in tqdm(valid_set):\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[1])+\"\\n\")"
      ],
      "metadata": {
        "id": "ByrUmAvFkKk9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenized_data(vocab_size,tokenized_setting,max_l,path_doc,clean_src_path,\n",
        "          clean_tgt_path,src_lang,tgt_lang,st_train_path,st_val_path,\n",
        "          tt_train_path,tt_val_path,):\n",
        "  tokenized(path_doc + clean_src_path,vocab_size,src_lang,tokenized_setting)\n",
        "  tokenized(path_doc + clean_tgt_path,vocab_size,tgt_lang,tokenized_setting)\n",
        "  src_tokenizer,tgt_tokenizer = get_tokenizers(path_doc,vocab_size,src_lang,tgt_lang)\n",
        "  data_set_preparing(path_doc,clean_src_path,clean_tgt_path,max_l,src_tokenizer,\n",
        "           tgt_tokenizer,st_train_path,st_val_path,tt_train_path,tt_val_path)\n",
        "  return src_tokenizer,tgt_tokenizer\n",
        "\n",
        "# test tokenized_data()\n",
        "# src_tokenizer,tgt_tokenizer = tokenized_data(\n",
        "#     vocab_size = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "#     tokenized_setting = {k:setting[\"tokenized_setting\"][k] for k in \\\n",
        "#               set(list(setting[\"tokenized_setting\"].keys()))-{\"vocab_size\",\"max_l\"}},\n",
        "#     max_l = setting[\"tokenized_setting\"][\"max_l\"],\n",
        "#     path_doc = setting[\"data_info\"][\"document\"],\n",
        "#     clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "#     clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"],\n",
        "#     src_lang = setting[\"data_info\"][\"source\"][\"lang\"],\n",
        "#     tgt_lang = setting[\"data_info\"][\"target\"][\"lang\"],\n",
        "#     st_train_path = setting[\"data_info\"][\"source\"][\"tokenized_train_data\"],\n",
        "#     st_val_path = setting[\"data_info\"][\"source\"][\"tokenized_val_data\"],\n",
        "#     tt_train_path = setting[\"data_info\"][\"target\"][\"tokenized_train_data\"],\n",
        "#     tt_val_path = setting[\"data_info\"][\"target\"][\"tokenized_val_data\"])"
      ],
      "metadata": {
        "id": "RkIHpc6qkRqh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make data set\n",
        "------\n",
        "> Using tokenized data to make dataset.  \n",
        "> Classmethod : padding_mask_batch which  \n",
        "> where the key padding mask is constucted  \n",
        "> also defined here."
      ],
      "metadata": {
        "id": "FV3kHGqlggdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class myDataset(Dataset):\n",
        "  def __init__(self,src_path,tgt_path):\n",
        "\n",
        "    self.src_path = src_path\n",
        "    self.tgt_path = tgt_path\n",
        "\n",
        "    src_list = []\n",
        "    with open(self.src_path,\"r\") as f :\n",
        "      d_l = f.readlines()\n",
        "      for line in tqdm(d_l):\n",
        "        int_list = [int(i) for i in line.split()]\n",
        "        src_list.append(int_list)\n",
        "    self.src = torch.LongTensor(src_list)\n",
        "\n",
        "    tgt_list = []\n",
        "    with open(self.tgt_path,\"r\") as f :\n",
        "      l_l = f.readlines()\n",
        "      for line in tqdm(l_l):\n",
        "        int_list = [int(i) for i in line.split()]\n",
        "        tgt_list.append(int_list)\n",
        "    self.tgt = torch.LongTensor(tgt_list)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.src)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.src[index], self.tgt[index]\n",
        "\n",
        "  # make key padding mask\n",
        "  @classmethod\n",
        "  def padding_mask_batch(cls,batch,pad_id):\n",
        "    \"\"\"Collate a batch of data.\"\"\"\n",
        "    src, tgt = zip(*batch)\n",
        "    src = torch.stack(src)\n",
        "    tgt = torch.stack(tgt)\n",
        "    src_padding = (src == pad_id)\n",
        "    tgt_padding = (tgt == pad_id)\n",
        "\n",
        "    return src, tgt , src_padding, tgt_padding\n",
        "# test myDataset\n",
        "# data = []\n",
        "# with open(\"/content/train_dev/tokenized_train_data_en.txt\",\"r\") as f :\n",
        "#   d_l = f.readlines()\n",
        "#   for line in tqdm(d_l):\n",
        "#     int_list = [int(i) for i in line.split()]\n",
        "#     data.append(int_list)\n",
        "# print(data[0])"
      ],
      "metadata": {
        "id": "dbfi-DvrlDhI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "def get_data_set(train_batch_size,valid_batch_size,num_workers,path_doc,\n",
        "         st_train_path,st_val_path,tt_train_path,tt_val_path,pad_id):\n",
        "\n",
        "  train_set = myDataset(src_path = path_doc + st_train_path,\n",
        "              tgt_path = path_doc + tt_train_path,\n",
        "              )\n",
        "  valid_set = myDataset(src_path = path_doc + st_val_path,\n",
        "              tgt_path = path_doc + tt_val_path,\n",
        "              )\n",
        "  train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size = train_batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    collate_fn = lambda x : myDataset.padding_mask_batch(x,\n",
        "                   pad_id = pad_id)\n",
        "  )\n",
        "  valid_loader = DataLoader(\n",
        "    valid_set,\n",
        "    batch_size = valid_batch_size,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    collate_fn = lambda x : myDataset.padding_mask_batch(x,\n",
        "                   pad_id = pad_id)\n",
        "  )\n",
        "  del train_set,valid_set\n",
        "  gc.collect()\n",
        "  return train_loader,valid_loader\n",
        "# test get_data_set()\n",
        "# train_set,valid_set = get_data_set()\n",
        "# batch = next(iter(valid_set))\n",
        "# src,tgt,src_mask,tgt_mask = batch\n",
        "# print(src.shape)"
      ],
      "metadata": {
        "id": "YBFKMSM45ppM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make model\n",
        "======\n",
        "positional encoding layer\n",
        "------\n",
        ">The first layer is embedding layer, where each integers  \n",
        ">in encoder sentence will be represent by a vector.   \n",
        ">I use build-in class in pytorch to finish these part,    \n",
        ">and combine it with encoder layers to form my encoder.\n",
        "\n",
        ">The layer below is the second layer :positional encoding layer  \n",
        ">in this layer the position infomation is add to each \"word\"  \n",
        ">in the sentence.\n",
        ">Here I use parameters instead of constant as  \n",
        ">position infomation so they will change during training process."
      ],
      "metadata": {
        "id": "RZAzqY3bwds8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Positional_Encoding(nn.Module):\n",
        "    def __init__(self,max_sentence_length,embedding_dimension):\n",
        "      super().__init__()\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.encoding_values = nn.Parameter(nn.init.normal_(torch.empty(max_sentence_length,1, embedding_dimension)))\n",
        "    def forward(self, x):\n",
        "        # the shape of x : [batch,length,e_dim]\n",
        "        # the shape of self.encoding_values : [batch,length,e_dim]\n",
        "        x = x + self.encoding_values.unsqueeze(0)\n",
        "        x = x.squeeze(-2)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "EoKH9m1LznWO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "multihead attention layer\n",
        "------\n"
      ],
      "metadata": {
        "id": "OURB2Fg-4lE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import math\n",
        "from torchinfo import summary\n",
        "# This part is modify from pytorch : torch.nn.functional.scaled_dot_product_attention\n",
        "# Efficient implementation equivalent to the following:\n",
        "class Scaled_Dot_Product_Attention(nn.Module):\n",
        "    def __init__(self,max_sentence_length,dropout_p):\n",
        "      super().__init__()\n",
        "      self.dropout_p = dropout_p\n",
        "      self.max_l = max_sentence_length\n",
        "      attn_bias = torch.zeros(self.max_l, self.max_l)\n",
        "      temp_mask = torch.ones(self.max_l, self.max_l, dtype=torch.bool).tril(diagonal=0)\n",
        "      attn_bias = attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
        "      self.register_buffer(\"attn_bias\",attn_bias)\n",
        "\n",
        "    def forward(self, is_last_batch, query, key, value, padding_mask=None, is_causal=False, scale=None) -> torch.Tensor:\n",
        "      # Efficient implementation equivalent to the following:\n",
        "\n",
        "      scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
        "      attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
        "\n",
        "      if is_causal:\n",
        "        if is_last_batch:\n",
        "          self.attn_bias = self.attn_bias[:query.size(-2),:query.size(-2)]\n",
        "        self.attn_bias.to(query.dtype)\n",
        "        attn_weight += self.attn_bias\n",
        "\n",
        "      if padding_mask is not None:\n",
        "          if padding_mask.dtype == torch.bool:\n",
        "            padding_mask = torch.zeros_like(padding_mask,dtype = float).masked_fill_(padding_mask, (float(\"-inf\")))\n",
        "\n",
        "          padding_mask = padding_mask.unsqueeze(0).unsqueeze(0)\n",
        "          padding_mask.to(query.dtype)\n",
        "\n",
        "          attn_weight = attn_weight.transpose(-4,-2)\n",
        "          attn_weight += padding_mask\n",
        "          attn_weight = attn_weight.transpose(-4,-2)\n",
        "\n",
        "      attn_weight = torch.softmax(attn_weight, dim=-1)\n",
        "      attn_weight = torch.dropout(attn_weight, self.dropout_p, train=True)\n",
        "      return attn_weight @ value\n",
        "# test scaled_dot_product_attention\n",
        "# t = torch.rand([2,3,4,5])\n",
        "# mask = torch.tensor([[False,False,True,True],[False,True,False,True]],dtype = torch.bool)\n",
        "# print(scaled_dot_product_attention(\"cpu\",t,t,t,padding_mask= mask, is_causal=True))\n",
        "# from torch.nn.functional import scaled_dot_product_attention\n",
        "class My_MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, max_sentence_length, kv_input_dimension, embedding_dimension, num_heads, dropout_p, if_decoder = False):\n",
        "        '''\n",
        "        embedding_dimension = input dimension\n",
        "        note that there are residual sublayers in MultiHeadedAttention\n",
        "        '''\n",
        "        super().__init__()\n",
        "        assert embedding_dimension % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.max_l = max_sentence_length\n",
        "        self.kv_d = kv_input_dimension\n",
        "        self.d = embedding_dimension\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_p = dropout_p\n",
        "        self.is_causal = if_decoder\n",
        "        self.sdpa = Scaled_Dot_Product_Attention(self.max_l,self.dropout_p)\n",
        "        self.linear_for_q = nn.Linear(self.d, self.d)\n",
        "        self.linear_for_kv = nn.Linear(self.kv_d, 2 * self.d)\n",
        "        self.linear_out_project = nn.Linear(self.d, self.d)\n",
        "\n",
        "    def forward(self, is_last_batch, q_input_data, kv_input_data , padding_mask = None):\n",
        "\n",
        "        query = self.linear_for_q(q_input_data)\n",
        "        key, value = self.linear_for_kv(kv_input_data).split(self.d,dim = -1)\n",
        "\n",
        "        query,key,value = \\\n",
        "          map(lambda x : x.view(x.size(0),x.size(1),self.num_heads,self.d//self.num_heads),[query,key,value])\n",
        "        query,key,value = \\\n",
        "          map(lambda x : x.transpose(-2,-3),[query,key,value])\n",
        "\n",
        "        x = self.sdpa(is_last_batch,query,key,value,padding_mask = padding_mask,is_causal = self.is_causal)\n",
        "        x = x.transpose(-2,-3).contiguous()\n",
        "        x = x.view(x.size(0),x.size(1),self.d)\n",
        "        x = self.linear_out_project(x)\n",
        "\n",
        "        return x\n",
        "# test My_MultiHeadedAttention\n",
        "# model = My_MultiHeadedAttention(64,128,2,0.0)\n",
        "# q_input = torch.rand(32,400,128)\n",
        "# kv_input = torch.rand(32,400,64)\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(q_input,kv_input,mask).size())\n",
        "# print(summary(model,device = \"cpu\",q_input_data = q_input, kv_input_data = kv_input,padding_mask = mask))"
      ],
      "metadata": {
        "id": "PLWLkr9UaKFD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "encoder layer(s)\n",
        "------"
      ],
      "metadata": {
        "id": "T0IjI-zC5fqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class My_Encoder_Layer(nn.Module):\n",
        "  def __init__(self,max_sentence_length,embedding_dimension,feedforward_dimension,num_heads,dropout_p):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "\n",
        "    self.attention = My_MultiHeadedAttention(self.max_l, self.emb_dim, self.emb_dim, self.num_heads, self.dropout_p)\n",
        "    self.layer_norm_attn = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_attn_layernorm = nn.Dropout(self.dropout_p)\n",
        "\n",
        "    self.feedforward = nn.Sequential(\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    )\n",
        "    self.layer_norm_feedforward = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_feedforward_layernorm = nn.Dropout(self.dropout_p)\n",
        "\n",
        "\n",
        "  def forward(self,is_last_batch,x,padding_mask):\n",
        "    x = x + self.attention(is_last_batch,x,x,padding_mask)\n",
        "    x = self.layer_norm_attn(x)\n",
        "\n",
        "    x = self.drop_out_attn_layernorm(x)\n",
        "\n",
        "    x = x + self.feedforward(x)\n",
        "    x = self.layer_norm_feedforward(x)\n",
        "    x = self.drop_out_feedforward_layernorm(x)\n",
        "\n",
        "    return x\n",
        "# test My_Encoder_Layer\n",
        "# model = My_Encoder_Layer(\"cpu\",128,256,2,0.0)\n",
        "# input = torch.rand((32,400,128))\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(input,mask).size())\n",
        "# print(summary(model,input_data = input,padding_mask = mask))\n",
        "# print(model.state_dict().keys())\n",
        "class My_Encoder(nn.Module):\n",
        "  def __init__(self,max_sentence_length,dictionary_length,embedding_dimension,feedforward_dimension,\n",
        "         padding_idx, num_heads, dropout_p, layer_num):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.dict_l = dictionary_length\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.padding_idx = padding_idx\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "    self.layer_num = layer_num\n",
        "\n",
        "    self.encoder_embedding = nn.Embedding(self.dict_l,self.emb_dim,self.padding_idx)\n",
        "    self.positional_encoding = Positional_Encoding(self.max_l,self.emb_dim)\n",
        "    self.encoder = nn.ModuleList([My_Encoder_Layer(self.max_l,self.emb_dim,self.fwd_dim,\\\n",
        "                    self.num_heads,self.dropout_p) for i in range(layer_num)])\n",
        "\n",
        "  def forward(self,is_last_batch,input,padding_mask):\n",
        "    x = self.encoder_embedding(input.unsqueeze(-1))* math.sqrt(self.emb_dim)\n",
        "    x = self.positional_encoding(x)\n",
        "\n",
        "    for index,module in enumerate(self.encoder):\n",
        "      if index == 0:\n",
        "        x = module(is_last_batch,x,padding_mask)\n",
        "      else:\n",
        "        x = module(is_last_batch,x,None)\n",
        "    return x\n",
        "# test My_Encoder\n",
        "# model = My_Encoder(\"cpu\",400,8000,128,256,0,2,0.0,2)\n",
        "# input = torch.randint(0,7999,(32,400),dtype = torch.long)\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(input,mask).size())\n",
        "# print(summary(model,input_data = input,padding_mask = mask))\n",
        "# print(model.state_dict().keys())"
      ],
      "metadata": {
        "id": "PYhd7muASnrY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "decoder layer(s)\n",
        "------"
      ],
      "metadata": {
        "id": "4O_rI3QV55Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class My_Decoder_Layer(nn.Module):\n",
        "  def __init__(self,max_sentence_length,encoder_embedding_dimension,embedding_dimension,feedforward_dimension,num_heads,dropout_p):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.encoder_dim = encoder_embedding_dimension\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "\n",
        "    self.self_attention = My_MultiHeadedAttention \\\n",
        "     (self.max_l,self.emb_dim,self.emb_dim, num_heads = self.num_heads,\\\n",
        "     dropout_p = self.dropout_p, if_decoder = True)\n",
        "    self.layer_norm_sa = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_sa = nn.Dropout(0)\n",
        "\n",
        "    self.feedforward_sa = nn.Sequential(\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    )\n",
        "    self.layer_norm_sa_fw = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_sa_fw = nn.Dropout(0)\n",
        "\n",
        "    self.cross_attention = My_MultiHeadedAttention \\\n",
        "    (self.max_l,self.encoder_dim, self.emb_dim, num_heads = self.num_heads,\n",
        "    dropout_p = self.dropout_p, if_decoder = True)\n",
        "    self.layer_norm_ca = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_ca = nn.Dropout(0)\n",
        "\n",
        "    self.feedforward_ca = nn.Sequential(\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    )\n",
        "    self.layer_norm_ca_fw = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_ca_fw = nn.Dropout(0)\n",
        "\n",
        "  def forward(self,is_last_batch,encoder_input,input,padding_mask):\n",
        "\n",
        "    x = input + self.self_attention(is_last_batch,input,input,padding_mask)\n",
        "    x = self.layer_norm_sa(x)\n",
        "    x = self.drop_out_sa(x)\n",
        "\n",
        "    x = x + self.feedforward_sa(x)\n",
        "    x = self.layer_norm_sa_fw(x)\n",
        "    x = self.drop_out_sa_fw(x)\n",
        "\n",
        "    x = x + self.cross_attention(is_last_batch,x,encoder_input,padding_mask)\n",
        "    x = self.layer_norm_ca(x)\n",
        "    x = self.drop_out_ca(x)\n",
        "\n",
        "    x = x + self.feedforward_ca(x)\n",
        "    x = self.layer_norm_ca_fw(x)\n",
        "    x = self.drop_out_ca_fw(x)\n",
        "\n",
        "    return x\n",
        "class My_Decoder(nn.Module):\n",
        "  def __init__(self,max_sentence_length, dictionary_length, encoder_embedding_dimension,\n",
        "         embedding_dimension, feedforward_dimension, padding_idx, num_heads, dropout_p, layer_num):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.dict_l = dictionary_length\n",
        "    self.encoder_dim = encoder_embedding_dimension\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.padding_idx = padding_idx\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "    self.layer_num = layer_num\n",
        "\n",
        "    self.decoder_embedding = nn.Embedding(self.dict_l,self.emb_dim,padding_idx=self.padding_idx)\n",
        "    self.positional_encoding = Positional_Encoding(self.max_l,self.emb_dim)\n",
        "    self.decoder = nn.ModuleList([My_Decoder_Layer(self.max_l,self.encoder_dim,self.emb_dim,\\\n",
        "                    self.fwd_dim,self.num_heads,self.dropout_p) for i in range(self.layer_num)])\n",
        "    # self.encoder = My_Encoder_Layer(self.emb_dim,self.fwd_dim)\n",
        "\n",
        "    self.generator = nn.Linear(self.emb_dim,self.dict_l)\n",
        "\n",
        "  def forward(self,is_last_batch,encoder_input,input,padding_mask):\n",
        "    x = self.decoder_embedding(input.unsqueeze(-1))* math.sqrt(self.emb_dim)\n",
        "    x = self.positional_encoding(x)\n",
        "    # x = self.encoder(x,padding_mask)\n",
        "    for index,module in enumerate(self.decoder):\n",
        "      if index == 0:\n",
        "        x = module(is_last_batch,encoder_input,x,padding_mask)\n",
        "      else:\n",
        "        x = module(is_last_batch,encoder_input,x,None)\n",
        "    x = self.generator(x)\n",
        "    x = F.log_softmax(x,dim = -1)\n",
        "    return x\n",
        "# test My_Decoder\n",
        "# model = My_Decoder(\"cpu\",400,8000,128,64,256,0,2,0.0,2)\n",
        "# encoder_input = torch.rand(32,400,128)\n",
        "# input = torch.randint(0,7999,(32,400),dtype = torch.long)\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(encoder_input = encoder_input,input = input, padding_mask = mask).size())\n",
        "# print(summary(model,encoder_input = encoder_input,input = input, padding_mask = mask))\n",
        "# print(model.state_dict().keys())"
      ],
      "metadata": {
        "id": "rEjaTbhyBmEV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer layer\n",
        "------"
      ],
      "metadata": {
        "id": "1gcz18nz6QTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class My_Transformer(nn.Module):\n",
        "  def __init__(self,max_sentence_length,dictionary_length,padding_idx,\n",
        "         encoder_embedding_dimension,decoder_embedding_dimension,\n",
        "         feedforward_dimension,num_heads,dropout_p,layer_num):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.dict_l = dictionary_length\n",
        "    self.padding_idx = padding_idx\n",
        "    self.en_dim = encoder_embedding_dimension\n",
        "    self.de_dim = decoder_embedding_dimension\n",
        "    self.fw_dim = feedforward_dimension\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "    self.layer_num = layer_num\n",
        "    self.encoder = My_Encoder \\\n",
        "     (self.max_l,self.dict_l,self.en_dim,self.fw_dim,\n",
        "      self.padding_idx,self.num_heads,self.dropout_p,self.layer_num)\n",
        "    self.decoder = My_Decoder \\\n",
        "     (self.max_l,self.dict_l,self.en_dim,self.de_dim,self.fw_dim,\n",
        "      self.padding_idx,self.num_heads,self.dropout_p,self.layer_num)\n",
        "\n",
        "  def forward(self,is_last_batch,src,tgt,src_mask,tgt_mask):\n",
        "    memory = self.encoder(is_last_batch,src,src_mask)\n",
        "    outputs = self.decoder(is_last_batch,memory,tgt,tgt_mask)\n",
        "    return outputs\n",
        "\n",
        "def build_model(max_sentence_length,dictionary_length,padding_idx,encoder_embedding_dimension,\n",
        "         decoder_embedding_dimension,feedforward_dimension,num_heads,dropout_p,layer_num):\n",
        "  return My_Transformer(max_sentence_length,dictionary_length,padding_idx,\n",
        "              encoder_embedding_dimension,decoder_embedding_dimension,\n",
        "              feedforward_dimension,num_heads,dropout_p,layer_num)\n",
        "# test My_Transformer\n",
        "# model = My_Transformer(\"cpu\",400,8000,0,128,64,256,2,0,2)\n",
        "# src = torch.randint(0,8000,(32,400),dtype = torch.long)\n",
        "# tgt = torch.randint(0,8000,(32,400),dtype = torch.long)\n",
        "# src_mask = torch.cat(((torch.FloatTensor(32,200).uniform_() > 1),(torch.FloatTensor(32,200).uniform_() > 0.15)),dim =1)\n",
        "# tgt_mask = torch.cat(((torch.FloatTensor(32,100).uniform_() > 1),(torch.FloatTensor(32,300).uniform_() > 0.15)),dim =1)\n",
        "# out = model(src,tgt,src_mask,tgt_mask)\n",
        "# print(out.size(),out.dim(),out[0][0])\n",
        "# print(summary(model,src = src,tgt = tgt,src_mask = src_mask,tgt_mask = tgt_mask))\n",
        "# print(model.state_dict().keys())\n",
        "\n",
        "# test build_model\n",
        "# model = build_model()\n",
        "# batch = next(iter(train_set))\n",
        "# src, tgt, src_mask, tgt_mask = batch\n",
        "# print(type(src),src.shape)\n",
        "# print(summary(model,src = src,tgt = tgt,src_mask = src_mask,tgt_mask = tgt_mask))\n",
        "# outputs = model(src,tgt,src_mask,tgt_mask)\n",
        "# print(outputs.shape)"
      ],
      "metadata": {
        "id": "PBDq_h1jKCo7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training and validation process\n",
        "======\n",
        "Noam optimizer\n",
        "------"
      ],
      "metadata": {
        "id": "sPbMfCre6cSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reference : https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "class NoamOpt:\n",
        "    def __init__(self,dictionary_length,factor,warmup,optimizer):\n",
        "        self.dict_len = dictionary_length\n",
        "        self.factor = factor\n",
        "        self.warmup = warmup\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self._rate = 0\n",
        "    def step(self):\n",
        "        self._step += 1\n",
        "        self._rate = self.factor *(self.dict_len ** (-0.5) * \\\n",
        "        min(self._step ** (-0.5), self._step * self.warmup ** (-1.5)))\n",
        "\n",
        "        self.optimizer.param_groups[0][\"lr\"] = self._rate\n",
        "        self.optimizer.step()\n",
        "    def zero_grad(self):\n",
        "        return self.optimizer.zero_grad()\n",
        "# test NoamOpt:\n",
        "# x = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "# x.param_groups[0][\"lr\"]"
      ],
      "metadata": {
        "id": "Lb8BnuysNCOQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "label smoothing\n",
        "------"
      ],
      "metadata": {
        "id": "3TyyebUNOeI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "  def __init__(self,batch_size,dictionary_length,padding_id,smoothing):\n",
        "        super().__init__()\n",
        "        self.dict_len = dictionary_length\n",
        "        self.smoothing = smoothing\n",
        "        self.padding_id = padding_id\n",
        "        shift = torch.full(size = (batch_size,1), dtype = torch.long, fill_value = self.padding_id)\n",
        "        self.register_buffer(\"shift\",shift)\n",
        "  def forward(self, is_last_batch, outputs , label):\n",
        "\n",
        "    # step1 : when using label in validation, shift is needed.\n",
        "    # label_shift : {type : tensor , shape : batch  X (max_sentence_length-1)\n",
        "    # value : int}\n",
        "    label_shift = label[:,1:]\n",
        "    # shift : {type : tensor , shape : batch  X 1 ,value : self.padding_id}\n",
        "\n",
        "    # label_shift : {type : tensor , shape : batch  X max_sentence_length\n",
        "    # value : int}\n",
        "    if is_last_batch:\n",
        "      label_shift = torch.cat((label_shift,self.shift[:label.size(0),:]),dim = 1)\n",
        "\n",
        "    else:\n",
        "      label_shift = torch.cat((label_shift,self.shift),dim = 1)\n",
        "\n",
        "    # step2 : convert label to onehot tensor, then apply label smoothing\n",
        "    # label_onehot : {type : tensor , shape : batch  X max_sentence_length X dictionary_length\n",
        "    # value : 0 or 1}\n",
        "    label_onehot = F.one_hot(label_shift,self.dict_len).float()\n",
        "    # add : {type : float}\n",
        "    add = self.smoothing / (self.dict_len)\n",
        "    # label_onehot : {type : tensor , shape : batch  X max_sentence_length X dictionary_length\n",
        "    # value : add or 1+add}\n",
        "    label_onehot += add\n",
        "    # label_smoothed : {type : tensor , shape : batch  X max_sentence_length X dictionary_length\n",
        "    # value : add or 1+add-self.smoothing}\n",
        "    label_smoothed = label_onehot.masked_fill_((label_onehot > 1),float(1-self.smoothing+add))\n",
        "\n",
        "    '''\n",
        "    Question : Is padding really needed?\n",
        "    '''\n",
        "    # step3 : use padding mask to ignore to loss from padding id, then calculate loss.\n",
        "    # loss : {type : tensor , shape : batch  X max_sentence_length X 1, value : float}\n",
        "    loss = -1*torch.sum((outputs*label_smoothed),dim = -1)\n",
        "    # label_padding_mask {type : tensor , shape : batch  X max_sentence_length, value : bool}\n",
        "    label_padding_mask = (label == self.padding_id)\n",
        "    # mask_loss : {type : tensor , shape : batch  X max_sentence_length,\n",
        "    # value : 0 or add or 1+add-self.smoothing}\n",
        "    mask_loss = loss.masked_fill_(label_padding_mask,0)\n",
        "    # # ignore_index_number : {type : int}\n",
        "    # ignore_index_number = (mask_loss == 0).sum().item()\n",
        "    # avg_loss : {type : int}\n",
        "    # avg_loss = mask_loss.sum()/(mask_loss.size(0)*mask_loss.size(1)-ignore_index_number)\n",
        "    avg_loss = mask_loss.sum()/mask_loss.size(0)\n",
        "    return(avg_loss)\n",
        "\n",
        "# test LabelSmoothedCrossEntropyCriterion\n",
        "# cal1 = LabelSmoothedCrossEntropyCriterion()\n",
        "# print(cal1(outputs,tgt))\n",
        "\n",
        "# ignore_index not work correctly\n",
        "# def LabelSmoothedCrossEntropy(outputs , label,dictionary_length,smooth,padding_id):\n",
        "#   print(outputs.shape)\n",
        "#   print(label.shape)\n",
        "#   label_onehot = label.transpose(-1,-2).squeeze()\n",
        "#   outputs = outputs.transpose(-1,-2)\n",
        "#   cal_loss = nn.CrossEntropyLoss(ignore_index = padding_idx,reduction = \"mean\", label_smoothing=smooth)\n",
        "#   return cal_loss(outputs,label_onehot)"
      ],
      "metadata": {
        "id": "8JltOQM_wq4m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://arxiv.org/pdf/1512.00567.pdf page 7\n",
        "\n",
        "#Ref 1 : Hong-Yi Li ML2021 HW5\n",
        "\n",
        "# class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "#     def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "#         super().__init__()\n",
        "#         self.smoothing = smoothing\n",
        "#         self.ignore_index = ignore_index\n",
        "#         self.reduce = reduce\n",
        "\n",
        "#     def forward(self, lprobs, target):\n",
        "#         if target.dim() == lprobs.dim() - 1:\n",
        "#             target = target.unsqueeze(-1)\n",
        "#         # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
        "#         nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "#         #  reserve some probability for other labels. thus when calculating cross-entropy,\n",
        "#         # equivalent to summing the log probs of all labels\n",
        "#         smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "#         if self.ignore_index is not None:\n",
        "#             pad_mask = target.eq(self.ignore_index)\n",
        "#             nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "#             smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "#         else:\n",
        "#             nll_loss = nll_loss.squeeze(-1)\n",
        "#             smooth_loss = smooth_loss.squeeze(-1)\n",
        "#         if self.reduce:\n",
        "#             nll_loss = nll_loss.sum()\n",
        "#             smooth_loss = smooth_loss.sum()\n",
        "#         # when calculating cross-entropy, add the loss of other labels\n",
        "#         eps_i = self.smoothing / lprobs.size(-1)\n",
        "#         loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "#         return loss\n",
        "\n",
        "#Ref 2 : By hemingkx : https://github.com/hemingkx/ChineseNMT\n",
        "\n",
        "# class LabelSmoothing(nn.Module):\n",
        "#     \"\"\"Implement label smoothing.\"\"\"\n",
        "\n",
        "#     def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "#         super(LabelSmoothing, self).__init__()\n",
        "#         self.criterion = nn.KLDivLoss(size_average=False)\n",
        "#         self.padding_idx = padding_idx\n",
        "#         self.confidence = 1.0 - smoothing\n",
        "#         self.smoothing = smoothing\n",
        "#         self.size = size\n",
        "#         self.true_dist = None\n",
        "\n",
        "\n",
        "#     def forward(self, x, target):\n",
        "#         assert x.size(1) == self.size\n",
        "#         true_dist = x.data.clone()\n",
        "#         true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "#         true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "#         true_dist[:, self.padding_idx] = 0\n",
        "#         mask = torch.nonzero(target.data == self.padding_idx)\n",
        "#         if mask.dim() > 0:\n",
        "#             true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "#         self.true_dist = true_dist\n",
        "#         return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "metadata": {
        "id": "uaE0-tA9Q9cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "beam search\n",
        "------"
      ],
      "metadata": {
        "id": "33m5daxZ7Cpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "class Decode_With_Beam_Search(nn.Module):\n",
        "    def __init__(self,batch_size,model,beam_num,max_sentence_length,\n",
        "           dictionary_length,bos_id,padding_id):\n",
        "      super().__init__()\n",
        "      self.batch_size = batch_size\n",
        "      self.model = model\n",
        "      self.beam_num = beam_num\n",
        "      self.max_sentence_length = max_sentence_length\n",
        "      self.dictionary_length = dictionary_length\n",
        "      self.bos_id = bos_id\n",
        "      self.padding_id = padding_id\n",
        "      # decoder_input : {type : tensor , shape : Batch X 1 , value : bos_id}\n",
        "      decoder_input = torch.full(size = (self.batch_size,1),fill_value = self.bos_id)\n",
        "      self.register_buffer(\"decoder_input\",decoder_input)\n",
        "      # repeat : {type : tensor , shape : Batch ,value : beam_num}\n",
        "      # each row repeat beam_num times before concatenate\n",
        "      repeat = torch.full([self.batch_size],fill_value = self.beam_num)\n",
        "      self.register_buffer(\"repeat\",repeat)\n",
        "      # decoder_probability {type : tensor , shape : Batch X beam_num X 1, value : 0.1}\n",
        "      decoder_probability = torch.full(size = (self.batch_size,self.beam_num,1),fill_value = 0.0)\n",
        "      self.register_buffer(\"decoder_probability\",decoder_probability)\n",
        "\n",
        "      # padding : {type : tensor , shape : (Batch X beam_num) X (max_sentence_length-(id+1)) ,value : int}\n",
        "      padding = torch.full(size = (batch_size*self.beam_num,self.max_sentence_length),fill_value = self.padding_id)\n",
        "      self.register_buffer(\"padding\",padding)\n",
        "\n",
        "      # row : {type : tensor , shape : batch X 1, value : [[0],[1],[2],...]}\n",
        "      row = torch.tensor(range(self.batch_size)).unsqueeze(1)\n",
        "      self.register_buffer(\"row\",row)\n",
        "\n",
        "    def forward(self,is_last_batch,src,src_mask):\n",
        "\n",
        "      if is_last_batch:\n",
        "        batch = src.size(0)\n",
        "      else :\n",
        "        batch = self.batch_size\n",
        "\n",
        "      if self.beam_num > batch:\n",
        "        beam_num = batch\n",
        "      else :\n",
        "        beam_num = self.beam_num\n",
        "\n",
        "      decoder_input = self.decoder_input[:batch,:]\n",
        "      repeat = self.repeat[:batch]\n",
        "      decoder_probability = self.decoder_probability[:batch,:,]\n",
        "      padding = self.padding[:batch*beam_num,:]\n",
        "      row = self.row[:batch,:]\n",
        "\n",
        "      # decoder_beam_expand : {type : tensor , shape : (Batch X beam_num) X 1 ,value : bos_id}\n",
        "      decoder_beam_expand = torch.repeat_interleave(decoder_input,repeat,dim=0)\n",
        "\n",
        "      # memory : {type : tensor , shape : Batch X max_sentence_length X encoder_output_dim ,value : arbitary float}\n",
        "      memory = self.model.encoder(is_last_batch,src,src_mask)\n",
        "      # memory_beam_expand : {type : tensor ,\n",
        "      # shape : (Batch X n_beam) X max_sentence_length X encoder_output_dim ,value : float}\n",
        "      memory_beam_expand = torch.repeat_interleave(memory,repeat,dim=0)\n",
        "\n",
        "      gc.collect()\n",
        "\n",
        "      for id in range(self.max_sentence_length-1):\n",
        "\n",
        "        # decoder_n_beam : {type : tensor , shape : (Batch X beam_num) X (id+1) ,value : int}\n",
        "        # decoder_probability {type : tensor , shape : Batch X beam_num X 1 , value : log_softmax probability}\n",
        "        new_decoder_beam_expand , new_decoder_probability = self.get_next_word(is_last_batch,memory_beam_expand,\n",
        "        decoder_beam_expand,decoder_probability,id,batch,beam_num,padding,repeat,row)\n",
        "\n",
        "        decoder_beam_expand,decoder_probability = new_decoder_beam_expand , new_decoder_probability\n",
        "        if id%10 ==0:\n",
        "          print(new_decoder_beam_expand[0])\n",
        "        gc.collect()\n",
        "\n",
        "      # out_beam_expand : {type : tensor , shape : Batch X beam_num X (max_sentence_length) ,value : 0 or 1}\n",
        "      decoder_beam_expand = decoder_beam_expand.view(batch,beam_num,self.max_sentence_length)\n",
        "      # max_probability : {type : tensor , shape :  Batch  X 1 ,value : int(max prob index)}\n",
        "      max_probability = torch.argmax(input = decoder_probability,dim = 1)\n",
        "      # max_probability_expand : {type : tensor , shape :  Batch  X 1 X max_sentence_length ,\n",
        "      # value : [[A,A,A....],[B,B,B...],...](A,B are int)}\n",
        "      max_probability_expand = max_probability.expand(batch, self.max_sentence_length).unsqueeze(1)\n",
        "      # decoder_out : {type : tensor , shape :  Batch X max_sentence_length ,\n",
        "      # value : [[int,int,...],[int,int...],...]}\n",
        "      decoder_out =  torch.gather(input = decoder_beam_expand ,dim = 1, index = max_probability_expand).squeeze(1)\n",
        "\n",
        "      print(decoder_out[0])\n",
        "      return decoder_out,F.one_hot(decoder_out,self.dictionary_length).float()\n",
        "\n",
        "    def get_next_word(self,is_last_batch,memory,out,out_probability,id,batch,beam_num,padding,repeat,row):\n",
        "      # padding : {type : tensor , shape : (Batch X beam_num) X (max_sentence_length-(id+1)) ,value : int}\n",
        "      padding = padding[:,:self.max_sentence_length-(id+1)]\n",
        "      # out_padding : {type : tensor , shape : (Batch X beam_num) X max_sentence_length,\n",
        "      # value : [[bos_id],[any_id],...[padding_id],....] X Batch}\n",
        "      out_padding = torch.cat((out,padding),dim = 1)\n",
        "      # tgt_padding : {type : tensor , shape : (Batch X beam_num) X max_sentence_length ,value: bool}\n",
        "      tgt_padding = (out_padding == self.padding_id).squeeze(-1)\n",
        "      # out_add : {type : tensor , shape : Batch X beam_num X dictionary_length ,value : int}\n",
        "      out_add = self.model.decoder(is_last_batch,memory,out_padding,tgt_padding)[:,id,:]\\\n",
        "            .view(batch,beam_num,self.dictionary_length)\n",
        "      # out_n_beam : {type : tensor , shape : (Batch X beam_num) X (id+1) ,value : int}\n",
        "      # out_probability {type : tensor , shape : Batch X beam_num X 1 , value : log_softmax probability}\n",
        "      out , out_probability = self.beam_search_one_step(batch,beam_num,repeat,row,out,out_probability,out_add)\n",
        "\n",
        "      gc.collect()\n",
        "      return(out , out_probability)\n",
        "\n",
        "    def beam_search_one_step(self,batch,beam_num,repeat,row,sentences,p_sentences,n_beam_output):\n",
        "    # sentences : {type : tensor , shape : (batch X beam_num) X now_sentences_length X 1 value : int}\n",
        "    # p_sentences : {type : tensor , shape : batch X beam_num X 1 value : log_softmax probability}\n",
        "    # n_beam_output : {type : tensor , shape : batch X beam_num X dictionary_length,\n",
        "    # value : [P1,P2,P3...] X beam_num times (Pk in [0,1])}\n",
        "\n",
        "      '''\n",
        "      TO DO : (set beam num = K)\n",
        "      for every batch:\n",
        "      expand sentences(total number = K) K times (so there are K-square sentences),then concat with\n",
        "      the index of top K consequence of each beam(total K beams) in n_beam_output (so there are also K-square values).\n",
        "      '''\n",
        "      # sentences : {type : tensor , shape : batch X beam_num X now_sentences_length value : int}\n",
        "      sentences = sentences.view(batch,beam_num,-1)\n",
        "      # repeat : {type : tensor , shape : beam_num ,value : beam_num}\n",
        "      # each row repeat beam_num times before concatenate\n",
        "      repeat = repeat[:beam_num]\n",
        "      # sentences_expand : {type : tensor , shape : batch X (beam_num X beam_num) X now_sentences_length ,\n",
        "      # value : [[[A,B...] X beam_num times,[C,D...] X beam_num times}...] A,B,C,D...are int}\n",
        "      sentences_expand = torch.repeat_interleave(sentences,repeat,dim=1)\n",
        "\n",
        "      # topk_prob : {type : tensor , shape : batch X beam_num X beam_num, value : log_softmax probability}\n",
        "      # topk_index : {type : tensor , shape : batch X beam_num X beam_num, value : int}\n",
        "      topk_prob, topk_index = torch.topk(n_beam_output,dim = -1,k = beam_num)\n",
        "\n",
        "      # topk_index : {type : tensor , shape : batch X (beam_num X beam_num) X 1, value : int}\n",
        "      topk_index = topk_index.view(batch,-1,1)\n",
        "      # sentences : {type : tensor , shape : batch X (beam_num X beam_num) X (now_sentences_length+1), value : int}\n",
        "      sentences_expand = torch.cat((sentences_expand,topk_index),dim = -1)\n",
        "      '''\n",
        "      TO DO :\n",
        "      multipies p_sentences with the probability of top K consequence of each beam(total K beams) in n_beam_output\n",
        "      (so there are also K-square values).\n",
        "\n",
        "      The final step is to choose Top K consequence from K-square sentences by using p_sentences.\n",
        "      '''\n",
        "\n",
        "      # p_sentences : {type : tensor , shape : batch X (beam_num X beam_num),\n",
        "      # value : [P1,P2,P3...] X beam_num times (Pk is log_softmax probability)}\n",
        "      p_sentences = (p_sentences+topk_prob).view(batch,-1)\n",
        "      # p_sentences : {type : tensor , shape : batch X beam_num, value : log_softmax probability}\n",
        "      # p_index : {type : tensor , shape : batch X beam_num, value : int}\n",
        "      p_sentences, p_index = torch.topk(p_sentences, dim = 1, k = beam_num)\n",
        "      p_sentences = p_sentences.unsqueeze(-1)\n",
        "      # row : {type : tensor , shape : batch X 1, value : [[0],[1],[2],...]}\n",
        "      # sentences : {type : tensor , shape : batch X beam_num X (now_sentences_length+1), value : log_softmax probability}\n",
        "      new_sentences = sentences_expand[row, p_index].view(batch*beam_num,-1)\n",
        "      sentences.data = new_sentences.data\n",
        "      gc.collect()\n",
        "      return sentences,p_sentences\n",
        "\n",
        "# test decode_with_beam_search\n",
        "# batch = 3\n",
        "# beam_num = 2\n",
        "# sentences = torch.randint(0,8000,(batch*beam_num,5))\n",
        "# p_sentences = torch.log(torch.rand((batch , beam_num , 1)))\n",
        "# n_beam_output = torch.rand((batch , beam_num , 8000))\n",
        "# print(sentences,p_sentences,n_beam_output)\n",
        "# print(beam_search_one_step(sentences,p_sentences,n_beam_output))\n",
        "# repeat = torch.full([beam_num],fill_value = beam_num)\n",
        "# sentences_expand = torch.repeat_interleave(sentences.view(batch,beam_num,-1),repeat,dim=1)\n",
        "# print(sentences_expand,sentences_expand.shape)\n",
        "# decode_model = Decode_With_Beam_Search(32,model,2,400,8000,2,0)\n",
        "# outputs_in_word,outputs = decode_model(False,src,src_mask)\n",
        "# print(output[0])"
      ],
      "metadata": {
        "id": "6hrE6wYFxTIs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def beam_search_one_step(device,sentences,p_sentences,n_beam_output):\n",
        "    # sentences : {type : tensor , shape : (batch X beam_num) X now_sentences_length X 1 value : int}\n",
        "    # p_sentences : {type : tensor , shape : batch X beam_num X 1 value : log_softmax probability}\n",
        "    # n_beam_output : {type : tensor , shape : batch X beam_num X dictionary_length,\n",
        "    # value : [P1,P2,P3...] X beam_num times (Pk in [0,1])}\n",
        "\n",
        "    '''\n",
        "    TO DO : (set beam num = K)\n",
        "    for every batch:\n",
        "    expand sentences(total number = K) K times (so there are K-square sentences),then concat with\n",
        "    the index of top K consequence of each beam(total K beams) in n_beam_output (so there are also K-square values).\n",
        "    '''\n",
        "    batch = n_beam_output.size(0)\n",
        "    beam_num = n_beam_output.size(1)\n",
        "    # sentences : {type : tensor , shape : batch X beam_num X now_sentences_length value : int}\n",
        "    sentences = sentences.view(batch,beam_num,-1)\n",
        "    # repeat : {type : tensor , shape : beam_num ,value : beam_num}\n",
        "    # each row repeat beam_num times before concatenate\n",
        "    repeat = torch.full([beam_num],fill_value = beam_num)\n",
        "    repeat = repeat.to(device)\n",
        "    # sentences_expand : {type : tensor , shape : batch X (beam_num X beam_num) X now_sentences_length ,\n",
        "    # value : [[[A,B...] X beam_num times,[C,D...] X beam_num times}...] A,B,C,D...are int}\n",
        "    sentences_expand = torch.repeat_interleave(sentences,repeat,dim=1)\n",
        "\n",
        "    # topk_prob : {type : tensor , shape : batch X beam_num X beam_num, value : log_softmax probability}\n",
        "    # topk_index : {type : tensor , shape : batch X beam_num X beam_num, value : int}\n",
        "    topk_prob, topk_index = torch.topk(n_beam_output,dim = -1,k = beam_num)\n",
        "\n",
        "    # topk_index : {type : tensor , shape : batch X (beam_num X beam_num) X 1, value : int}\n",
        "    topk_index = topk_index.view(batch,-1,1)\n",
        "    # sentences : {type : tensor , shape : batch X (beam_num X beam_num) X (now_sentences_length+1), value : int}\n",
        "    sentences_expand = torch.cat((sentences_expand,topk_index),dim = -1)\n",
        "    '''\n",
        "    TO DO :\n",
        "    multipies p_sentences with the probability of top K consequence of each beam(total K beams) in n_beam_output\n",
        "    (so there are also K-square values).\n",
        "\n",
        "    The final step is to choose Top K consequence from K-square sentences by using p_sentences.\n",
        "    '''\n",
        "\n",
        "    # p_sentences : {type : tensor , shape : batch X (beam_num X beam_num),\n",
        "    # value : [P1,P2,P3...] X beam_num times (Pk is log_softmax probability)}\n",
        "    p_sentences = (p_sentences+topk_prob).view(batch,-1)\n",
        "    # p_sentences : {type : tensor , shape : batch X beam_num, value : log_softmax probability}\n",
        "    # p_index : {type : tensor , shape : batch X beam_num, value : int}\n",
        "    p_sentences, p_index = torch.topk(p_sentences, dim = 1, k = beam_num)\n",
        "    p_sentences = p_sentences.unsqueeze(-1)\n",
        "    # row : {type : tensor , shape : batch X 1, value : [[0],[1],[2],...]}\n",
        "    row = torch.tensor(range(batch)).unsqueeze(1)\n",
        "    row = row.to(device)\n",
        "    # sentences : {type : tensor , shape : batch X beam_num X (now_sentences_length+1), value : log_softmax probability}\n",
        "    new_sentences = sentences_expand[row, p_index].view(batch*beam_num,-1)\n",
        "    sentences.data = new_sentences.data\n",
        "    gc.collect()\n",
        "    return sentences,p_sentences\n",
        "\n",
        "def get_next_word(model,is_last_batch,device,memory,out,out_probability,id,batch,beam_num,max_sentence_length,dictionary_length,padding_id):\n",
        "    # padding : {type : tensor , shape : (Batch X beam_num) X (max_sentence_length-(id+1)) ,value : int}\n",
        "    padding = torch.full(size = (batch*beam_num,max_sentence_length-(id+1)),fill_value = padding_id)\n",
        "    padding = padding.to(device)\n",
        "    # out_padding : {type : tensor , shape : (Batch X beam_num) X max_sentence_length,\n",
        "    # value : [[bos_id],[any_id],...[padding_id],....] X Batch}\n",
        "    out_padding = torch.cat((out,padding),dim = 1)\n",
        "    # tgt_padding : {type : tensor , shape : (Batch X beam_num) X max_sentence_length ,value: bool}\n",
        "    tgt_padding = (out_padding == padding_id).squeeze(-1)\n",
        "    # out_add : {type : tensor , shape : Batch X beam_num X dictionary_length ,value : int}\n",
        "    out_add = model.decoder(is_last_batch,memory,out_padding,tgt_padding)[:,id,:].view(batch,beam_num,dictionary_length)\n",
        "    # out_n_beam : {type : tensor , shape : (Batch X beam_num) X (id+1) ,value : int}\n",
        "    # out_probability {type : tensor , shape : Batch X beam_num X 1 , value : log_softmax probability}\n",
        "    out , out_probability = beam_search_one_step(device,out,out_probability,out_add)\n",
        "\n",
        "    gc.collect()\n",
        "    return(out , out_probability)\n",
        "\n",
        "def decode_with_beam_search(device,is_last_batch,model,src,src_mask,beam_num,max_sentence_length,\n",
        "               dictionary_length,bos_id,padding_id):\n",
        "    with torch.no_grad():\n",
        "      batch = src.size(0)\n",
        "      # decoder_input : {type : tensor , shape : Batch X 1 , value : bos_id}\n",
        "      decoder_input = torch.full(size = (batch,1),fill_value = bos_id)\n",
        "      decoder_input = decoder_input.to(device)\n",
        "      # repeat : {type : tensor , shape : Batch ,value : beam_num}\n",
        "      # each row repeat beam_num times before concatenate\n",
        "      repeat = torch.full([batch],fill_value = beam_num)\n",
        "      repeat = repeat.to(device)\n",
        "      # decoder_beam_expand : {type : tensor , shape : (Batch X beam_num) X 1 ,value : bos_id}\n",
        "      decoder_beam_expand = torch.repeat_interleave(decoder_input,repeat,dim=0)\n",
        "\n",
        "      # decoder_probability {type : tensor , shape : Batch X beam_num X 1, value : 0.1}\n",
        "      decoder_probability = torch.full(size = (batch,beam_num,1),fill_value = 0.0)\n",
        "      decoder_probability = decoder_probability.to(device)\n",
        "\n",
        "      # memory : {type : tensor , shape : Batch X max_sentence_length X encoder_output_dim ,value : arbitary float}\n",
        "      memory = model.encoder(is_last_batch,src,src_mask)\n",
        "      # memory_beam_expand : {type : tensor ,\n",
        "      # shape : (Batch X n_beam) X max_sentence_length X encoder_output_dim ,value : float}\n",
        "      memory_beam_expand = torch.repeat_interleave(memory,repeat,dim=0)\n",
        "\n",
        "      gc.collect()\n",
        "\n",
        "      for id in range(max_sentence_length-1):\n",
        "\n",
        "        # decoder_n_beam : {type : tensor , shape : (Batch X beam_num) X (id+1) ,value : int}\n",
        "        # decoder_probability {type : tensor , shape : Batch X beam_num X 1 , value : log_softmax probability}\n",
        "        new_decoder_beam_expand , new_decoder_probability = \\\n",
        "        get_next_word(model,is_last_batch,device,memory_beam_expand,decoder_beam_expand,\n",
        "               decoder_probability,id,batch,beam_num,max_sentence_length,dictionary_length,padding_id)\n",
        "\n",
        "        decoder_beam_expand,decoder_probability = new_decoder_beam_expand , new_decoder_probability\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "      # out_beam_expand : {type : tensor , shape : Batch X beam_num X (max_sentence_length) ,value : 0 or 1}\n",
        "      decoder_beam_expand = decoder_beam_expand.view(batch,beam_num,max_sentence_length)\n",
        "      # max_probability : {type : tensor , shape :  Batch  X 1 ,value : int(max prob index)}\n",
        "      max_probability = torch.argmax(input = decoder_probability,dim = 1)\n",
        "      # max_probability_expand : {type : tensor , shape :  Batch  X 1 X max_sentence_length ,\n",
        "      # value : [[A,A,A....],[B,B,B...],...](A,B are int)}\n",
        "      max_probability_expand = max_probability.expand(batch, max_sentence_length).unsqueeze(1)\n",
        "      # decoder_out : {type : tensor , shape :  Batch X max_sentence_length ,\n",
        "      # value : [[int,int,...],[int,int...],...]}\n",
        "      decoder_out =  torch.gather(input = decoder_beam_expand ,dim = 1, index = max_probability_expand).squeeze(1)\n",
        "    print(decoder_out[0])\n",
        "    return decoder_out,F.one_hot(decoder_out,dictionary_length).float()\n",
        "\n",
        "# test decode_with_beam_search\n",
        "# batch = 3\n",
        "# beam_num = 2\n",
        "# sentences = torch.randint(0,8000,(batch*beam_num,5))\n",
        "# p_sentences = torch.log(torch.rand((batch , beam_num , 1)))\n",
        "# n_beam_output = torch.rand((batch , beam_num , 8000))\n",
        "# print(sentences,p_sentences,n_beam_output)\n",
        "# print(beam_search_one_step(sentences,p_sentences,n_beam_output))\n",
        "# repeat = torch.full([beam_num],fill_value = beam_num)\n",
        "# sentences_expand = torch.repeat_interleave(sentences.view(batch,beam_num,-1),repeat,dim=1)\n",
        "# print(sentences_expand,sentences_expand.shape)\n",
        "# output = decode_with_beam_search(model,src,src_mask,2,400,8000,)\n",
        "# print(output[0])"
      ],
      "metadata": {
        "id": "sz_zI5fDh9UU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bleu\n",
        "------"
      ],
      "metadata": {
        "id": "Pt1o-F1Ik8cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torcheval.metrics.functional.text import bleu\n",
        "def get_bleu_score(outputs,tgt,tgt_tokenizer,eos_id):\n",
        "    outputs = np.array(outputs.detach().tolist())\n",
        "    outputs = [x[:np.nonzero(x == eos_id)[0][0]].tolist() if len(np.nonzero(x == eos_id)[0])>0 \\\n",
        "              else x.tolist() for x in outputs ]\n",
        "\n",
        "    outputs_decode = tgt_tokenizer.decode(outputs)\n",
        "    out = [outputs_decode[i] for i in range(len(outputs_decode)) if len(outputs_decode[i])>= 4]\n",
        "    print(out)\n",
        "    out = [\" \".join(list(x)) for x in out]\n",
        "    print(out)\n",
        "    tgt_decode = tgt_tokenizer.decode(tgt.detach().tolist())\n",
        "    tgt = [tgt_decode[i] for i in range(len(outputs_decode)) if len(outputs_decode[i])>= 4]\n",
        "    print(tgt)\n",
        "    tgt = [\" \".join(list(x)) for x in tgt]\n",
        "    print(tgt)\n",
        "    return bleu.bleu_score(out, tgt, n_gram=4).detach().item()\n",
        "# test bleu\n",
        "# test_tokenizer = spm.SentencePieceProcessor(model_file = \"/content/spm_8000_zh.model\")\n",
        "# candidates = torch.tensor([[21,3,9,99,42],[5,78,89,3,31]])\n",
        "# references = torch.tensor([[18,5,9,3,42],[3,5,78,89,50]])\n",
        "# get_bleu_score(candidates,references,test_tokenizer,3)"
      ],
      "metadata": {
        "id": "ISpHxGVQk7dh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train and validation function\n",
        "------"
      ],
      "metadata": {
        "id": "GF4GshwGjz-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "\n",
        "def train_one_epoch(device,model,loss_calculator,is_last_batch,\n",
        "          src,tgt,src_mask,tgt_mask,dictionary_length,\n",
        "          optimizer):\n",
        "\n",
        "    outputs = model(is_last_batch,src,tgt,src_mask,tgt_mask)\n",
        "\n",
        "    loss = loss_calculator(is_last_batch,outputs,tgt)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return loss.detach().item()\n",
        "\n",
        "def valid(device,model,loss_calculator,batch_size_setting,valid_loader,beam_num,max_sentence_length,\n",
        "      dictionary_length,bos_id,eos_id,pad_id,tgt_tokenizer):\n",
        "    batch_loss = []\n",
        "    batch_bleu_score = []\n",
        "    with torch.no_grad():\n",
        "      for val_batch in tqdm(valid_loader,desc=\"valid_step\", unit=\" step\"):\n",
        "        src,tgt,src_mask,tgt_mask = val_batch\n",
        "        src,tgt,src_mask = src.to(device),tgt.to(device),src_mask.to(device)\n",
        "\n",
        "        batch_size = src.size(0)\n",
        "\n",
        "        is_last_batch = False\n",
        "        if batch_size != batch_size_setting:\n",
        "          is_last_batch = True\n",
        "        decode_model = Decode_With_Beam_Search(batch_size,model,beam_num,max_sentence_length,\n",
        "                            dictionary_length,bos_id,pad_id)\n",
        "        decode_model.to(device)\n",
        "        outputs_in_word,outputs = decode_model(is_last_batch,src,src_mask)\n",
        "        # outputs_in_word,outputs = decode_with_beam_search(device,is_last_batch,model,src,src_mask,beam_num,\\\n",
        "        #       max_sentence_length,dictionary_length,bos_id,pad_id)\n",
        "\n",
        "\n",
        "        loss = loss_calculator(is_last_batch,outputs,tgt)\n",
        "\n",
        "        bleu_score = get_bleu_score(outputs_in_word,tgt,tgt_tokenizer,eos_id)\n",
        "\n",
        "        batch_loss.append(loss)\n",
        "        batch_bleu_score.append(bleu_score)\n",
        "\n",
        "      avg_valid_loss = batch_loss.sum()/len(batch_loss).detach().item()\n",
        "      avg_bleu_score = batch_bleu_score.sum()/len(batch_bleu_score).detach().item()\n",
        "\n",
        "    return avg_valid_loss,avg_bleu_score"
      ],
      "metadata": {
        "id": "bwXkljhMFqKV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main function\n",
        "------"
      ],
      "metadata": {
        "id": "R7I8W9cQj7ES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def main(setting,dataset_is_prepare = False):\n",
        "\n",
        "    # set random seed\n",
        "    myseed = 1\n",
        "    np.random.seed(myseed)\n",
        "    torch.manual_seed(myseed)\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed_all(myseed)\n",
        "\n",
        "    # data set & tokenizer\n",
        "    if not dataset_is_prepare:\n",
        "        clean_data_and_save(\n",
        "        path_doc = setting[\"data_info\"][\"document\"],\n",
        "        raw_src_path = setting[\"data_info\"][\"source\"][\"raw_data_path\"],\n",
        "        raw_tgt_path = setting[\"data_info\"][\"target\"][\"raw_data_path\"],\n",
        "        clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "        clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"],\n",
        "        threshold = setting[\"tokenized_setting\"][\"max_l\"])\n",
        "\n",
        "        src_tokenizer,tgt_tokenizer = tokenized_data(\n",
        "            vocab_size = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "            tokenized_setting = {k:setting[\"tokenized_setting\"][k] for k in \\\n",
        "                      set(list(setting[\"tokenized_setting\"].keys()))-{\"vocab_size\",\"max_l\"}},\n",
        "            max_l = setting[\"tokenized_setting\"][\"max_l\"],\n",
        "            path_doc = setting[\"data_info\"][\"document\"],\n",
        "            clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "            clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"],\n",
        "            src_lang = setting[\"data_info\"][\"source\"][\"lang\"],\n",
        "            tgt_lang = setting[\"data_info\"][\"target\"][\"lang\"],\n",
        "            st_train_path = setting[\"data_info\"][\"source\"][\"tokenized_train_data\"],\n",
        "            st_val_path = setting[\"data_info\"][\"source\"][\"tokenized_val_data\"],\n",
        "            tt_train_path = setting[\"data_info\"][\"target\"][\"tokenized_train_data\"],\n",
        "            tt_val_path = setting[\"data_info\"][\"target\"][\"tokenized_val_data\"])\n",
        "    else:\n",
        "        src_tokenizer,tgt_tokenizer = get_tokenizers(\n",
        "            path_doc = setting[\"data_info\"][\"document\"],\n",
        "            vocab_size = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "            src_lang = setting[\"data_info\"][\"source\"][\"lang\"],\n",
        "            tgt_lang = setting[\"data_info\"][\"target\"][\"lang\"],)\n",
        "\n",
        "    # data loader\n",
        "    train_batch_size_setting = setting[\"training_hparas\"][\"train_batch_size\"]\n",
        "    valid_batch_size_setting = setting[\"training_hparas\"][\"valid_batch_size\"]\n",
        "    train_loader,valid_loader = get_data_set(\n",
        "        train_batch_size = train_batch_size_setting,\n",
        "        valid_batch_size = valid_batch_size_setting,\n",
        "        num_workers = setting[\"training_hparas\"][\"workers\"],\n",
        "        path_doc = setting[\"data_info\"][\"document\"],\n",
        "        st_train_path = setting[\"data_info\"][\"source\"][\"tokenized_train_data\"],\n",
        "        st_val_path = setting[\"data_info\"][\"source\"][\"tokenized_val_data\"],\n",
        "        tt_train_path = setting[\"data_info\"][\"target\"][\"tokenized_train_data\"],\n",
        "        tt_val_path = setting[\"data_info\"][\"target\"][\"tokenized_val_data\"],\n",
        "        pad_id = setting[\"tokenized_setting\"][\"pad_id\"])\n",
        "    train_iter = iter(train_loader)\n",
        "    valid_iter = iter(valid_loader)\n",
        "\n",
        "    # model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model =  build_model(\n",
        "          max_sentence_length = setting[\"tokenized_setting\"][\"max_l\"],\n",
        "          dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "          padding_idx = setting[\"tokenized_setting\"][\"pad_id\"],\n",
        "          encoder_embedding_dimension = setting[\"model\"][\"encoder_embedding_dimension\"],\n",
        "          decoder_embedding_dimension = setting[\"model\"][\"decoder_embedding_dimension\"],\n",
        "          feedforward_dimension = setting[\"model\"][\"feedforward_dimension\"],\n",
        "          num_heads = setting[\"model\"][\"num_heads\"],\n",
        "          dropout_p = setting[\"model\"][\"dropout_p\"],\n",
        "          layer_num = setting[\"model\"][\"layer_num\"])\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_loss_calculator = LabelSmoothedCrossEntropyCriterion(\n",
        "                batch_size = train_batch_size_setting,\n",
        "                dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "                padding_id = setting[\"tokenized_setting\"][\"pad_id\"],\n",
        "                smoothing = setting[\"training_hparas\"][\"label_smoothing\"])\n",
        "\n",
        "    valid_loss_calculator = LabelSmoothedCrossEntropyCriterion(\n",
        "            batch_size = valid_batch_size_setting,\n",
        "            dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "            padding_id = setting[\"tokenized_setting\"][\"pad_id\"],\n",
        "            smoothing = 0)\n",
        "\n",
        "    train_loss_calculator,valid_loss_calculator = \\\n",
        "    train_loss_calculator.to(device),valid_loss_calculator.to(device)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), **(setting[\"training_hparas\"][\"optimization\"][\"optimizer\"]))\n",
        "\n",
        "    Noam_optimizer = NoamOpt(\n",
        "             dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "             factor = setting[\"training_hparas\"][\"optimization\"][\"factor\"],\n",
        "             warmup = setting[\"training_hparas\"][\"optimization\"][\"warmup\"],\n",
        "             optimizer = optimizer)\n",
        "\n",
        "    # step\n",
        "    total_step = setting[\"training_hparas\"][\"total_step\"]\n",
        "    early_stop_epoch = setting[\"training_hparas\"][\"early_stop_step\"]\n",
        "    do_valid_steps = setting[\"training_hparas\"][\"do_valid_step\"]\n",
        "    early_stop_count = 0\n",
        "    progress_bar = tqdm(total = do_valid_steps, desc=\"train_step\", unit=\" step\")\n",
        "\n",
        "    # output datas\n",
        "    train_loss_every_batchs = []\n",
        "    valid_loss = []\n",
        "    bleu_score = []\n",
        "    best_bleu_score = 0\n",
        "\n",
        "    for step in range(total_step):\n",
        "\n",
        "      # training\n",
        "      # iter batch\n",
        "      try:\n",
        "        train_batch = next(train_iter)\n",
        "      except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        train_batch = next(train_iter)\n",
        "\n",
        "      # compute batch loss and update parameters in model\n",
        "      model.train()\n",
        "\n",
        "      src,tgt,src_mask,tgt_mask = train_batch\n",
        "      src,tgt,src_mask,tgt_mask = src.to(device),tgt.to(device),\\\n",
        "                     src_mask.to(device),tgt_mask.to(device)\n",
        "      batch_size = src.size(0)\n",
        "\n",
        "      is_last_batch = False\n",
        "      if batch_size != train_batch_size_setting:\n",
        "        is_last_batch = True\n",
        "\n",
        "      train_loss = train_one_epoch(\n",
        "              device = device,\n",
        "              model = model,\n",
        "              loss_calculator = train_loss_calculator,\n",
        "              is_last_batch = is_last_batch,\n",
        "              src = src,\n",
        "              tgt = tgt,\n",
        "              src_mask = src_mask,\n",
        "              tgt_mask = tgt_mask,\n",
        "              dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "              optimizer = Noam_optimizer)\n",
        "\n",
        "      train_loss_every_batchs.append(train_loss)\n",
        "      if (step+1) % (do_valid_steps//10) == 0:\n",
        "        print(train_loss_every_batchs[-1])\n",
        "\n",
        "      progress_bar.update()\n",
        "      if (step+1) % do_valid_steps == 0:\n",
        "\n",
        "        print(train_loss_every_batchs[-1])\n",
        "\n",
        "        progress_bar.close()\n",
        "\n",
        "        model.eval()\n",
        "        avg_val_loss,avg_bleu_score = valid(\n",
        "                        device = device,\n",
        "                        model = model,\n",
        "                        loss_calculator = valid_loss_calculator,\n",
        "                        batch_size_setting = valid_batch_size_setting,\n",
        "                        valid_loader = valid_loader,\n",
        "                        beam_num = setting[\"training_hparas\"][\"beam_num\"],\n",
        "                        max_sentence_length = setting[\"tokenized_setting\"][\"max_l\"],\n",
        "                        dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "                        bos_id = setting[\"tokenized_setting\"][\"bos_id\"],\n",
        "                        eos_id = setting[\"tokenized_setting\"][\"eos_id\"],\n",
        "                        pad_id = setting[\"tokenized_setting\"][\"pad_id\"],\n",
        "                        tgt_tokenizer = tgt_tokenizer)\n",
        "        valid_loss.append(avg_val_loss)\n",
        "        bleu_score.append(avg_bleu_score)\n",
        "\n",
        "        # print avg loss\n",
        "        print(f\"average train loss = {sum(train_loss_every_batchs[-1*do_valid_steps:-1])/len(do_valid_steps):.4f}\")\n",
        "        print(f\"average valid loss = {valid_loss[-1]:.4f}\")\n",
        "        print(f\"average valid loss = {bleu_score[-1]:.4f}\")\n",
        "\n",
        "        # saving model and check early stop criterion\n",
        "        if bleu_score[-1] > best_bleu_score:\n",
        "          torch.save(model.state_dict(), setting[\"tokenized_setting\"][\"model_saving_path\"])\n",
        "        else :\n",
        "          early_stop_count += 1\n",
        "\n",
        "        if early_stop_count == early_stop_epoch:\n",
        "          break\n",
        "\n",
        "        progress_bar = tqdm(total = do_valid_steps, desc=\"train_step\", unit=\" step\")\n",
        "\n",
        "    progress_bar.close()\n",
        "\n",
        "    return train_loss_every_batchs,valid_loss,bleu_score"
      ],
      "metadata": {
        "id": "e7hn-NAjHSQ5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(setting,dataset_is_prepare = True)\n",
        "# gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aGUYF3WnrI9F",
        "outputId": "cc6bfc38-c14e-4165-c430-9b7a86b26df5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 384064/384064 [00:33<00:00, 11381.91it/s]\n",
            "100%|██████████| 384064/384064 [00:32<00:00, 11990.70it/s]\n",
            "100%|██████████| 3879/3879 [00:00<00:00, 16375.39it/s]\n",
            "100%|██████████| 3879/3879 [00:00<00:00, 15980.95it/s]\n",
            "train_step:  10%|█         | 100/1000 [00:49<07:27,  2.01 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "174.676025390625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  20%|██        | 200/1000 [01:40<06:46,  1.97 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "173.82054138183594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  30%|███       | 300/1000 [02:30<05:54,  1.98 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "169.50616455078125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  40%|████      | 400/1000 [03:21<05:03,  1.98 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "132.4024200439453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  50%|█████     | 500/1000 [04:12<04:13,  1.97 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "159.17282104492188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  60%|██████    | 600/1000 [05:02<03:22,  1.98 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160.2572784423828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  70%|███████   | 700/1000 [05:53<02:32,  1.97 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "139.4862060546875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  80%|████████  | 800/1000 [06:44<01:41,  1.97 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127.3679428100586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  90%|█████████ | 900/1000 [07:34<00:50,  1.98 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130.0306396484375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step: 100%|██████████| 1000/1000 [08:25<00:00,  1.98 step/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "143.32481384277344\n",
            "143.32481384277344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rvalid_step:   0%|          | 0/39 [00:00<?, ? step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   2, 2174], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2177,    3,    0, 2177], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2175,    3,    0, 2177,    3,    0,\n",
            "        2177,    3], device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2175,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2175,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2175,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rvalid_step:   3%|▎         | 1/39 [04:30<2:51:35, 270.94s/ step]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   2, 2174, 2224, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175, 2176, 2175,\n",
            "        2176, 2175, 2176, 2175,    3,    0, 2175,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2175,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2175,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2175,    3,    0,\n",
            "        2175,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0,\n",
            "        2177,    3,    0, 2177], device='cuda:0')\n",
            "['說,我,我,我,我,我,我,我,我,我,我,我,我,', '說,我,我,我,我,我,我,我,', ',我,我,我,我,我,', '說,我,我,我,', '說,我,我,', '說,我,我,我,', '我們,我,', '說,我,我,我,我,我,我,我,我,我,我,我,我,我們,', ',我,我,我,我,我,我,我,我,我,我,我,我,我,我,', '說,我們,我,', '看的,我,我們,我們。', '我們,我們,我。', ',我我我,我,我,我,我,我,', '我,我,我,我,我,我,我,我。', ',我,我們,我們,我們,我們,', '說,我,我,我,', '說,我,我,我,我,我,我。', ',我我,我,我,我,我,我,我,我,我,', ',我們,', ',我,我,我們,我們,我,我,我,我,我,我,我們,我們,我,我,', ',我們,我,我,我們,我們,我,我,我們,我,我們,', ',我,我,我們,我們,我們,我們,我們,我們。', '的,我,我,我,我,我,我,', '說,我,', '看,我,我,我。', ',我,我,我,我,我,我,我,', '看,我,我,我,我,我,我,我,我,我,我,我。', '我,我,我,我,我,我,我,我,我,我,我,我,我,我,', ',我們,我,我,我,我,我,我,我,我,我,我,我,我,我,', '說,我,我,我,', '說,我,我,我,我,我,我,我,我,我,我,我,我,我,', ',我們,我,我,我,我,我,我,我,我,我,', ',我,我,我,我,我,我,我,我,', '說,我,我,我,我,我,我,我,我,我,我,我,我,我們,', ',我,我,我,我,我,我,我,我,我,我,我,', '說,我,我,我,我,我,我,我,我,我,我,我,我,', ',我們,我們,我們,我們,我們,我們,', '說,我,我,', ',我,我,', '說,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,', ',我,我,我,我,我,我,我,', '看,我,我,我,我們,我,', '我我我,我,我,我,我,我。', '說,我,我,我,我,', '說,我,我,我,我,我,', '看,我,我,我,我,我,我,我,我,我,我,我,我,我,', '我我,我,我,我,我,我,我,', '我們,我,', '說,我,我們,我們,我們,我們,我,我,我們,', '說,我們,我們,我們,我們,我們,我們,我,我們,', ',我們,我,我,', '所以,我,我,我,我,', '說,我,我,我,我,我,', '我我我,我,我,', ',我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,', '說,我,我,我,我,', '說,我,我,我,我,我,', ',我,我,我,我,我,我,我,我,我。', ',我,我們,我們,我們,我們,我們,我們,我們,我們,我們,', '我們,我們,我們,我們,我們,我們,我,我,我們,', '的,我,我,我,我,我,我,我,我,我,我,我,我,我們,', '看,我,我,我,', ',我,我,我,我,', ',我,我,我,我。', '說,我,我,我,我,', '我,我,我,我,我,我,我,我。', '的,我,', '我們,我們,我們,我們,我們,我們,我們,我,我,', ',我,我,我,我,我,我,我們,', '我我,我,我,我,我,我,我,', '我們,我們,我們,我們,我,我,我,', '我,我,我,', ',我,我,我,我,', ',我,我,我,我,我,我,我,我,我,我,', '我我,我,我。', '所以,我,我們,我們,我們,我,我,我,我,我,我們,我,我,', ',我,我,我,', ',我,我,我,我們,我,我,我,', '說,我,我,我,我,我,我,我,我,我們,', ',我,我們,我們,我們,我們,', '我我,我,我,我,我,我,我,我,我,我,我,', '我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,我,', '說,我,我,我,我,我,我,我,我,']\n",
            "['說 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '說 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', ', 我 , 我 , 我 , 我 , 我 ,', '說 , 我 , 我 , 我 ,', '說 , 我 , 我 ,', '說 , 我 , 我 , 我 ,', '我 們 , 我 ,', '說 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 們 ,', ', 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '說 , 我 們 , 我 ,', '看 的 , 我 , 我 們 , 我 們 。', '我 們 , 我 們 , 我 。', ', 我 我 我 , 我 , 我 , 我 , 我 , 我 ,', '我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 。', ', 我 , 我 們 , 我 們 , 我 們 , 我 們 ,', '說 , 我 , 我 , 我 ,', '說 , 我 , 我 , 我 , 我 , 我 , 我 。', ', 我 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', ', 我 們 ,', ', 我 , 我 , 我 們 , 我 們 , 我 , 我 , 我 , 我 , 我 , 我 , 我 們 , 我 們 , 我 , 我 ,', ', 我 們 , 我 , 我 , 我 們 , 我 們 , 我 , 我 , 我 們 , 我 , 我 們 ,', ', 我 , 我 , 我 們 , 我 們 , 我 們 , 我 們 , 我 們 , 我 們 。', '的 , 我 , 我 , 我 , 我 , 我 , 我 ,', '說 , 我 ,', '看 , 我 , 我 , 我 。', ', 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '看 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 。', '我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', ', 我 們 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '說 , 我 , 我 , 我 ,', '說 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', ', 我 們 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', ', 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '說 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 們 ,', ', 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '說 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', ', 我 們 , 我 們 , 我 們 , 我 們 , 我 們 , 我 們 ,', '說 , 我 , 我 ,', ', 我 , 我 ,', '說 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', ', 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '看 , 我 , 我 , 我 , 我 們 , 我 ,', '我 我 我 , 我 , 我 , 我 , 我 , 我 。', '說 , 我 , 我 , 我 , 我 ,', '說 , 我 , 我 , 我 , 我 , 我 ,', '看 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '我 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '我 們 , 我 ,', '說 , 我 , 我 們 , 我 們 , 我 們 , 我 們 , 我 , 我 , 我 們 ,', '說 , 我 們 , 我 們 , 我 們 , 我 們 , 我 們 , 我 們 , 我 , 我 們 ,', ', 我 們 , 我 , 我 ,', '所 以 , 我 , 我 , 我 , 我 ,', '說 , 我 , 我 , 我 , 我 , 我 ,', '我 我 我 , 我 , 我 ,', ', 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '說 , 我 , 我 , 我 , 我 ,', '說 , 我 , 我 , 我 , 我 , 我 ,', ', 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 。', ', 我 , 我 們 , 我 們 , 我 們 , 我 們 , 我 們 , 我 們 , 我 們 , 我 們 , 我 們 ,', '我 們 , 我 們 , 我 們 , 我 們 , 我 們 , 我 們 , 我 , 我 , 我 們 ,', '的 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 們 ,', '看 , 我 , 我 , 我 ,', ', 我 , 我 , 我 , 我 ,', ', 我 , 我 , 我 , 我 。', '說 , 我 , 我 , 我 , 我 ,', '我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 。', '的 , 我 ,', '我 們 , 我 們 , 我 們 , 我 們 , 我 們 , 我 們 , 我 們 , 我 , 我 ,', ', 我 , 我 , 我 , 我 , 我 , 我 , 我 們 ,', '我 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '我 們 , 我 們 , 我 們 , 我 們 , 我 , 我 , 我 ,', '我 , 我 , 我 ,', ', 我 , 我 , 我 , 我 ,', ', 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '我 我 , 我 , 我 。', '所 以 , 我 , 我 們 , 我 們 , 我 們 , 我 , 我 , 我 , 我 , 我 , 我 們 , 我 , 我 ,', ', 我 , 我 , 我 ,', ', 我 , 我 , 我 , 我 們 , 我 , 我 , 我 ,', '說 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 們 ,', ', 我 , 我 們 , 我 們 , 我 們 , 我 們 ,', '我 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,', '說 , 我 , 我 , 我 , 我 , 我 , 我 , 我 , 我 ,']\n",
            "['西方工人失業,看到他們的收入停滯不前,顯然人們不得不考慮新的競爭政策,工人需要再培訓,工人需要新的技能。', 'kathryn:我很怕不知道會看到啥。', '我也捕捉到整個景觀時間轉換之際。', '人生將是由很多可能的線條所組成朝向各種可能的方向去在立體的空間', '跟上德雷克的節奏', '雖然有些地方我們想要修復', '有一個大改革近來持續的在進行,是針對我們所知道的機械學習。', '而且這些公司普遍相信,只要將攻擊歸咎於某個國家,就可以逃避主管機關的監督──或是至少拖延一段時間。', '所以我今天來到這裡,我想應該是我們來到這裡,試著要用這個用資料組成的,善的病毒,來感染你們,我們稱呼它為真相運動者。', '會繼續報導嗎?', '這就是所謂的「禮貌性的不在意」。', '只花了一分鐘', '我也知道聽眾裡有很多人想要生孩子,但是對未來感到害怕。', '我失去平衡並倒下,然後發現有四個全副武裝的人圍繞著我,', '但你也可能這麼問,喂,到底有沒有隱居褐蛛坐在我旁邊的椅子上?', '它會喊出被過濾掉的隱藏聲音。', '就我所知,它只是在測量我們的睡眠習慣。', '但是我只是想確定我們得先花時間考慮我們復活猛瑪的理由', '它是那樣的簡單設計與基礎。', '我們作為醫師和科學家,理智上明白到人類作為一個物種,也只不過是一個物種,並不比其他物種更為獨特。', '還有我們社會裡的各種機構正在以流感傳播的速度一樣幫助產生虐待傾向的男人究竟這些機構扮演著什麼角色?', '但,那就是我們在談的,也是我們所做的。', '答案也許隨著研究的特定概念的變化而變化,但它可以像是一個扭曲的禪宗公案。', '他們就是知道他們是什麼人,有著和你們同等的信心。', '這不是疥瘡。', '之後,在密克羅尼西亞的雅浦島上發生了非常不尋常的事。', '這一塊生鏽的鎳鐵,看起來可能沒有什麼特別之處。', '我收到了第一封的超級粉絲來信這個小孩超愛《monkeyboy》愛到他想要個《monkeyboy》生日蛋糕', '身體裡有細菌存在,不僅正常而且事實上對於生命諸事例如消化、預防疾病等等都非常地重要', '他們同時也生產紅樹林在人工林裡。', '這是一個很大的問題政府可以入侵電腦恐怖分子、有戀童癖的人、販毒者、記者以及人權運動人士都使用相同種類的電腦', '「組織犯罪及貪腐報告計畫」成員包括記者和公民他們藉由群眾提供的情報揭露全球獨裁者和恐怖份子如何濫用公共資金有一個更具戲劇性的例子,墨西哥在過去6年間受五萬件和毒品有關的謀殺案荼毒。', '當你走過像這樣的創傷經歷,大家對待你的方式會不同。', '瞭解病人的生活,即他們生活及工作的背景很重要是一回事,但有能力在我們工作的系統中為之出力又是另一回事。', '看著這些數字增長著,令人感到渺小自卑,我迫不及待地想讓它變成1億。', '他們在討論他們的小卡通人物他們的小戰果或徽章,或他們能拿到手的東西,', '所以我們相當確定海洋影響了這個過程', '他們不想被認為能力不足,他們不知道該向誰尋求幫助,他們不想給別人造成負擔。', '讓我帶各位回顧看看為什麼我們要這麼做。', '資金充足他們得以起步不必涉及到風險投資只用全力做出讓大家驚喜的產品', '原因就是他的身體對食物產生了一種會導致程序性細胞死亡的', '而接下來的這個是藉由cymascope來播放pinkfloyd的\"machine\"', '我在非常年輕的時候就加入這個領域,當時我七歲,', '咬痕明顯集中於面部', '他們做的是類似綠色能源諮商的事情。', '還不能存檔,所以每次當機我就得一次又一次輸入所有的公式。', '我說,\"所羅門,我們的交易還在進行嗎?', '我們總結吧。', '每天,我們都走不同的路線,才不會讓人對我們要去哪裡起疑。', '想像一下你可以直接去網路上--這已成為現實--在停產的數據庫中找到這個備用零件找到這個備用零件下載關於這個產品的信息并在家裡把它做出來並且馬上在你需要時就能用', '會根據情況擴大或縮小。', '男人的工作變得更重要:他們必須搬石頭、砍樹、犁田。', '因為薇薇安深知離鄉背井的感受。', '我思考為什麼喜愛自己的工作。', '當使用極短波長時,就像是用音叉讓天線產生共鳴,這會讓天線產生出比起地底反射獲得的還要更高的能量,這樣會造成分析上更多的困難。', '這有兩面很不一樣的牆壁,很不相同的幾何圖片。', '大概有二十五家出版商來找過我,他們有興趣將「被忽略」出版成書。', '因此,我認為角色扮演是很有價值的,可用在思考各種體驗。', '因此,我們可以從這張圖中獲悉的是勞動力供應狀況,因此,勞動力人口,在德國將呈下降趨勢,而且這一趨勢會越來越明顯。', '我們都傾向於認爲知覺就像個窗口,從那能看到現實的真貌。', '這黃線包含了分析87名伊朗的決策人士,以及大量的外部有力人士他們對伊朗施壓要改變它的行為,許多在美國以及埃及的人士,還有沙烏地阿拉伯、蘇俄、歐盟、日本,等等', '廢棄物不僅是減少而已;根本就是沒有廢棄物。', '但處女膜完全不需要破裂。', '所以,紀錄文件會幫忙解決這個問題。', '在那裡,他們拿出幾隻這種綠色的猴樹蛙,這些可是大吸蟲,牠們就像這樣,牠們開始舔舐。', '我有塑膠叉子,你不知道嗎?', '說話就像呼吸一樣自然。', '我們花非常大量的時間待在建築物內,建築物是被高度控制的環境,像這座建築物--裝設有機械通風系統包括過濾,暖氣和空氣調節設備。', '當然,剛剛所描述的場景,在場只要稍微懂政治的都知道這超難做到,我完全同意這種想法。', '我曾被款待搭上私人飛機,飛到世界各地。', '我們想做的僅僅只是停止讓塑膠袋繼續包圍破壞我們美麗的家園。', '也佈置回家作業。', '但,需要了解的關鍵點是,這種加速現象已經發生很長一段時間了。', '所以我想,也許對我而言,緊張一點可能也不錯。', '能和她在一起,我很幸運', '你在那兒看到的大部分是吸入器,頂上是個很小的gps收發器,讓你知道哮喘發生的時間和地點給你對自身脆弱性與時間和環境因素間的關聯有一個全新的瞭解。', '所以,如果你跟它說話,它可能會回話。', '也同樣是那時候美國國内抵制吃薯條', '如果今年剛巧就是政黨輪替的一年,如你所言,這不再只是一個黨的問題,而你凝聚了雙方人馬,有科學、各種投資機會背書,有你說服大家的理由背書,朋友,這真是非常振奮人心,', '現在我們應該如何處理它?', '我甚至還準備了《陰道獨白》裡的台詞。', '我知道這個,因為在我寫這本書時,曾和150個人對談,許多來自科學和科技領域,他們都覺得自己被分配到小孩桌。', '在座有以下情況請舉手:有人22歲的時候從未犯錯,或從未做過後悔的事嗎?']\n",
            "['西 方 工 人 失 業 , 看 到 他 們 的 收 入 停 滯 不 前 , 顯 然 人 們 不 得 不 考 慮 新 的 競 爭 政 策 , 工 人 需 要 再 培 訓 , 工 人 需 要 新 的 技 能 。', 'k a t h r y n : 我 很 怕 不 知 道 會 看 到 啥 。', '我 也 捕 捉 到 整 個 景 觀 時 間 轉 換 之 際 。', '人 生 將 是 由 很 多 可 能 的 線 條 所 組 成 朝 向 各 種 可 能 的 方 向 去 在 立 體 的 空 間', '跟 上 德 雷 克 的 節 奏', '雖 然 有 些 地 方 我 們 想 要 修 復', '有 一 個 大 改 革 近 來 持 續 的 在 進 行 , 是 針 對 我 們 所 知 道 的 機 械 學 習 。', '而 且 這 些 公 司 普 遍 相 信 , 只 要 將 攻 擊 歸 咎 於 某 個 國 家 , 就 可 以 逃 避 主 管 機 關 的 監 督 ─ ─ 或 是 至 少 拖 延 一 段 時 間 。', '所 以 我 今 天 來 到 這 裡 , 我 想 應 該 是 我 們 來 到 這 裡 , 試 著 要 用 這 個 用 資 料 組 成 的 , 善 的 病 毒 , 來 感 染 你 們 , 我 們 稱 呼 它 為 真 相 運 動 者 。', '會 繼 續 報 導 嗎 ?', '這 就 是 所 謂 的 「 禮 貌 性 的 不 在 意 」 。', '只 花 了 一 分 鐘', '我 也 知 道 聽 眾 裡 有 很 多 人 想 要 生 孩 子 , 但 是 對 未 來 感 到 害 怕 。', '我 失 去 平 衡 並 倒 下 , 然 後 發 現 有 四 個 全 副 武 裝 的 人 圍 繞 著 我 ,', '但 你 也 可 能 這 麼 問 , 喂 , 到 底 有 沒 有 隱 居 褐 蛛 坐 在 我 旁 邊 的 椅 子 上 ?', '它 會 喊 出 被 過 濾 掉 的 隱 藏 聲 音 。', '就 我 所 知 , 它 只 是 在 測 量 我 們 的 睡 眠 習 慣 。', '但 是 我 只 是 想 確 定 我 們 得 先 花 時 間 考 慮 我 們 復 活 猛 瑪 的 理 由', '它 是 那 樣 的 簡 單 設 計 與 基 礎 。', '我 們 作 為 醫 師 和 科 學 家 , 理 智 上 明 白 到 人 類 作 為 一 個 物 種 , 也 只 不 過 是 一 個 物 種 , 並 不 比 其 他 物 種 更 為 獨 特 。', '還 有 我 們 社 會 裡 的 各 種 機 構 正 在 以 流 感 傳 播 的 速 度 一 樣 幫 助 產 生 虐 待 傾 向 的 男 人 究 竟 這 些 機 構 扮 演 著 什 麼 角 色 ?', '但 , 那 就 是 我 們 在 談 的 , 也 是 我 們 所 做 的 。', '答 案 也 許 隨 著 研 究 的 特 定 概 念 的 變 化 而 變 化 , 但 它 可 以 像 是 一 個 扭 曲 的 禪 宗 公 案 。', '他 們 就 是 知 道 他 們 是 什 麼 人 , 有 著 和 你 們 同 等 的 信 心 。', '這 不 是 疥 瘡 。', '之 後 , 在 密 克 羅 尼 西 亞 的 雅 浦 島 上 發 生 了 非 常 不 尋 常 的 事 。', '這 一 塊 生 鏽 的 鎳 鐵 , 看 起 來 可 能 沒 有 什 麼 特 別 之 處 。', '我 收 到 了 第 一 封 的 超 級 粉 絲 來 信 這 個 小 孩 超 愛 《 m o n k e y b o y 》 愛 到 他 想 要 個 《 m o n k e y b o y 》 生 日 蛋 糕', '身 體 裡 有 細 菌 存 在 , 不 僅 正 常 而 且 事 實 上 對 於 生 命 諸 事 例 如 消 化 、 預 防 疾 病 等 等 都 非 常 地 重 要', '他 們 同 時 也 生 產 紅 樹 林 在 人 工 林 裡 。', '這 是 一 個 很 大 的 問 題 政 府 可 以 入 侵 電 腦 恐 怖 分 子 、 有 戀 童 癖 的 人 、 販 毒 者 、 記 者 以 及 人 權 運 動 人 士 都 使 用 相 同 種 類 的 電 腦', '「 組 織 犯 罪 及 貪 腐 報 告 計 畫 」 成 員 包 括 記 者 和 公 民 他 們 藉 由 群 眾 提 供 的 情 報 揭 露 全 球 獨 裁 者 和 恐 怖 份 子 如 何 濫 用 公 共 資 金 有 一 個 更 具 戲 劇 性 的 例 子 , 墨 西 哥 在 過 去 6 年 間 受 五 萬 件 和 毒 品 有 關 的 謀 殺 案 荼 毒 。', '當 你 走 過 像 這 樣 的 創 傷 經 歷 , 大 家 對 待 你 的 方 式 會 不 同 。', '瞭 解 病 人 的 生 活 , 即 他 們 生 活 及 工 作 的 背 景 很 重 要 是 一 回 事 , 但 有 能 力 在 我 們 工 作 的 系 統 中 為 之 出 力 又 是 另 一 回 事 。', '看 著 這 些 數 字 增 長 著 , 令 人 感 到 渺 小 自 卑 , 我 迫 不 及 待 地 想 讓 它 變 成 1 億 。', '他 們 在 討 論 他 們 的 小 卡 通 人 物 他 們 的 小 戰 果 或 徽 章 , 或 他 們 能 拿 到 手 的 東 西 ,', '所 以 我 們 相 當 確 定 海 洋 影 響 了 這 個 過 程', '他 們 不 想 被 認 為 能 力 不 足 , 他 們 不 知 道 該 向 誰 尋 求 幫 助 , 他 們 不 想 給 別 人 造 成 負 擔 。', '讓 我 帶 各 位 回 顧 看 看 為 什 麼 我 們 要 這 麼 做 。', '資 金 充 足 他 們 得 以 起 步 不 必 涉 及 到 風 險 投 資 只 用 全 力 做 出 讓 大 家 驚 喜 的 產 品', '原 因 就 是 他 的 身 體 對 食 物 產 生 了 一 種 會 導 致 程 序 性 細 胞 死 亡 的', '而 接 下 來 的 這 個 是 藉 由 c y m a s c o p e 來 播 放 p i n k f l o y d 的 \" m a c h i n e \"', '我 在 非 常 年 輕 的 時 候 就 加 入 這 個 領 域 , 當 時 我 七 歲 ,', '咬 痕 明 顯 集 中 於 面 部', '他 們 做 的 是 類 似 綠 色 能 源 諮 商 的 事 情 。', '還 不 能 存 檔 , 所 以 每 次 當 機 我 就 得 一 次 又 一 次 輸 入 所 有 的 公 式 。', '我 說 , \" 所 羅 門 , 我 們 的 交 易 還 在 進 行 嗎 ?', '我 們 總 結 吧 。', '每 天 , 我 們 都 走 不 同 的 路 線 , 才 不 會 讓 人 對 我 們 要 去 哪 裡 起 疑 。', '想 像 一 下 你 可 以 直 接 去 網 路 上 - - 這 已 成 為 現 實 - - 在 停 產 的 數 據 庫 中 找 到 這 個 備 用 零 件 找 到 這 個 備 用 零 件 下 載 關 於 這 個 產 品 的 信 息 并 在 家 裡 把 它 做 出 來 並 且 馬 上 在 你 需 要 時 就 能 用', '會 根 據 情 況 擴 大 或 縮 小 。', '男 人 的 工 作 變 得 更 重 要 : 他 們 必 須 搬 石 頭 、 砍 樹 、 犁 田 。', '因 為 薇 薇 安 深 知 離 鄉 背 井 的 感 受 。', '我 思 考 為 什 麼 喜 愛 自 己 的 工 作 。', '當 使 用 極 短 波 長 時 , 就 像 是 用 音 叉 讓 天 線 產 生 共 鳴 , 這 會 讓 天 線 產 生 出 比 起 地 底 反 射 獲 得 的 還 要 更 高 的 能 量 , 這 樣 會 造 成 分 析 上 更 多 的 困 難 。', '這 有 兩 面 很 不 一 樣 的 牆 壁 , 很 不 相 同 的 幾 何 圖 片 。', '大 概 有 二 十 五 家 出 版 商 來 找 過 我 , 他 們 有 興 趣 將 「 被 忽 略 」 出 版 成 書 。', '因 此 , 我 認 為 角 色 扮 演 是 很 有 價 值 的 , 可 用 在 思 考 各 種 體 驗 。', '因 此 , 我 們 可 以 從 這 張 圖 中 獲 悉 的 是 勞 動 力 供 應 狀 況 , 因 此 , 勞 動 力 人 口 , 在 德 國 將 呈 下 降 趨 勢 , 而 且 這 一 趨 勢 會 越 來 越 明 顯 。', '我 們 都 傾 向 於 認 爲 知 覺 就 像 個 窗 口 , 從 那 能 看 到 現 實 的 真 貌 。', '這 黃 線 包 含 了 分 析 8 7 名 伊 朗 的 決 策 人 士 , 以 及 大 量 的 外 部 有 力 人 士 他 們 對 伊 朗 施 壓 要 改 變 它 的 行 為 , 許 多 在 美 國 以 及 埃 及 的 人 士 , 還 有 沙 烏 地 阿 拉 伯 、 蘇 俄 、 歐 盟 、 日 本 , 等 等', '廢 棄 物 不 僅 是 減 少 而 已 ; 根 本 就 是 沒 有 廢 棄 物 。', '但 處 女 膜 完 全 不 需 要 破 裂 。', '所 以 , 紀 錄 文 件 會 幫 忙 解 決 這 個 問 題 。', '在 那 裡 , 他 們 拿 出 幾 隻 這 種 綠 色 的 猴 樹 蛙 , 這 些 可 是 大 吸 蟲 , 牠 們 就 像 這 樣 , 牠 們 開 始 舔 舐 。', '我 有 塑 膠 叉 子 , 你 不 知 道 嗎 ?', '說 話 就 像 呼 吸 一 樣 自 然 。', '我 們 花 非 常 大 量 的 時 間 待 在 建 築 物 內 , 建 築 物 是 被 高 度 控 制 的 環 境 , 像 這 座 建 築 物 - - 裝 設 有 機 械 通 風 系 統 包 括 過 濾 , 暖 氣 和 空 氣 調 節 設 備 。', '當 然 , 剛 剛 所 描 述 的 場 景 , 在 場 只 要 稍 微 懂 政 治 的 都 知 道 這 超 難 做 到 , 我 完 全 同 意 這 種 想 法 。', '我 曾 被 款 待 搭 上 私 人 飛 機 , 飛 到 世 界 各 地 。', '我 們 想 做 的 僅 僅 只 是 停 止 讓 塑 膠 袋 繼 續 包 圍 破 壞 我 們 美 麗 的 家 園 。', '也 佈 置 回 家 作 業 。', '但 , 需 要 了 解 的 關 鍵 點 是 , 這 種 加 速 現 象 已 經 發 生 很 長 一 段 時 間 了 。', '所 以 我 想 , 也 許 對 我 而 言 , 緊 張 一 點 可 能 也 不 錯 。', '能 和 她 在 一 起 , 我 很 幸 運', '你 在 那 兒 看 到 的 大 部 分 是 吸 入 器 , 頂 上 是 個 很 小 的 g p s 收 發 器 , 讓 你 知 道 哮 喘 發 生 的 時 間 和 地 點 給 你 對 自 身 脆 弱 性 與 時 間 和 環 境 因 素 間 的 關 聯 有 一 個 全 新 的 瞭 解 。', '所 以 , 如 果 你 跟 它 說 話 , 它 可 能 會 回 話 。', '也 同 樣 是 那 時 候 美 國 國 内 抵 制 吃 薯 條', '如 果 今 年 剛 巧 就 是 政 黨 輪 替 的 一 年 , 如 你 所 言 , 這 不 再 只 是 一 個 黨 的 問 題 , 而 你 凝 聚 了 雙 方 人 馬 , 有 科 學 、 各 種 投 資 機 會 背 書 , 有 你 說 服 大 家 的 理 由 背 書 , 朋 友 , 這 真 是 非 常 振 奮 人 心 ,', '現 在 我 們 應 該 如 何 處 理 它 ?', '我 甚 至 還 準 備 了 《 陰 道 獨 白 》 裡 的 台 詞 。', '我 知 道 這 個 , 因 為 在 我 寫 這 本 書 時 , 曾 和 1 5 0 個 人 對 談 , 許 多 來 自 科 學 和 科 技 領 域 , 他 們 都 覺 得 自 己 被 分 配 到 小 孩 桌 。', '在 座 有 以 下 情 況 請 舉 手 : 有 人 2 2 歲 的 時 候 從 未 犯 錯 , 或 從 未 做 過 後 悔 的 事 嗎 ?']\n",
            "tensor([   2, 2174], device='cuda:0')\n",
            "tensor([   2, 2174, 2175, 2176, 2175, 2176, 2175, 2176, 2177,    3,    0, 2177],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2175, 2176, 2175, 2176, 2175, 2176, 2177,    3,    0, 2177,\n",
            "           3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3],\n",
            "       device='cuda:0')\n",
            "tensor([   2, 2174, 2175, 2176, 2175, 2176, 2175, 2176, 2177,    3,    0, 2177,\n",
            "           3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,\n",
            "           3,    0, 2177,    3,    0, 2177,    3,    0], device='cuda:0')\n",
            "tensor([   2, 2174, 2175, 2176, 2175, 2176, 2175, 2176, 2177,    3,    0, 2177,\n",
            "           3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,\n",
            "           3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,\n",
            "           3,    0, 2177,    3,    0, 2177], device='cuda:0')\n",
            "tensor([   2, 2174, 2175, 2176, 2175, 2176, 2175, 2176, 2177,    3,    0, 2177,\n",
            "           3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,\n",
            "           3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,\n",
            "           3,    0, 2177,    3,    0, 2177,    3,    0, 2177,    3,    0, 2177,\n",
            "           3,    0, 2177,    3], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rvalid_step:   3%|▎         | 1/39 [05:07<3:14:59, 307.87s/ step]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9dd25b4ef014>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_is_prepare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# gc.collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-e1ae591fec8f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(setting, dataset_is_prepare)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         avg_val_loss,avg_bleu_score = valid(\n\u001b[0m\u001b[1;32m    158\u001b[0m                         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-d52fb780c2cb>\u001b[0m in \u001b[0;36mvalid\u001b[0;34m(device, model, loss_calculator, batch_size_setting, valid_loader, beam_num, max_sentence_length, dictionary_length, bos_id, eos_id, pad_id, tgt_tokenizer)\u001b[0m\n\u001b[1;32m     34\u001b[0m                             dictionary_length,bos_id,pad_id)\n\u001b[1;32m     35\u001b[0m         \u001b[0mdecode_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moutputs_in_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_last_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m# outputs_in_word,outputs = decode_with_beam_search(device,is_last_batch,model,src,src_mask,beam_num,\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#       max_sentence_length,dictionary_length,bos_id,pad_id)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-5ead7ee18a3e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, is_last_batch, src, src_mask)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# decoder_n_beam : {type : tensor , shape : (Batch X beam_num) X (id+1) ,value : int}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# decoder_probability {type : tensor , shape : Batch X beam_num X 1 , value : log_softmax probability}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         new_decoder_beam_expand , new_decoder_probability = self.get_next_word(is_last_batch,memory_beam_expand,\n\u001b[0m\u001b[1;32m     66\u001b[0m         decoder_beam_expand,decoder_probability,id,batch,beam_num,padding,repeat,row)\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-5ead7ee18a3e>\u001b[0m in \u001b[0;36mget_next_word\u001b[0;34m(self, is_last_batch, memory, out, out_probability, id, batch, beam_num, padding, repeat, row)\u001b[0m\n\u001b[1;32m     98\u001b[0m       \u001b[0;31m# out_n_beam : {type : tensor , shape : (Batch X beam_num) X (id+1) ,value : int}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0;31m# out_probability {type : tensor , shape : Batch X beam_num X 1 , value : log_softmax probability}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mout_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_search_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeam_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-5ead7ee18a3e>\u001b[0m in \u001b[0;36mbeam_search_one_step\u001b[0;34m(self, batch, beam_num, repeat, row, sentences, p_sentences, n_beam_output)\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0;31m# sentences_expand : {type : tensor , shape : batch X (beam_num X beam_num) X now_sentences_length ,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[0;31m# value : [[[A,B...] X beam_num times,[C,D...] X beam_num times}...] A,B,C,D...are int}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m       \u001b[0msentences_expand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0;31m# topk_prob : {type : tensor , shape : batch X beam_num X beam_num, value : log_softmax probability}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save({\n",
        "#             'epoch': epoch,\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'loss': loss,\n",
        "#             ...\n",
        "#             }, PATH)\n",
        "# model = TheModelClass(*args, **kwargs)\n",
        "# optimizer = TheOptimizerClass(*args, **kwargs)\n",
        "\n",
        "# checkpoint = torch.load(PATH)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# epoch = checkpoint['epoch']\n",
        "# loss = checkpoint['loss']\n",
        "\n",
        "# model.eval()\n",
        "# # - or -\n",
        "# model.train()"
      ],
      "metadata": {
        "id": "NAQywvDycPXJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}