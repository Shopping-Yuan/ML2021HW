{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqZThHlVAvrFI1I54wPYFv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW5/HW05_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "# !pip install --upgrade jupyter ipywidgets\n",
        "# !git clone https://github.com/pytorch/fairseq.git\n",
        "# !cd fairseq && git checkout 9a1c497\n",
        "# !pip install --upgrade ./fairseq/"
      ],
      "metadata": {
        "id": "hwhMkDjdvARr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# import pdb\n",
        "# import pprint\n",
        "# import logging\n",
        "# import os\n",
        "# import random\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.utils import data\n",
        "# import numpy as np\n",
        "# import tqdm.auto as tqdm\n",
        "# from pathlib import Path\n",
        "# from argparse import Namespace\n",
        "# #from fairseq import utils\n",
        "\n",
        "# import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FgIV_VBavJ2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVHTBvhfZyZY"
      },
      "outputs": [],
      "source": [
        "# seed = 1\n",
        "# random.seed(seed)\n",
        "# torch.manual_seed(seed)\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.manual_seed(seed)\n",
        "#     torch.cuda.manual_seed_all(seed)\n",
        "# np.random.seed(seed)\n",
        "# torch.backends.cudnn.benchmark = False\n",
        "# torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "shutil.copyfile('/content/drive/MyDrive/ted2020.tgz','/content/ted2020.tgz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "X23RaTWe9hoU",
        "outputId": "9c1697e8-deda-4c18-ba92-aee2b9e3d66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/ted2020.tgz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "rawdata_file = \"/content/ted2020.tgz\"\n",
        "unzip_path = \"/content/train_dev/\"\n",
        "# open file\n",
        "file = tarfile.open(rawdata_file)\n",
        "\n",
        "# extracting file\n",
        "file.extractall(unzip_path)\n",
        "\n",
        "file.close()\n"
      ],
      "metadata": {
        "id": "2GhVMRgifeKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_lang = \"raw.en\"\n",
        "tgt_lang = \"raw.zh\"\n",
        "\n",
        "src_path = f\"{unzip_path}{src_lang}\"\n",
        "tgt_path = f\"{unzip_path}{tgt_lang}\"\n",
        "print(tgt_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH2bCPFbvhK2",
        "outputId": "cff23e3a-43a4-4e37-81bb-cdfded524d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/train_dev/raw.zh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for path in [src_path,tgt_path]:\n",
        "  with open(path, \"r\") as f:\n",
        "  #  data = f.readlines()\n",
        "  #  print(type(data),data[0:5])#<class 'list'> ['Thank you so much, Chris.\\n', \"And it'......on.\\n']\n",
        "    data = f.read().splitlines()\n",
        "    print(type(data),data[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjIUpASA076k",
        "outputId": "c03a586a-77bf-494e-df82-04a621ea07de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> ['Thank you so much, Chris.', \"And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\", 'I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.', 'And I say that sincerely, partly because  I need that.', 'Put yourselves in my position.']\n",
            "<class 'list'> ['非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台', '真是一大榮幸。我非常感激。', '這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。', '我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!', '請你們設身處地為我想一想！']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "print(ord(\"，\"))\n",
        "print(chr(65292))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhEDa7SN6us_",
        "outputId": "d66e175a-2345-43f4-8fdd-5fc10f2e384e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65292\n",
            "，\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "def to_halfwidth(string):\n",
        "  return \"\".join(unicodedata.normalize('NFKC',letter) for letter in string)\n",
        "print(to_halfwidth(\"ＡＳＤＦＦＦＦＱ\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFgPn-1i1EZR",
        "outputId": "90d7ff18-7d8d-46d6-bd68-9e8b95d5cead"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASDFFFFQ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def clean_s(s, lang):\n",
        "  s = to_halfwidth(s)\n",
        "  if lang == 'zh':\n",
        "    # step 1 : delete — _\n",
        "    delete = \"— _()[]\"\n",
        "    delete_rules = s.maketrans(\"\",\"\",delete)\n",
        "    s = s.translate(delete_rules)\n",
        "\n",
        "    # step 2 : replace “” with \"\"\n",
        "    to_be_replace = '“”'\n",
        "    replace = '\"\"'\n",
        "    replace_dict = dict(zip(to_be_replace,replace))\n",
        "    # print(replace_dict.items()) : dict_items([('“', '\"'), ('”', '\"')])\n",
        "\n",
        "    # step 3 : add space before and after punctuation\n",
        "\n",
        "    \"\"\"\n",
        "    Please ensure that space char \" \" has bee remove in previous step ,\n",
        "    it's the reason s must be translated twice.\n",
        "\n",
        "    \"\"\"\n",
        "    punctuation = \"。,;!?()\\\"~「」\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = \" \" + char + \" \"\n",
        "    #print(len(replace_dict.items())) : 13\n",
        "\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "  elif lang == 'en':\n",
        "\n",
        "    replace_dict = {}\n",
        "\n",
        "    delete = \"-()[]\"\n",
        "    for char in delete:\n",
        "      replace_dict[char] = \"\"\n",
        "\n",
        "    punctuation = \"([.,;!?()\\\"])\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = \" \" + char + \" \"\n",
        "\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "  return s\n",
        "\n",
        "clean_s(\"[]“” ?ＥＫｇ!“” wg “”/\",'zh')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5dwWeVz-sPI2",
        "outputId": "19da39b1-9a9e-44cc-ea13-7f5c5f7e4a0d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"\" ? EKg ! \"\"wg\"\"/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "s = \"測試,測試,測試。\"\n",
        "s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s)\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoNi7CeA3P9G",
        "outputId": "99c174d8-dc15-4c49-f4ac-1188c447c77a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "測試 , 測試 , 測試 。 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_s(s, lang):\n",
        "  s = to_halfwidth(s)\n",
        "  s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "  if lang == 'en':\n",
        "      s = s.replace('-', '') # remove '-'\n",
        "      s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
        "  elif lang == 'zh':\n",
        "\n",
        "      s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
        "  s = ' '.join(s.strip().split())\n",
        "  return s"
      ],
      "metadata": {
        "id": "-3kye-KkFFQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_s(s, lang):\n",
        "    if lang == 'en':\n",
        "        s = re.sub(r\"\\([^()]*\\)-\", \"\", s) #\n",
        "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) #\n",
        "    # elif lang == 'zh':\n",
        "    #     s = strQ2B(s) # Q2B\n",
        "    #     s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "    #     s = s.replace(' ', '')\n",
        "    #     s = s.replace('—', '')\n",
        "    #     s = s.replace('“', '\"')\n",
        "    #     s = s.replace('”', '\"')\n",
        "    #     s = s.replace('_', '')\n",
        "    #     s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
        "    s = ' '.join(s.strip().split())\n",
        "    return s\n",
        "print()"
      ],
      "metadata": {
        "id": "tylmMm9n5y1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def len_s(s, lang):\n",
        "    if lang == 'zh':\n",
        "        return len(s)\n",
        "    return len(s.split())\n",
        "\n",
        "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
        "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
        "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
        "        return\n",
        "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
        "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
        "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
        "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
        "                    for s1 in l1_in_f:\n",
        "                        s1 = s1.strip()\n",
        "                        s2 = l2_in_f.readline().strip()\n",
        "                        s1 = clean_s(s1, l1)\n",
        "                        s2 = clean_s(s2, l2)\n",
        "                        s1_len = len_s(s1, l1)\n",
        "                        s2_len = len_s(s2, l2)\n",
        "                        if min_len > 0: # remove short sentence\n",
        "                            if s1_len < min_len or s2_len < min_len:\n",
        "                                continue\n",
        "                        if max_len > 0: # remove long sentence\n",
        "                            if s1_len > max_len or s2_len > max_len:\n",
        "                                continue\n",
        "                        if ratio > 0: # remove by ratio of length\n",
        "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
        "                                continue\n",
        "                        print(s1, file=l1_out_f)\n",
        "                        print(s2, file=l2_out_f)\n",
        "\n",
        "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
        "    print(f'train/valid splits exists. skipping split.')\n",
        "else:\n",
        "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
        "    labels = list(range(line_num))\n",
        "    random.shuffle(labels)\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
        "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
        "        count = 0\n",
        "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
        "            if labels[count]/line_num < train_ratio:\n",
        "                train_f.write(line)\n",
        "            else:\n",
        "                valid_f.write(line)\n",
        "            count += 1\n",
        "        train_f.close()\n",
        "        valid_f.close()"
      ],
      "metadata": {
        "id": "wS-Tl_5JhUVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "if (prefix/f'spm{vocab_size}.model').exists():\n",
        "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "else:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
        "                        f'{prefix}/valid.clean.{src_lang}',\n",
        "                        f'{prefix}/train.clean.{tgt_lang}',\n",
        "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
        "        model_prefix=prefix/f'spm{vocab_size}',\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1,\n",
        "        model_type='unigram', # 'bpe' 也可\n",
        "        input_sentence_size=1e6,\n",
        "        shuffle_input_sentence=True,\n",
        "        normalization_rule_name='nmt_nfkc_cf',\n",
        "    )\n",
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "in_tag = {\n",
        "    'train': 'train.clean',\n",
        "    'valid': 'valid.clean',\n",
        "    'test': 'test.raw.clean',\n",
        "}\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        out_path = prefix/f'{split}.{lang}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)"
      ],
      "metadata": {
        "id": "oO1-anUThuCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
        "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5\n",
        "binpath = Path('./DATA/data-bin', dataset_name)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess \\\n",
        "        --source-lang {src_lang}\\\n",
        "        --target-lang {tgt_lang}\\\n",
        "        --trainpref {prefix/'train'}\\\n",
        "        --validpref {prefix/'valid'}\\\n",
        "        --testpref {prefix/'test'}\\\n",
        "        --destdir {binpath}\\\n",
        "        --joined-dictionary\\\n",
        "        --workers 2\n"
      ],
      "metadata": {
        "id": "xIWAdyf6h0bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "!cd fairseq && git checkout 9a1c497\n",
        "!pip install --upgrade ./fairseq/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh_F67loqcro",
        "outputId": "9c6923cc-51e4-41d2-a957-1f1eac69cea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 34933, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 34933 (delta 56), reused 53 (delta 36), pack-reused 34850\u001b[K\n",
            "Receiving objects: 100% (34933/34933), 25.08 MiB | 18.76 MiB/s, done.\n",
            "Resolving deltas: 100% (25346/25346), done.\n",
            "Note: switching to '9a1c497'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 9a1c4970 Make Hydra logging work with DDP (#1568)\n",
            "Processing ./fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+9a1c497) (3.0.5)\n",
            "Collecting hydra-core<1.1 (from fairseq==1.0.0a0+9a1c497)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==1.0.0a0+9a1c497)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+9a1c497) (2023.6.3)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq==1.0.0a0+9a1c497)\n",
            "  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+9a1c497) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+9a1c497) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.23.5)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1->fairseq==1.0.0a0+9a1c497)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+9a1c497) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+9a1c497) (4.5.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq==1.0.0a0+9a1c497)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+9a1c497) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq==1.0.0a0+9a1c497)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+9a1c497) (4.9.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==1.0.0a0+9a1c497) (2.21)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+9a1c497) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+9a1c497) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+9a1c497) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+9a1c497) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+9a1c497) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+9a1c497) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq==1.0.0a0+9a1c497) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq==1.0.0a0+9a1c497) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+9a1c497-cp310-cp310-linux_x86_64.whl size=2215721 sha256=9ff29a19463283ec0d5c3178277d7f4855720330debffce27367c83d0201a072\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s2vqqkzg/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=dae38d1c14e9d8091c6eda33a3484ac6f856498187affa4725fa68650a73c8af\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 colorama-0.4.6 fairseq-1.0.0a0+9a1c497 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fairseq.models import (\n",
        "    FairseqEncoder,\n",
        "    FairseqIncrementalDecoder,\n",
        "    FairseqEncoderDecoderModel\n",
        ")"
      ],
      "metadata": {
        "id": "15D1GAe2qyEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "## setup task\n",
        "# task_cfg = TranslationConfig(\n",
        "#     data=config.datadir,\n",
        "#     source_lang=config.source_lang,\n",
        "#     target_lang=config.target_lang,\n",
        "#     train_subset=\"train\",\n",
        "#     required_seq_len_multiple=8,\n",
        "#     dataset_impl=\"mmap\",\n",
        "#     upsample_primary=1,\n",
        "# )\n",
        "# task = TranslationTask.setup_task(task_cfg)"
      ],
      "metadata": {
        "id": "t2yyy9E9k4kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ],
      "metadata": {
        "id": "QG-tr09Ok9E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "    \"Source: \" + \\\n",
        "    task.source_dictionary.string(\n",
        "        sample['source'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")\n",
        "pprint.pprint(\n",
        "    \"Target: \" + \\\n",
        "    task.target_dictionary.string(\n",
        "        sample['target'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "hGkiMjLAk_4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "    batch_iterator = task.get_batch_iterator(\n",
        "        dataset=task.dataset(split),\n",
        "        max_tokens=max_tokens,\n",
        "        max_sentences=None,\n",
        "        max_positions=utils.resolve_max_positions(\n",
        "            task.max_positions(),\n",
        "            max_tokens,\n",
        "        ),\n",
        "        ignore_invalid_inputs=True,\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        epoch=epoch,\n",
        "        disable_iterator_cache=not cached,\n",
        "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond\n",
        "        # first call of this method has no effect.\n",
        "    )\n",
        "    return batch_iterator\n",
        "\n",
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "sample = next(demo_iter)\n",
        "sample"
      ],
      "metadata": {
        "id": "dbfi-DvrlDhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = {\n",
        "  \"id\": id, # 每個 example 的 id\n",
        "  \"nsentences\": len(samples), # batch size 句子數\n",
        "  \"ntokens\": ntokens, # batch size 字數\n",
        "  \"net_input\": {\n",
        "      \"src_tokens\": src_tokens, # 來源語言的序列\n",
        "      \"src_lengths\": src_lengths, # 每句話沒有 pad 過的長度\n",
        "      \"prev_output_tokens\": prev_output_tokens, # 上面提到右 shift 一格後的目標序列\n",
        "  },\n",
        "  \"target\": target, # 目標序列\n",
        "}"
      ],
      "metadata": {
        "id": "0VgNDQMVlGLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_env = utils.CudaEnvironment()\n",
        "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "cYgg7zwHk1jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # HINT: transformer 架構\n",
        "# from fairseq.models.transformer import (\n",
        "#     TransformerEncoder,\n",
        "#     TransformerDecoder,\n",
        "# )\n",
        "\n",
        "def build_model(args, task):\n",
        "    \"\"\" 按照參數設定建置模型 \"\"\"\n",
        "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "    # 詞嵌入\n",
        "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "\n",
        "    # 編碼器與解碼器\n",
        "    # TODO: 替換成 TransformerEncoder 和 TransformerDecoder\n",
        "    encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "    # 序列到序列模型\n",
        "    model = Seq2Seq(args, encoder, decoder)\n",
        "\n",
        "    # 序列到序列模型的初始化很重要 需要特別處理\n",
        "    def init_params(module):\n",
        "        from fairseq.modules import MultiheadAttention\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        if isinstance(module, MultiheadAttention):\n",
        "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.RNNBase):\n",
        "            for name, param in module.named_parameters():\n",
        "                if \"weight\" in name or \"bias\" in name:\n",
        "                    param.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    # 初始化模型\n",
        "    model.apply(init_params)\n",
        "    return model"
      ],
      "metadata": {
        "id": "-Zy7vAVXlTAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "    def __init__(self, args, encoder, decoder):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.args = args\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src_tokens,\n",
        "        src_lengths,\n",
        "        prev_output_tokens,\n",
        "        return_all_hiddens: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run the forward pass for an encoder-decoder model.\n",
        "        \"\"\"\n",
        "        encoder_out = self.encoder(\n",
        "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "        )\n",
        "        logits, extra = self.decoder(\n",
        "            prev_output_tokens,\n",
        "            encoder_out=encoder_out,\n",
        "            src_lengths=src_lengths,\n",
        "            return_all_hiddens=return_all_hiddens,\n",
        "        )\n",
        "        return logits, extra"
      ],
      "metadata": {
        "id": "hBNhGpgS08gI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}