{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW5/HW05_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "###Part 0 setting and installing package\n",
        "------\n",
        "###Part 1 preparing data set\n",
        "------\n",
        "######load data file\n",
        "######clean data\n",
        "######pick up line pairs\n",
        "######tokenize : using sentencepiece\n",
        "######make data set\n",
        "------\n",
        "###Part 2 make model\n",
        "------\n",
        "######positional encoding layer\n",
        "######multihead attention layer\n",
        "######encoder layer(s)\n",
        "######decoder layer(s)\n",
        "######transformer layer\n",
        "------\n",
        "###Part 3 training and validation process\n",
        "------\n",
        "######Noam optimizer\n",
        "######label smoothing\n",
        "######beam search\n",
        "######bleu\n",
        "######training and validation function\n",
        "######main function\n",
        "######get plot\n",
        "------\n",
        "###Part 4 inference\n",
        "------\n",
        "######infer dataset\n",
        "######infer function\n",
        "######inference main function\n"
      ],
      "metadata": {
        "id": "WTv4XN2qB_fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "setting\n",
        "======\n",
        ">Here are all parameters using in this project."
      ],
      "metadata": {
        "id": "iVQ2D_mLcx4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "setting = {\n",
        "# information of the path of dataset\n",
        "\"data_info\" : {\n",
        "    \"drive_path\":\"/content/drive\",\n",
        "    \"document\":\"/content/drive/MyDrive\",\n",
        "    \"raw_file_name\":\"/ted2020.tgz\",\n",
        "    \"unzip_path\":\"/train_dev/\",\n",
        "    \"source\":{\n",
        "        \"lang\":\"en\",\n",
        "        \"raw_data_path\":\"/train_dev/raw.en\",\n",
        "        \"clean_data_path\":\"/train_dev/clean_en.txt\",\n",
        "        \"tokenized_train_data\":\"/train_dev/tokenized_train_data_en.txt\",\n",
        "        \"tokenized_val_data\":\"/train_dev/tokenized_val_data_en.txt\"\n",
        "        },\n",
        "    \"target\":{\n",
        "        \"lang\":\"zh\",\n",
        "        \"raw_data_path\":\"/train_dev/raw.zh\",\n",
        "        \"clean_data_path\":\"/train_dev/clean_zh.txt\",\n",
        "        \"tokenized_train_data\":\"/train_dev/tokenized_train_data_zh.txt\",\n",
        "        \"tokenized_val_data\":\"/train_dev/tokenized_val_data_zh.txt\"\n",
        "        }\n",
        "},\n",
        "# tokenized setting for spm\n",
        "\"tokenized_setting\" : {\n",
        "    \"vocab_size\" : 8000,\n",
        "    \"character_coverage\" : 1,\n",
        "    \"model_type\" : \"bpe\", # \"unigram\",\n",
        "    \"input_sentence_size\" : 400000,\n",
        "    \"shuffle_input_sentence\" : True,\n",
        "    \"normalization_rule_name\" : \"nmt_nfkc_cf\",\n",
        "    \"pad_id\":0,\n",
        "    \"unk_id\":1,\n",
        "    \"bos_id\":2,\n",
        "    \"eos_id\":3,\n",
        "    \"max_l\":400\n",
        "},\n",
        "# model structure setting\n",
        "\"model\" : {\n",
        "      \"encoder_embedding_dimension\" : 256,\n",
        "      \"decoder_embedding_dimension\" : 256,\n",
        "      \"feedforward_dimension\" : 2048,\n",
        "      \"num_heads\" : 2,\n",
        "      \"dropout_p\" : 0.0,\n",
        "      \"layer_num\" : 6\n",
        "},\n",
        "\n",
        "# setting in training and validation process ,\n",
        "# including optimization setting.\n",
        "\"training_hparas\" : {\n",
        "    \"total_step\" : 60000,\n",
        "    \"do_valid_step\" : 60000,\n",
        "    \"early_stop_step\" : 2,\n",
        "    \"temp_save_step\" : 1000,\n",
        "    \"train_batch_size\" : 40,\n",
        "    \"valid_batch_size\" : 100,\n",
        "    \"workers\" : 2,\n",
        "    \"label_smoothing\" : 0.1,\n",
        "    \"beam_num\" : 2,\n",
        "    \"optimization\":{\n",
        "        \"factor\" : 2,\n",
        "        \"warmup\"  : 4000,\n",
        "        \"optimizer\" : {\n",
        "                \"lr\" : 0,\n",
        "                \"betas\" : (0.9, 0.98),\n",
        "                \"eps\" : 1e-9,\n",
        "                \"weight_decay\" : 0.0001\n",
        "                }\n",
        "            },\n",
        "    \"model_saving_path\" : \"/content/drive/MyDrive/model.pth\",\n",
        "    \"model_temporary_saving_path\" : \"/content/drive/MyDrive/temporary_model.pth\"\n",
        "},\n",
        "\"inference_out_path\" : \"/infer_out.json\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "-WoR-01STMor"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "installing package\n",
        "------"
      ],
      "metadata": {
        "id": "cdURO12Sntrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# used in part 1\n",
        "!pip install sentencepiece\n",
        "# used in part 1 and 3\n",
        "!pip install tqdm\n",
        "# used in part 2\n",
        "!pip install torchinfo\n",
        "# used in part 3\n",
        "!pip install torcheval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ti-UGuHNhh",
        "outputId": "70f3a8e1-ca3f-4f0e-9eca-6c26bfc7efc7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing data set\n",
        "=============\n",
        "\n",
        "load data file\n",
        "-------------\n",
        ">Here I load dataset from my drive,  \n",
        ">but it also can be download from the link below."
      ],
      "metadata": {
        "id": "mMkT3K4JXs60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 : download dataset from drive to google colab\n",
        "# original dataset is in \"https://mega.nz/#!vEcTCISJ!3Rw0eHTZWPpdHBTbQEqBDikDEdFPr7fI8WxaXK9yZ9U\"\n",
        "\n",
        "path_doc = setting[\"data_info\"][\"document\"]\n",
        "rawdata_file_name = setting[\"data_info\"][\"raw_file_name\"]\n",
        "rawdata_file_path = path_doc + rawdata_file_name\n",
        "unzip_path = path_doc + setting[\"data_info\"][\"unzip_path\"]\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive_path = setting[\"data_info\"][\"drive_path\"]\n",
        "drive.mount(drive_path)\n",
        "\n",
        "# copy file from drive\n",
        "# import shutil\n",
        "# shutil.copyfile(path_doc + rawdata_file_name, rawdata_file_path)\n",
        "\n",
        "# step 2 : unzip dataset\n",
        "import tarfile\n",
        "# open file\n",
        "file = tarfile.open(rawdata_file_path)\n",
        "# extracting file\n",
        "file.extractall(unzip_path)\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X23RaTWe9hoU",
        "outputId": "bc3d0ed0-cbd3-4c17-c834-b7e80e32f37a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean data\n",
        "------\n",
        ">First each dataset (source or target) is clean  \n",
        ">seperately, change to halfwidth and remove/replace  \n",
        ">some kind of punctuations.\n",
        "\n",
        ">Also because the number of sentences in one line may be  \n",
        ">different in line pairs of source and target set (its an error),  \n",
        ">some special punctuations is add to the end of sentences  \n",
        ">for the next process dealing with these problem by  \n",
        ">using sentence pairs instead of lines pairs to form datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "-y54N2UNimor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "# convert fullwidth to halfwidth\n",
        "def to_halfwidth(string):\n",
        "  return \"\".join(unicodedata.normalize('NFKC',letter) for letter in string)\n",
        "def clean_s_zh(s):\n",
        "    s = to_halfwidth(s)\n",
        "    # step 1 : delete — _\n",
        "    delete = \" _()[]\"\n",
        "    delete_rules = s.maketrans(\"\",\"\",delete)\n",
        "    s = s.translate(delete_rules)\n",
        "\n",
        "    # step 2 : replace “” with \"\"\n",
        "    to_be_replace = '“”'\n",
        "    replace = '\"\"'\n",
        "    replace_dict = dict(zip(to_be_replace,replace))\n",
        "\n",
        "    # step 3 : add **END** before and after punctuation\n",
        "\n",
        "    \"\"\"\n",
        "    The number of sentences in one line may be different\n",
        "    in line pairs of source and target set.\n",
        "    \"**END**\" is add after \"。!?\" and \".!?\", which can be used\n",
        "    to check if the number of sentence in the pair are equal\n",
        "    in the next process.\n",
        "    also in english, \".\" may be use in abbreviation,\n",
        "    these different use must be identified.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    punctuation = \"。!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    zh_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return zh_list\n",
        "\n",
        "def clean_s_en(s):\n",
        "    s = to_halfwidth(s)\n",
        "\n",
        "    replace_dict = {}\n",
        "\n",
        "    delete = \"-()[]\"\n",
        "    for char in delete:\n",
        "      replace_dict[char] = \"\"\n",
        "\n",
        "    punctuation = \"!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    # Identify if \".\" is used in abbreviation,\n",
        "    # if not, add \"**END**\" after it.\n",
        "    pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "    s = pattern.sub(\"**END**\",s)\n",
        "\n",
        "    # test pattern\n",
        "    # pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "    # result = pattern.sub(\"**END**\",\"There are many people in U.S. w.r.t. in Taiwan.Thank you.\")\n",
        "\n",
        "    en_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return en_list"
      ],
      "metadata": {
        "id": "5dwWeVz-sPI2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pick up line pairs\n",
        "------\n",
        ">pick up line pairs has equal number of sentences and  \n",
        ">split them into sentences to form sourse/target dataset.  \n",
        ">Remove sentences with too many words for training and validation."
      ],
      "metadata": {
        "id": "M2EfnpIXV91l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using \"**END**\" to split line pairs to check if they have equal sentence\n",
        "def divide_by_END(s):\n",
        "    list_s = []\n",
        "    for line_string in s.strip(\"**END**\").split(\"**END**\"):\n",
        "      if line_string not in [\"\",\" \"]:\n",
        "         list_s.append(line_string)\n",
        "    return(list_s)\n",
        "'''\n",
        "warning : devide_en_again function is apply just beacause\n",
        "in \"this\" dataset english sentences end with \":\" or \";\"\n",
        "sometimes not splited well.\n",
        "If the dataset is change, this part may need to be\n",
        "eliminated or modified.\n",
        "'''\n",
        "def devide_en_again(s,punctuation = \":;\"):\n",
        "    replace_dict = {}\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules_src = s.maketrans(replace_dict)\n",
        "    new_s = divide_by_END(s.translate(replace_rules_src))\n",
        "    return new_s\n",
        "\n",
        "# remove \"sentence\" if it is too long.\n",
        "def remove_too_long(src_list,tgt_list,threshold):\n",
        "    too_long_src = 0\n",
        "    too_long_tgt = 0\n",
        "    remove = False\n",
        "    new_s = []\n",
        "    new_t = []\n",
        "    for i in range(len(src_list)):\n",
        "      if ((len(src_list[i])>threshold)):\n",
        "        remove = True\n",
        "        too_long_src += 1\n",
        "      if (len(tgt_list[i])>threshold):\n",
        "        remove = True\n",
        "        too_long_tgt += 1\n",
        "      if remove == False:\n",
        "        new_s.append(src_list[i])\n",
        "        new_t.append(tgt_list[i])\n",
        "      else :\n",
        "        remove = False\n",
        "    return(new_s,new_t,too_long_src,too_long_tgt)\n",
        "\n",
        "# pick up good line pairs for traning and validation model\n",
        "def check_data_pairs(src_list,tgt_list,threshold):\n",
        "    index = 0\n",
        "    new_src_list = []\n",
        "    new_tgt_list = []\n",
        "\n",
        "    same = 0\n",
        "    add_next = 0\n",
        "    split_again = 0\n",
        "    not_use = 0\n",
        "\n",
        "    while(index < len(src_list)):\n",
        "\n",
        "      src = divide_by_END(src_list[index])\n",
        "      tgt = divide_by_END(tgt_list[index])\n",
        "\n",
        "      # case 1 : src is as long as tgt , finished.\n",
        "      if len(src) == len(tgt):\n",
        "        new_src_list += src\n",
        "        new_tgt_list += tgt\n",
        "        same += 1\n",
        "        index += 1\n",
        "\n",
        "      else :\n",
        "        # if it is not the last one : both src and tgt add next sentence\n",
        "        if index != len(src_list)-1:\n",
        "          src_add_next = divide_by_END(src_list[index] + src_list[index+1])\n",
        "          tgt_add_next = divide_by_END(tgt_list[index] + tgt_list[index+1])\n",
        "          # case 2 : src_add_next is as long as tgt_add_next , finished.\n",
        "          if len(src_add_next) == len(tgt_add_next):\n",
        "            new_src_list += src_add_next\n",
        "            new_tgt_list += tgt_add_next\n",
        "            add_next += 2\n",
        "            index += 2\n",
        "\n",
        "          # using new punctuation to divide tgt (english) sentence.\n",
        "          # note that this part could cause negative effects if the dataset is change.\n",
        "          else :\n",
        "            src_add_next = devide_en_again(src_list[index] + src_list[index+1])\n",
        "            # case 3 : src_add_next is as long as tgt_add_next , finished.\n",
        "            if len(src_add_next) == len(tgt_add_next):\n",
        "              new_src_list += src_add_next\n",
        "              new_tgt_list += tgt_add_next\n",
        "              split_again +=2\n",
        "              index += 2\n",
        "\n",
        "            # case 4 : sentence will not be used.\n",
        "            else :\n",
        "              not_use += 1\n",
        "              # if to_do == 1 :\n",
        "              #   print(index,src_add_next,tgt_add_next,len(src_add_next),len(tgt_add_next))\n",
        "              index += 1\n",
        "\n",
        "        # if it is the last one\n",
        "        else :\n",
        "          not_use += 1\n",
        "          index += 1\n",
        "    # print information\n",
        "    print(f\"The original total number of line is {index}.\")\n",
        "    print(f\"The number of line pairs have the equal sentences is {same}.\")\n",
        "    print(f\"The number of line pairs have the equal sentences after combine the next lines is {add_next}.\")\n",
        "    print(f\"The number of line pairs have the equal sentences after combine the next lines\"+\\\n",
        "       f\"and resplit english lines using :; is {split_again}.\")\n",
        "    print(f\"The number of line we don't use is {not_use}.\")\n",
        "    print(f\"Note that {index} = {same}+{add_next}+{split_again}+{not_use}.\")\n",
        "\n",
        "    # remove long lines\n",
        "    print(f\"The total number of sentence pairs before remove long sentences is {len(new_src_list)}.\")\n",
        "    new_src_list,new_tgt_list,too_long_src,too_long_tgt = remove_too_long(new_src_list,new_tgt_list,threshold)\n",
        "    print(f\"The finally total number of sentence pairs using is {len(new_src_list)}.\")\n",
        "    print(f\"Note that {len(new_src_list)} are the number of sentence pairs, not line pairs\")\n",
        "\n",
        "    return(new_src_list,new_tgt_list)"
      ],
      "metadata": {
        "id": "UVYvYCpuDQDa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load and clean data\n",
        "------"
      ],
      "metadata": {
        "id": "1KPMwJwck437"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load and clean data\n",
        "def load_file(path,function):\n",
        "    with open(path, \"r\") as f:\n",
        "      data = f.read()\n",
        "      return function(data)\n",
        "# saving to new path\n",
        "def clean_data_and_save(\n",
        "    path_doc,raw_src_path,raw_tgt_path,\n",
        "    clean_src_path,clean_tgt_path,threshold\n",
        "    ):\n",
        "    raw_src_path = path_doc + raw_src_path\n",
        "    raw_tgt_path = path_doc + raw_tgt_path\n",
        "    src = load_file(raw_src_path,clean_s_en),\n",
        "    tgt = load_file(raw_tgt_path,clean_s_zh),\n",
        "    # src , tgt are tuples with only one term : src_list, tgt_list\n",
        "    src_list = src[0]\n",
        "    tgt_list = tgt[0]\n",
        "    clean_src_list, clean_tgt_list = check_data_pairs(src_list,tgt_list,threshold)\n",
        "    with open(path_doc + clean_src_path, \"w\") as f:\n",
        "      f.write(\"\\n\".join(clean_src_list))\n",
        "    with open(path_doc + clean_tgt_path, \"w\") as f:\n",
        "      f.write(\"\\n\".join(clean_tgt_list))\n",
        "# test clean_data_and_save\n",
        "# clean_data_and_save(\n",
        "#     path_doc = setting[\"data_info\"][\"document\"],\n",
        "#     raw_src_path = setting[\"data_info\"][\"source\"][\"raw_data_path\"],\n",
        "#     raw_tgt_path = setting[\"data_info\"][\"target\"][\"raw_data_path\"],\n",
        "#     clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "#     clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"],\n",
        "#     threshold = setting[\"tokenized_setting\"][\"max_l\"]\n",
        "# )"
      ],
      "metadata": {
        "id": "DhVI2xylk1tr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenize\n",
        "------\n",
        ">using sentencepiece to tokenize sentences,  \n",
        ">first make the english/chinese dictionary separately,  \n",
        ">then use these dict to encode sentence pair in dataset,  \n",
        ">including add bos/eos/padding to tokenized sentences.  \n",
        ">Finally split then into train/val set and save."
      ],
      "metadata": {
        "id": "S_-APGe5okQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.utils.data as data\n",
        "def tokenized(path_doc,\n",
        "       clean_data_path,\n",
        "       vocab_size,\n",
        "       lang,\n",
        "       tokenized_setting\n",
        "       ):\n",
        "  model_prefix = f\"spm_{vocab_size}_{lang}\"\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      input= path_doc+clean_data_path,\n",
        "      **tokenized_setting,\n",
        "      model_prefix = path_doc +\"/\"+ model_prefix,\n",
        "  )\n",
        "  return(model_prefix)\n",
        "\n",
        "def get_tokenizers(path_doc,vocab_size,src_lang,tgt_lang):\n",
        "  src_tokenizer = spm.SentencePieceProcessor(model_file = path_doc + f\"/spm_{vocab_size}_{src_lang}\" +\".model\")\n",
        "  tgt_tokenizer = spm.SentencePieceProcessor(model_file = path_doc + f\"/spm_{vocab_size}_{tgt_lang}\" +\".model\")\n",
        "  return src_tokenizer,tgt_tokenizer\n",
        "\n",
        "def bos_eos_padding(dataset,\n",
        "          max_l,\n",
        "          src_tokenizer,\n",
        "          tgt_tokenizer,\n",
        "          bos_id,\n",
        "          eos_id,\n",
        "          pad_id,\n",
        "          ):\n",
        "\n",
        "\n",
        "  padding_src = []\n",
        "  padding_tgt = []\n",
        "  len_s = 0\n",
        "  len_t = 0\n",
        "  for src,tgt in dataset:\n",
        "    s = src_tokenizer.encode(src, out_type=int)\n",
        "    s = np.append(s,[eos_id])\n",
        "    s = np.append([bos_id],np.pad(s,(0, max_l-len(s)-1), constant_values = pad_id))\n",
        "    padding_src.append(s)\n",
        "\n",
        "    t = tgt_tokenizer.encode(tgt, out_type=int)\n",
        "    t = np.append(t,[eos_id])\n",
        "    t = np.append([bos_id],np.pad(t,(0, max_l-len(t)-1), constant_values = pad_id))\n",
        "    padding_tgt.append(t)\n",
        "\n",
        "  return(list(zip(padding_src,padding_tgt)))\n",
        "# test SentencePieceProcessor and bos_eos_padding\n",
        "# s_src = spm.SentencePieceProcessor(model_file=\"/content/spm8000_en.model\")\n",
        "# s_src.encode(\"hello world!\", out_type=int)\n",
        "# bos_eos_padding([(\"hello world\",\"_哈囉\")],5,10)\n",
        "\n",
        "def data_set_preparing(path_doc,\n",
        "            clean_src_path,\n",
        "            clean_tgt_path,\n",
        "            max_l,\n",
        "            src_tokenizer,\n",
        "            tgt_tokenizer,\n",
        "            st_train_path,\n",
        "            st_val_path,\n",
        "            tt_train_path,\n",
        "            tt_val_path,\n",
        "            bos_id,\n",
        "            eos_id,\n",
        "            pad_id,\n",
        "            ):\n",
        "    src_set = []\n",
        "    tgt_set = []\n",
        "\n",
        "    with open(path_doc+clean_src_path,\"r\") as in_f :\n",
        "      for line in tqdm(in_f):\n",
        "        src_set.append(line)\n",
        "    with open(path_doc+clean_tgt_path,\"r\") as in_f :\n",
        "      for line in tqdm(in_f):\n",
        "        tgt_set.append(line)\n",
        "\n",
        "    dataset = list(zip(src_set,tgt_set))\n",
        "    dataset = bos_eos_padding(dataset,max_l,src_tokenizer,tgt_tokenizer)\n",
        "    train_set, valid_set = data.random_split(dataset,[0.99,0.01])\n",
        "    # print(train_set[0][0])\n",
        "\n",
        "    with open(path_doc + st_train_path, 'w') as out_f:\n",
        "      for line_pair in tqdm(train_set):\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[0])+\"\\n\")\n",
        "    with open(path_doc + st_val_path, 'w') as out_f:\n",
        "      for line_pair in tqdm(valid_set):\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[0])+\"\\n\")\n",
        "    with open(path_doc + tt_train_path, 'w') as out_f:\n",
        "      for line_pair in tqdm(train_set):\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[1])+\"\\n\")\n",
        "    with open(path_doc + tt_val_path, 'w') as out_f:\n",
        "      for line_pair in tqdm(valid_set):\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[1])+\"\\n\")"
      ],
      "metadata": {
        "id": "ByrUmAvFkKk9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenized_data(vocab_size,tokenized_setting,max_l,path_doc,clean_src_path,\n",
        "          clean_tgt_path,src_lang,tgt_lang,st_train_path,st_val_path,\n",
        "          tt_train_path,tt_val_path,bos_id,eos_id,pad_id):\n",
        "  tokenized(path_doc,clean_src_path,vocab_size,src_lang,tokenized_setting)\n",
        "  tokenized(path_doc,clean_tgt_path,vocab_size,tgt_lang,tokenized_setting)\n",
        "  src_tokenizer,tgt_tokenizer = get_tokenizers(path_doc,vocab_size,src_lang,tgt_lang)\n",
        "  data_set_preparing(path_doc,clean_src_path,clean_tgt_path,max_l,src_tokenizer,\n",
        "           tgt_tokenizer,st_train_path,st_val_path,tt_train_path,tt_val_path,\n",
        "           bos_id,eos_id,pad_id)\n",
        "  return src_tokenizer,tgt_tokenizer\n",
        "\n",
        "# test tokenized_data()\n",
        "# src_tokenizer,tgt_tokenizer = tokenized_data(\n",
        "#     vocab_size = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "#     tokenized_setting = {k:setting[\"tokenized_setting\"][k] for k in \\\n",
        "#               set(list(setting[\"tokenized_setting\"].keys()))-{\"vocab_size\",\"max_l\"}},\n",
        "#     max_l = setting[\"tokenized_setting\"][\"max_l\"],\n",
        "#     path_doc = setting[\"data_info\"][\"document\"],\n",
        "#     clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "#     clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"],\n",
        "#     src_lang = setting[\"data_info\"][\"source\"][\"lang\"],\n",
        "#     tgt_lang = setting[\"data_info\"][\"target\"][\"lang\"],\n",
        "#     st_train_path = setting[\"data_info\"][\"source\"][\"tokenized_train_data\"],\n",
        "#     st_val_path = setting[\"data_info\"][\"source\"][\"tokenized_val_data\"],\n",
        "#     tt_train_path = setting[\"data_info\"][\"target\"][\"tokenized_train_data\"],\n",
        "#     tt_val_path = setting[\"data_info\"][\"target\"][\"tokenized_val_data\"])"
      ],
      "metadata": {
        "id": "RkIHpc6qkRqh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make data set\n",
        "------\n",
        "> Using tokenized data to make dataset.  \n",
        "> Classmethod : padding_mask_batch which  \n",
        "> where the key padding mask is constucted  \n",
        "> also defined here."
      ],
      "metadata": {
        "id": "FV3kHGqlggdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class myDataset(Dataset):\n",
        "  def __init__(self,src_path,tgt_path):\n",
        "\n",
        "    self.src_path = src_path\n",
        "    self.tgt_path = tgt_path\n",
        "\n",
        "    src_list = []\n",
        "    with open(self.src_path,\"r\") as f :\n",
        "      d_l = f.readlines()\n",
        "      for line in tqdm(d_l):\n",
        "        int_list = [int(i) for i in line.split()]\n",
        "        src_list.append(int_list)\n",
        "    self.src = torch.LongTensor(src_list)\n",
        "\n",
        "    tgt_list = []\n",
        "    with open(self.tgt_path,\"r\") as f :\n",
        "      l_l = f.readlines()\n",
        "      for line in tqdm(l_l):\n",
        "        int_list = [int(i) for i in line.split()]\n",
        "        tgt_list.append(int_list)\n",
        "    self.tgt = torch.LongTensor(tgt_list)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.src)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.src[index], self.tgt[index]\n",
        "\n",
        "  # make key padding mask\n",
        "  @classmethod\n",
        "  def padding_mask_batch(cls,batch,pad_id):\n",
        "    \"\"\"Collate a batch of data.\"\"\"\n",
        "    src, tgt = zip(*batch)\n",
        "    src = torch.stack(src)\n",
        "    tgt = torch.stack(tgt)\n",
        "    src_padding = (src == pad_id)\n",
        "    tgt_padding = (tgt == pad_id)\n",
        "\n",
        "    return src, tgt , src_padding, tgt_padding\n",
        "# test myDataset\n",
        "# data = []\n",
        "# with open(\"/content/train_dev/tokenized_train_data_en.txt\",\"r\") as f :\n",
        "#   d_l = f.readlines()\n",
        "#   for line in tqdm(d_l):\n",
        "#     int_list = [int(i) for i in line.split()]\n",
        "#     data.append(int_list)\n",
        "# print(data[0])"
      ],
      "metadata": {
        "id": "dbfi-DvrlDhI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "def get_data_set(train_batch_size,valid_batch_size,num_workers,path_doc,\n",
        "         st_train_path,st_val_path,tt_train_path,tt_val_path,pad_id):\n",
        "\n",
        "  train_set = myDataset(src_path = path_doc + st_train_path,\n",
        "              tgt_path = path_doc + tt_train_path,\n",
        "              )\n",
        "  valid_set = myDataset(src_path = path_doc + st_val_path,\n",
        "              tgt_path = path_doc + tt_val_path,\n",
        "              )\n",
        "  train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size = train_batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    collate_fn = lambda x : myDataset.padding_mask_batch(x,\n",
        "                   pad_id = pad_id)\n",
        "  )\n",
        "  valid_loader = DataLoader(\n",
        "    valid_set,\n",
        "    batch_size = valid_batch_size,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    collate_fn = lambda x : myDataset.padding_mask_batch(x,\n",
        "                   pad_id = pad_id)\n",
        "  )\n",
        "  del train_set,valid_set\n",
        "  gc.collect()\n",
        "  return train_loader,valid_loader\n",
        "# test get_data_set()\n",
        "# train_set,valid_set = get_data_set()\n",
        "# batch = next(iter(valid_set))\n",
        "# src,tgt,src_mask,tgt_mask = batch\n",
        "# print(src.shape)"
      ],
      "metadata": {
        "id": "YBFKMSM45ppM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make model\n",
        "======\n",
        "positional encoding layer\n",
        "------\n",
        ">The first layer is embedding layer, where each integers  \n",
        ">in encoder sentence will be represent by a vector.   \n",
        ">I use build-in class in pytorch to finish these part,    \n",
        ">and combine it with encoder layers to form my encoder.\n",
        "\n",
        ">The layer below is the second layer :positional encoding layer  \n",
        ">in this layer the position infomation is add to each \"word\"  \n",
        ">in the sentence.\n",
        ">Here I use parameters instead of constant as  \n",
        ">position infomation so they will change during training process."
      ],
      "metadata": {
        "id": "RZAzqY3bwds8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Positional_Encoding(nn.Module):\n",
        "    def __init__(self,max_sentence_length,embedding_dimension):\n",
        "      super().__init__()\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.encoding_values = nn.Parameter(nn.init.normal_(torch.empty(max_sentence_length,1, embedding_dimension)))\n",
        "    def forward(self, x):\n",
        "        # the shape of x : [batch,length,e_dim]\n",
        "        # the shape of self.encoding_values : [batch,length,e_dim]\n",
        "        x = x + self.encoding_values.unsqueeze(0)\n",
        "        x = x.squeeze(-2)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "EoKH9m1LznWO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "multihead attention layer\n",
        "------\n"
      ],
      "metadata": {
        "id": "OURB2Fg-4lE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import math\n",
        "from torchinfo import summary\n",
        "# This part is modify from pytorch : torch.nn.functional.scaled_dot_product_attention\n",
        "# Efficient implementation equivalent to the following:\n",
        "class Scaled_Dot_Product_Attention(nn.Module):\n",
        "    def __init__(self,max_sentence_length,dropout_p):\n",
        "      super().__init__()\n",
        "      self.dropout_p = dropout_p\n",
        "      self.max_l = max_sentence_length\n",
        "      attn_bias = torch.zeros(self.max_l, self.max_l)\n",
        "      temp_mask = torch.ones(self.max_l, self.max_l, dtype=torch.bool).tril(diagonal=0)\n",
        "      attn_bias = attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
        "      self.register_buffer(\"attn_bias\",attn_bias)\n",
        "\n",
        "    def forward(self, is_last_batch, query, key, value, padding_mask=None, is_causal=False, scale=None) -> torch.Tensor:\n",
        "      # Efficient implementation equivalent to the following:\n",
        "\n",
        "      scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
        "      attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
        "\n",
        "      if is_causal:\n",
        "        if is_last_batch:\n",
        "          self.attn_bias = self.attn_bias[:query.size(-2),:query.size(-2)]\n",
        "        self.attn_bias.to(query.dtype)\n",
        "        attn_weight += self.attn_bias\n",
        "\n",
        "      if padding_mask is not None:\n",
        "          if padding_mask.dtype == torch.bool:\n",
        "            padding_mask = torch.zeros_like(padding_mask,dtype = float).masked_fill_(padding_mask, (float(\"-inf\")))\n",
        "\n",
        "          padding_mask = padding_mask.unsqueeze(0).unsqueeze(0)\n",
        "          padding_mask.to(query.dtype)\n",
        "\n",
        "          attn_weight = attn_weight.transpose(-4,-2)\n",
        "          attn_weight += padding_mask\n",
        "          attn_weight = attn_weight.transpose(-4,-2)\n",
        "\n",
        "      attn_weight = torch.softmax(attn_weight, dim=-1)\n",
        "      attn_weight = torch.dropout(attn_weight, self.dropout_p, train=True)\n",
        "      return attn_weight @ value\n",
        "# test scaled_dot_product_attention\n",
        "# t = torch.rand([2,3,4,5])\n",
        "# mask = torch.tensor([[False,False,True,True],[False,True,False,True]],dtype = torch.bool)\n",
        "# print(scaled_dot_product_attention(\"cpu\",t,t,t,padding_mask= mask, is_causal=True))\n",
        "# from torch.nn.functional import scaled_dot_product_attention\n",
        "class My_MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, max_sentence_length, kv_input_dimension, embedding_dimension, num_heads, dropout_p, if_decoder = False):\n",
        "        '''\n",
        "        embedding_dimension = input dimension\n",
        "        note that there are residual sublayers in MultiHeadedAttention\n",
        "        '''\n",
        "        super().__init__()\n",
        "        assert embedding_dimension % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.max_l = max_sentence_length\n",
        "        self.kv_d = kv_input_dimension\n",
        "        self.d = embedding_dimension\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_p = dropout_p\n",
        "        self.is_causal = if_decoder\n",
        "        self.sdpa = Scaled_Dot_Product_Attention(self.max_l,self.dropout_p)\n",
        "        self.linear_for_q = nn.Linear(self.d, self.d)\n",
        "        self.linear_for_kv = nn.Linear(self.kv_d, 2 * self.d)\n",
        "        self.linear_out_project = nn.Linear(self.d, self.d)\n",
        "\n",
        "    def forward(self, is_last_batch, q_input_data, kv_input_data , padding_mask = None):\n",
        "\n",
        "        query = self.linear_for_q(q_input_data)\n",
        "        key, value = self.linear_for_kv(kv_input_data).split(self.d,dim = -1)\n",
        "\n",
        "        query,key,value = \\\n",
        "          map(lambda x : x.view(x.size(0),x.size(1),self.num_heads,self.d//self.num_heads),[query,key,value])\n",
        "        query,key,value = \\\n",
        "          map(lambda x : x.transpose(-2,-3),[query,key,value])\n",
        "\n",
        "        x = self.sdpa(is_last_batch,query,key,value,padding_mask = padding_mask,is_causal = self.is_causal)\n",
        "        x = x.transpose(-2,-3).contiguous()\n",
        "        x = x.view(x.size(0),x.size(1),self.d)\n",
        "        x = self.linear_out_project(x)\n",
        "\n",
        "        return x\n",
        "# test My_MultiHeadedAttention\n",
        "# model = My_MultiHeadedAttention(64,128,2,0.0)\n",
        "# q_input = torch.rand(32,400,128)\n",
        "# kv_input = torch.rand(32,400,64)\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(q_input,kv_input,mask).size())\n",
        "# print(summary(model,device = \"cpu\",q_input_data = q_input, kv_input_data = kv_input,padding_mask = mask))"
      ],
      "metadata": {
        "id": "PLWLkr9UaKFD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "encoder layer(s)\n",
        "------"
      ],
      "metadata": {
        "id": "T0IjI-zC5fqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class My_Encoder_Layer(nn.Module):\n",
        "  def __init__(self,max_sentence_length,embedding_dimension,feedforward_dimension,num_heads,dropout_p):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "\n",
        "    self.attention = My_MultiHeadedAttention(self.max_l, self.emb_dim, self.emb_dim, self.num_heads, self.dropout_p)\n",
        "    self.layer_norm_attn = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_attn_layernorm = nn.Dropout(self.dropout_p)\n",
        "\n",
        "    self.feedforward = nn.Sequential(\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    )\n",
        "    self.layer_norm_feedforward = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_feedforward_layernorm = nn.Dropout(self.dropout_p)\n",
        "\n",
        "\n",
        "  def forward(self,is_last_batch,x,padding_mask):\n",
        "    x = x + self.attention(is_last_batch,x,x,padding_mask)\n",
        "    x = self.layer_norm_attn(x)\n",
        "\n",
        "    x = self.drop_out_attn_layernorm(x)\n",
        "\n",
        "    x = x + self.feedforward(x)\n",
        "    x = self.layer_norm_feedforward(x)\n",
        "    x = self.drop_out_feedforward_layernorm(x)\n",
        "\n",
        "    return x\n",
        "# test My_Encoder_Layer\n",
        "# model = My_Encoder_Layer(\"cpu\",128,256,2,0.0)\n",
        "# input = torch.rand((32,400,128))\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(input,mask).size())\n",
        "# print(summary(model,input_data = input,padding_mask = mask))\n",
        "# print(model.state_dict().keys())\n",
        "class My_Encoder(nn.Module):\n",
        "  def __init__(self,max_sentence_length,dictionary_length,embedding_dimension,feedforward_dimension,\n",
        "         padding_idx, num_heads, dropout_p, layer_num):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.dict_l = dictionary_length\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.padding_idx = padding_idx\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "    self.layer_num = layer_num\n",
        "\n",
        "    self.encoder_embedding = nn.Embedding(self.dict_l,self.emb_dim,self.padding_idx)\n",
        "    self.positional_encoding = Positional_Encoding(self.max_l,self.emb_dim)\n",
        "    self.encoder = nn.ModuleList([My_Encoder_Layer(self.max_l,self.emb_dim,self.fwd_dim,\\\n",
        "                    self.num_heads,self.dropout_p) for i in range(layer_num)])\n",
        "\n",
        "  def forward(self,is_last_batch,input,padding_mask):\n",
        "    x = self.encoder_embedding(input.unsqueeze(-1))* math.sqrt(self.emb_dim)\n",
        "    x = self.positional_encoding(x)\n",
        "\n",
        "    for index,module in enumerate(self.encoder):\n",
        "      if index == 0:\n",
        "        x = module(is_last_batch,x,padding_mask)\n",
        "      else:\n",
        "        x = module(is_last_batch,x,None)\n",
        "    return x\n",
        "# test My_Encoder\n",
        "# model = My_Encoder(\"cpu\",400,8000,128,256,0,2,0.0,2)\n",
        "# input = torch.randint(0,7999,(32,400),dtype = torch.long)\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(input,mask).size())\n",
        "# print(summary(model,input_data = input,padding_mask = mask))\n",
        "# print(model.state_dict().keys())"
      ],
      "metadata": {
        "id": "PYhd7muASnrY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "decoder layer(s)\n",
        "------"
      ],
      "metadata": {
        "id": "4O_rI3QV55Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class My_Decoder_Layer(nn.Module):\n",
        "  def __init__(self,max_sentence_length,encoder_embedding_dimension,embedding_dimension,feedforward_dimension,num_heads,dropout_p):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.encoder_dim = encoder_embedding_dimension\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "\n",
        "    self.self_attention = My_MultiHeadedAttention \\\n",
        "     (self.max_l,self.emb_dim,self.emb_dim, num_heads = self.num_heads,\\\n",
        "     dropout_p = self.dropout_p, if_decoder = True)\n",
        "    self.layer_norm_sa = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_sa = nn.Dropout(0)\n",
        "\n",
        "    self.feedforward_sa = nn.Sequential(\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    )\n",
        "    self.layer_norm_sa_fw = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_sa_fw = nn.Dropout(0)\n",
        "\n",
        "    self.cross_attention = My_MultiHeadedAttention \\\n",
        "    (self.max_l,self.encoder_dim, self.emb_dim, num_heads = self.num_heads,\n",
        "    dropout_p = self.dropout_p, if_decoder = True)\n",
        "    self.layer_norm_ca = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_ca = nn.Dropout(0)\n",
        "\n",
        "    self.feedforward_ca = nn.Sequential(\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    )\n",
        "    self.layer_norm_ca_fw = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_ca_fw = nn.Dropout(0)\n",
        "\n",
        "  def forward(self,is_last_batch,encoder_input,input,padding_mask):\n",
        "\n",
        "    x = input + self.self_attention(is_last_batch,input,input,padding_mask)\n",
        "    x = self.layer_norm_sa(x)\n",
        "    x = self.drop_out_sa(x)\n",
        "\n",
        "    x = x + self.feedforward_sa(x)\n",
        "    x = self.layer_norm_sa_fw(x)\n",
        "    x = self.drop_out_sa_fw(x)\n",
        "\n",
        "    x = x + self.cross_attention(is_last_batch,x,encoder_input,padding_mask)\n",
        "    x = self.layer_norm_ca(x)\n",
        "    x = self.drop_out_ca(x)\n",
        "\n",
        "    x = x + self.feedforward_ca(x)\n",
        "    x = self.layer_norm_ca_fw(x)\n",
        "    x = self.drop_out_ca_fw(x)\n",
        "\n",
        "    return x\n",
        "class My_Decoder(nn.Module):\n",
        "  def __init__(self,max_sentence_length, dictionary_length, encoder_embedding_dimension,\n",
        "         embedding_dimension, feedforward_dimension, padding_idx, num_heads, dropout_p, layer_num):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.dict_l = dictionary_length\n",
        "    self.encoder_dim = encoder_embedding_dimension\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.padding_idx = padding_idx\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "    self.layer_num = layer_num\n",
        "\n",
        "    self.decoder_embedding = nn.Embedding(self.dict_l,self.emb_dim,padding_idx=self.padding_idx)\n",
        "    self.positional_encoding = Positional_Encoding(self.max_l,self.emb_dim)\n",
        "    self.decoder = nn.ModuleList([My_Decoder_Layer(self.max_l,self.encoder_dim,self.emb_dim,\\\n",
        "                    self.fwd_dim,self.num_heads,self.dropout_p) for i in range(self.layer_num)])\n",
        "    # self.encoder = My_Encoder_Layer(self.emb_dim,self.fwd_dim)\n",
        "\n",
        "    self.generator = nn.Linear(self.emb_dim,self.dict_l)\n",
        "\n",
        "  def forward(self,is_last_batch,encoder_input,input,padding_mask):\n",
        "    x = self.decoder_embedding(input.unsqueeze(-1))* math.sqrt(self.emb_dim)\n",
        "    x = self.positional_encoding(x)\n",
        "    # x = self.encoder(x,padding_mask)\n",
        "    for index,module in enumerate(self.decoder):\n",
        "      if index == 0:\n",
        "        x = module(is_last_batch,encoder_input,x,padding_mask)\n",
        "      else:\n",
        "        x = module(is_last_batch,encoder_input,x,None)\n",
        "    x = self.generator(x)\n",
        "    x = F.log_softmax(x,dim = -1)\n",
        "    return x\n",
        "# test My_Decoder\n",
        "# model = My_Decoder(\"cpu\",400,8000,128,64,256,0,2,0.0,2)\n",
        "# encoder_input = torch.rand(32,400,128)\n",
        "# input = torch.randint(0,7999,(32,400),dtype = torch.long)\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(encoder_input = encoder_input,input = input, padding_mask = mask).size())\n",
        "# print(summary(model,encoder_input = encoder_input,input = input, padding_mask = mask))\n",
        "# print(model.state_dict().keys())"
      ],
      "metadata": {
        "id": "rEjaTbhyBmEV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer layer\n",
        "------"
      ],
      "metadata": {
        "id": "1gcz18nz6QTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class My_Transformer(nn.Module):\n",
        "  def __init__(self,max_sentence_length,dictionary_length,padding_idx,\n",
        "         encoder_embedding_dimension,decoder_embedding_dimension,\n",
        "         feedforward_dimension,num_heads,dropout_p,layer_num):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.dict_l = dictionary_length\n",
        "    self.padding_idx = padding_idx\n",
        "    self.en_dim = encoder_embedding_dimension\n",
        "    self.de_dim = decoder_embedding_dimension\n",
        "    self.fw_dim = feedforward_dimension\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "    self.layer_num = layer_num\n",
        "    self.encoder = My_Encoder \\\n",
        "     (self.max_l,self.dict_l,self.en_dim,self.fw_dim,\n",
        "      self.padding_idx,self.num_heads,self.dropout_p,self.layer_num)\n",
        "    self.decoder = My_Decoder \\\n",
        "     (self.max_l,self.dict_l,self.en_dim,self.de_dim,self.fw_dim,\n",
        "      self.padding_idx,self.num_heads,self.dropout_p,self.layer_num)\n",
        "\n",
        "  def forward(self,is_last_batch,src,tgt,src_mask,tgt_mask):\n",
        "    memory = self.encoder(is_last_batch,src,src_mask)\n",
        "    outputs = self.decoder(is_last_batch,memory,tgt,tgt_mask)\n",
        "    return outputs\n",
        "\n",
        "def build_model(max_sentence_length,dictionary_length,padding_idx,encoder_embedding_dimension,\n",
        "         decoder_embedding_dimension,feedforward_dimension,num_heads,dropout_p,layer_num):\n",
        "  return My_Transformer(max_sentence_length,dictionary_length,padding_idx,\n",
        "              encoder_embedding_dimension,decoder_embedding_dimension,\n",
        "              feedforward_dimension,num_heads,dropout_p,layer_num)\n",
        "# test Transformer\n",
        "# model = My_Transformer(max_sentence_length = 12,\n",
        "#             dictionary_length = 15,\n",
        "#             padding_idx = 0,\n",
        "#             encoder_embedding_dimension = 128,\n",
        "#             decoder_embedding_dimension = 64,\n",
        "#             feedforward_dimension = 256,\n",
        "#             num_heads = 2,\n",
        "#             dropout_p = 0,\n",
        "#             layer_num = 2)\n",
        "# src = torch.randint(0,15,(5,12),dtype = torch.long)\n",
        "# tgt = torch.randint(0,15,(5,12),dtype = torch.long)\n",
        "# src_mask = torch.cat(((torch.FloatTensor(5,6).uniform_() > 1),(torch.FloatTensor(5,6).uniform_() > 0.15)),dim =1)\n",
        "# tgt_mask = torch.cat(((torch.FloatTensor(5,4).uniform_() > 1),(torch.FloatTensor(5,8).uniform_() > 0.15)),dim =1)\n",
        "# out = model(False,src,tgt,src_mask,tgt_mask)\n",
        "# print(summary(model,src = src,tgt = tgt,src_mask = src_mask,tgt_mask = tgt_mask))\n",
        "\n",
        "# test build_model\n",
        "# model = build_model()\n",
        "# batch = next(iter(train_set))\n",
        "# src, tgt, src_mask, tgt_mask = batch\n",
        "# print(type(src),src.shape)\n",
        "# print(summary(model,src = src,tgt = tgt,src_mask = src_mask,tgt_mask = tgt_mask))\n",
        "# outputs = model(src,tgt,src_mask,tgt_mask)\n",
        "# print(outputs.shape)"
      ],
      "metadata": {
        "id": "PBDq_h1jKCo7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training and validation process\n",
        "======\n",
        "Noam optimizer\n",
        "------"
      ],
      "metadata": {
        "id": "sPbMfCre6cSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reference : https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "class NoamOpt:\n",
        "    def __init__(self,dictionary_length,factor,warmup,optimizer):\n",
        "        self.dict_len = dictionary_length\n",
        "        self.factor = factor\n",
        "        self.warmup = warmup\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self._rate = 0\n",
        "    def step(self):\n",
        "        self._step += 1\n",
        "        self._rate = self.factor *(self.dict_len ** (-0.5) * \\\n",
        "        min(self._step ** (-0.5), self._step * self.warmup ** (-1.5)))\n",
        "\n",
        "        self.optimizer.param_groups[0][\"lr\"] = self._rate\n",
        "        self.optimizer.step()\n",
        "    def zero_grad(self):\n",
        "        return self.optimizer.zero_grad()\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.optimizer.state_dict()\n",
        "\n",
        "    def load_state_dict(self,state_dict):\n",
        "        return self.optimizer.load_state_dict(state_dict)\n",
        "\n",
        "    def set_step(self,step):\n",
        "        self._step = step\n",
        "# test NoamOpt:\n",
        "# x = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "# x.param_groups[0][\"lr\"]"
      ],
      "metadata": {
        "id": "Lb8BnuysNCOQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "label smoothing\n",
        "------"
      ],
      "metadata": {
        "id": "3TyyebUNOeI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "  def __init__(self,batch_size,dictionary_length,padding_id,smoothing,apply_mask):\n",
        "        super().__init__()\n",
        "        self.dict_len = dictionary_length\n",
        "        self.smoothing = smoothing\n",
        "        self.padding_id = padding_id\n",
        "        shift = torch.full(size = (batch_size,1), dtype = torch.long, fill_value = self.padding_id)\n",
        "        self.register_buffer(\"shift\",shift)\n",
        "        self.apply_mask = apply_mask\n",
        "  def forward(self, is_last_batch, outputs , label):\n",
        "\n",
        "    # step1 : when using label in validation, shift is needed.\n",
        "    # label_shift : {type : tensor , shape : batch  X (max_sentence_length-1)\n",
        "    # value : int}\n",
        "    label_shift = label[:,1:]\n",
        "    # shift : {type : tensor , shape : batch  X 1 ,value : self.padding_id}\n",
        "\n",
        "    # label_shift : {type : tensor , shape : batch  X max_sentence_length\n",
        "    # value : int}\n",
        "    if is_last_batch:\n",
        "      label_shift = torch.cat((label_shift,self.shift[:label.size(0),:]),dim = 1)\n",
        "\n",
        "    else:\n",
        "      label_shift = torch.cat((label_shift,self.shift),dim = 1)\n",
        "\n",
        "    # step2 : convert label to onehot tensor, then apply label smoothing\n",
        "    # label_onehot : {type : tensor , shape : batch  X max_sentence_length X dictionary_length\n",
        "    # value : 0 or 1}\n",
        "    label_onehot = F.one_hot(label_shift,self.dict_len).float()\n",
        "    # add : {type : float}\n",
        "    add = self.smoothing / (self.dict_len)\n",
        "    # label_onehot : {type : tensor , shape : batch  X max_sentence_length X dictionary_length\n",
        "    # value : add or 1+add}\n",
        "    label_onehot += add\n",
        "    # label_smoothed : {type : tensor , shape : batch  X max_sentence_length X dictionary_length\n",
        "    # value : add or 1+add-self.smoothing}\n",
        "    label_smoothed = label_onehot.masked_fill_((label_onehot > 1),float(1-self.smoothing+add))\n",
        "\n",
        "    '''\n",
        "    Question : Is padding really needed?\n",
        "\n",
        "      If padding is applied, the model can't learning that there should be\n",
        "    no other interger but padd_id after the first bos_id.\n",
        "\n",
        "      But if padding is ignored at the beginning of the training process,\n",
        "    it's difficult to train the front part. The reason may be that loss of\n",
        "    the padding part is much easier to reduce then the front part , so the\n",
        "    parameters take a local minimun whose padding part is good but the\n",
        "    front is not good enough rapidly and keeping inside it.\n",
        "\n",
        "    So we start with padding, and remove padding when fine-tune model.\n",
        "    '''\n",
        "    # step3 : use padding mask to ignore to loss from padding id, then calculate loss.\n",
        "    # loss : {type : tensor , shape : batch  X max_sentence_length X 1, value : float}\n",
        "    loss = -1*torch.sum((outputs*label_smoothed),dim = -1)\n",
        "    if self.apply_mask == True:\n",
        "      # label_padding_mask {type : tensor , shape : batch  X max_sentence_length, value : bool}\n",
        "      label_padding_mask = (label == self.padding_id)\n",
        "      # loss : {type : tensor , shape : batch  X max_sentence_length,\n",
        "      # value : 0 or add or 1+add-self.smoothing}\n",
        "      loss = loss.masked_fill_(label_padding_mask,0)\n",
        "    # # ignore_index_number : {type : int}\n",
        "    # ignore_index_number = (mask_loss == 0).sum().item()\n",
        "    # avg_loss : {type : int}\n",
        "    # avg_loss = mask_loss.sum()/(mask_loss.size(0)*mask_loss.size(1)-ignore_index_number)\n",
        "    avg_loss = loss.sum()/loss.size(0)\n",
        "    return(avg_loss)\n",
        "\n",
        "# test LabelSmoothedCrossEntropyCriterion\n",
        "# cal1 = LabelSmoothedCrossEntropyCriterion()\n",
        "# print(cal1(outputs,tgt))\n",
        "\n",
        "# ignore_index not work correctly\n",
        "# def LabelSmoothedCrossEntropy(outputs , label,dictionary_length,smooth,padding_id):\n",
        "#   print(outputs.shape)\n",
        "#   print(label.shape)\n",
        "#   label_onehot = label.transpose(-1,-2).squeeze()\n",
        "#   outputs = outputs.transpose(-1,-2)\n",
        "#   cal_loss = nn.CrossEntropyLoss(ignore_index = padding_idx,reduction = \"mean\", label_smoothing=smooth)\n",
        "#   return cal_loss(outputs,label_onehot)"
      ],
      "metadata": {
        "id": "8JltOQM_wq4m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://arxiv.org/pdf/1512.00567.pdf page 7\n",
        "\n",
        "#Ref 1 : Hong-Yi Li ML2021 HW5\n",
        "\n",
        "# class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "#     def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "#         super().__init__()\n",
        "#         self.smoothing = smoothing\n",
        "#         self.ignore_index = ignore_index\n",
        "#         self.reduce = reduce\n",
        "\n",
        "#     def forward(self, lprobs, target):\n",
        "#         if target.dim() == lprobs.dim() - 1:\n",
        "#             target = target.unsqueeze(-1)\n",
        "#         # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
        "#         nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "#         #  reserve some probability for other labels. thus when calculating cross-entropy,\n",
        "#         # equivalent to summing the log probs of all labels\n",
        "#         smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "#         if self.ignore_index is not None:\n",
        "#             pad_mask = target.eq(self.ignore_index)\n",
        "#             nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "#             smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "#         else:\n",
        "#             nll_loss = nll_loss.squeeze(-1)\n",
        "#             smooth_loss = smooth_loss.squeeze(-1)\n",
        "#         if self.reduce:\n",
        "#             nll_loss = nll_loss.sum()\n",
        "#             smooth_loss = smooth_loss.sum()\n",
        "#         # when calculating cross-entropy, add the loss of other labels\n",
        "#         eps_i = self.smoothing / lprobs.size(-1)\n",
        "#         loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "#         return loss\n",
        "\n",
        "#Ref 2 : By hemingkx : https://github.com/hemingkx/ChineseNMT\n",
        "\n",
        "# class LabelSmoothing(nn.Module):\n",
        "#     \"\"\"Implement label smoothing.\"\"\"\n",
        "\n",
        "#     def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "#         super(LabelSmoothing, self).__init__()\n",
        "#         self.criterion = nn.KLDivLoss(size_average=False)\n",
        "#         self.padding_idx = padding_idx\n",
        "#         self.confidence = 1.0 - smoothing\n",
        "#         self.smoothing = smoothing\n",
        "#         self.size = size\n",
        "#         self.true_dist = None\n",
        "\n",
        "\n",
        "#     def forward(self, x, target):\n",
        "#         assert x.size(1) == self.size\n",
        "#         true_dist = x.data.clone()\n",
        "#         true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "#         true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "#         true_dist[:, self.padding_idx] = 0\n",
        "#         mask = torch.nonzero(target.data == self.padding_idx)\n",
        "#         if mask.dim() > 0:\n",
        "#             true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "#         self.true_dist = true_dist\n",
        "#         return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "metadata": {
        "id": "uaE0-tA9Q9cq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "beam search\n",
        "------"
      ],
      "metadata": {
        "id": "33m5daxZ7Cpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "class Decode_With_Beam_Search(nn.Module):\n",
        "    def __init__(self,batch_size,model,beam_num,max_sentence_length,\n",
        "           dictionary_length,bos_id,padding_id):\n",
        "      super().__init__()\n",
        "      self.batch_size = batch_size\n",
        "      self.model = model\n",
        "      self.beam_num = beam_num\n",
        "      self.max_sentence_length = max_sentence_length\n",
        "      self.dictionary_length = dictionary_length\n",
        "      self.bos_id = bos_id\n",
        "      self.padding_id = padding_id\n",
        "      # decoder_input : {type : tensor , shape : Batch X 1 , value : bos_id}\n",
        "      decoder_input = torch.full(size = (self.batch_size,1),fill_value = self.bos_id)\n",
        "      self.register_buffer(\"decoder_input\",decoder_input)\n",
        "      # repeat : {type : tensor , shape : Batch ,value : beam_num}\n",
        "      # each row repeat beam_num times before concatenate\n",
        "      repeat = torch.full([self.batch_size],fill_value = self.beam_num)\n",
        "      self.register_buffer(\"repeat\",repeat)\n",
        "      # decoder_probability {type : tensor , shape : Batch X beam_num X 1, value : 0}\n",
        "      decoder_probability = torch.full(size = (self.batch_size,self.beam_num,1),fill_value = 0.0)\n",
        "      self.register_buffer(\"decoder_probability\",decoder_probability)\n",
        "\n",
        "      # padding : {type : tensor , shape : (Batch X beam_num) X (max_sentence_length-(id+1)) ,value : int}\n",
        "      padding = torch.full(size = (batch_size*self.beam_num,self.max_sentence_length),fill_value = self.padding_id)\n",
        "      self.register_buffer(\"padding\",padding)\n",
        "\n",
        "      # row : {type : tensor , shape : batch X 1, value : [[0],[1],[2],...]}\n",
        "      row = torch.tensor(range(self.batch_size)).unsqueeze(1)\n",
        "      self.register_buffer(\"row\",row)\n",
        "\n",
        "    def forward(self,is_last_batch,src,src_mask):\n",
        "      if is_last_batch:\n",
        "        batch = src.size(0)\n",
        "      else :\n",
        "        batch = self.batch_size\n",
        "\n",
        "      if self.beam_num > batch:\n",
        "        beam_num = batch\n",
        "      else :\n",
        "        beam_num = self.beam_num\n",
        "\n",
        "      decoder_input = self.decoder_input[:batch,:]\n",
        "      repeat = self.repeat[:batch]\n",
        "      decoder_probability = self.decoder_probability[:batch,:,]\n",
        "      padding = self.padding[:batch*beam_num,:]\n",
        "      row = self.row[:batch,:]\n",
        "\n",
        "      # step 1 prepare first input\n",
        "\n",
        "      # memory : {type : tensor , shape : Batch X max_sentence_length X encoder_output_dim ,value : arbitary float}\n",
        "      memory = self.model.encoder(is_last_batch,src,src_mask)\n",
        "      # padding_init : {type : tensor , shape : (Batch X beam_num) X (max_sentence_length-1) ,value : int}\n",
        "      padding_init = padding[:batch,:self.max_sentence_length-1]\n",
        "      # input_with_padding : {type : tensor , shape : (Batch X beam_num) X max_sentence_length,\n",
        "      # value : [[bos_id],[any_id],...[padding_id],....] X Batch}\n",
        "      input_with_padding = torch.cat((decoder_input,padding_init),dim = 1)\n",
        "      # add : {type : tensor , shape : Batch X beam_num X 1,\n",
        "      # value : [[bos_id],[any_id],...[padding_id],....] X Batch}\n",
        "      add_prob, add = \\\n",
        "      torch.topk(self.model.decoder(is_last_batch,memory,decoder_input,None)[:,1,:],k = beam_num)\n",
        "      # add : {type : tensor , shape : (Batch X beam_num) X 1,\n",
        "      # value : [[bos_id],[any_id],...[padding_id],....] X Batch}\n",
        "      add = add.view(batch*beam_num,1)\n",
        "      # decoder_beam_expand : {type : tensor , shape : (Batch X beam_num) X 1 ,value : bos_id}\n",
        "      decoder_beam_expand = torch.repeat_interleave(decoder_input,repeat,dim=0)\n",
        "      # decoder_beam_expand : {type : tensor , shape : (Batch X beam_num) X 2 ,value : int}\n",
        "      decoder_beam_expand = torch.cat((decoder_beam_expand,add),dim = 1)\n",
        "\n",
        "      # decoder_probability : {type : tensor , shape : Batch X beam_num X 1,\n",
        "      # value : [P1,P2,P3...] X beam_num times (Pk is log_softmax probability)}\n",
        "\n",
        "      decoder_probability = (decoder_probability+add_prob.unsqueeze(-1))\n",
        "\n",
        "      # memory_beam_expand : {type : tensor ,\n",
        "      # shape : (Batch X beam_num) X max_sentence_length X encoder_output_dim ,value : float}\n",
        "      memory_beam_expand = torch.repeat_interleave(memory,repeat,dim=0)\n",
        "\n",
        "\n",
        "      # step 2 : iteration to create sentence\n",
        "\n",
        "      for id in range(self.max_sentence_length-2):\n",
        "        # padding : {type : tensor , shape : (Batch X beam_num) X (max_sentence_length-(id+2)) ,value : int}\n",
        "\n",
        "        padding = padding[:,:self.max_sentence_length-(id+2)]\n",
        "        # out_with_padding : {type : tensor , shape : (Batch X beam_num) X max_sentence_length,\n",
        "        # value : [[bos_id],[any_id],...[padding_id],....] X Batch}\n",
        "        out_with_padding = torch.cat((decoder_beam_expand,padding),dim = 1)\n",
        "\n",
        "        # model decoder spend more than 80% of calculation time\n",
        "        # out_add : {type : tensor , shape : Batch X beam_num X dictionary_length ,value : int}\n",
        "        out_add = self.model.decoder(is_last_batch,memory_beam_expand,out_with_padding,None)[:,id+2,:]\\\n",
        "              .view(batch,beam_num,self.dictionary_length)\n",
        "\n",
        "        # decoder_beam_expand : {type : tensor , shape : (Batch X beam_num) X (id+2) ,value : int}\n",
        "        # decoder_probability {type : tensor , shape : Batch X beam_num X 1 , value : log_softmax probability}\n",
        "\n",
        "        decoder_beam_expand , decoder_probability = \\\n",
        "        self.beam_search_one_step(batch,beam_num,repeat,row,decoder_beam_expand,decoder_probability,out_add)\n",
        "\n",
        "      # step 3 : choose the best sentences\n",
        "\n",
        "      # decoder_beam_expand : {type : tensor , shape : Batch X beam_num X (max_sentence_length) ,value : 0 or 1}\n",
        "      decoder_beam_expand = decoder_beam_expand.view(batch,beam_num,self.max_sentence_length)\n",
        "      # decoder_out : {type : tensor , shape :  Batch X max_sentence_length ,\n",
        "      # value : [[int,int,...],[int,int...],...]}\n",
        "      decoder_out =  decoder_beam_expand[:,0,:].view(batch,-1)\n",
        "\n",
        "      return decoder_out,F.one_hot(decoder_out,self.dictionary_length).float()\n",
        "\n",
        "    def beam_search_one_step(self,batch,beam_num,repeat,row,sentences,p_sentences,n_beam_output):\n",
        "    # sentences : {type : tensor , shape : (batch X beam_num) X now_sentences_length X 1 value : int}\n",
        "    # p_sentences : {type : tensor , shape : batch X beam_num X 1 value : log_softmax probability}\n",
        "    # n_beam_output : {type : tensor , shape : batch X beam_num X dictionary_length,\n",
        "    # value : [P1,P2,P3...] X beam_num times (Pk in [0,1])}\n",
        "      '''\n",
        "      TO DO : (set beam num = K)\n",
        "      for every batch:\n",
        "      expand sentences(total number = K) K times (so there are K-square sentences),then concat with\n",
        "      the index of top K consequence of each beam(total K beams) in n_beam_output (so there are also K-square values).\n",
        "      '''\n",
        "\n",
        "      # sentences : {type : tensor , shape : batch X beam_num X now_sentences_length value : int}\n",
        "      sentences = sentences.view(batch,beam_num,-1)\n",
        "\n",
        "      # repeat : {type : tensor , shape : beam_num ,value : beam_num}\n",
        "      # each row repeat beam_num times before concatenate\n",
        "      repeat = repeat[:beam_num]\n",
        "      # sentences_expand : {type : tensor , shape : batch X (beam_num X beam_num) X now_sentences_length ,\n",
        "      # value : [[[A,B...] X beam_num times,[C,D...] X beam_num times}...] A,B,C,D...are int}\n",
        "      sentences_expand = torch.repeat_interleave(sentences,repeat,dim=1)\n",
        "      # sentences_expand = sentences.repeat(1,beam_num,1topk_)\n",
        "\n",
        "\n",
        "      # topk_prob : {type : tensor , shape : batch X beam_num X beam_num, value : log_softmax probability}\n",
        "      # topk_index : {type : tensor , shape : batch X beam_num X beam_num, value : int}\n",
        "      topk_prob, topk_index = torch.topk(n_beam_output,dim = -1,k = beam_num)\n",
        "      # topk_index : {type : tensor , shape : batch X (beam_num X beam_num) X 1, value : int}\n",
        "      topk_index = topk_index.view(batch,-1,1)\n",
        "\n",
        "      # sentences : {type : tensor , shape : batch X (beam_num X beam_num) X (now_sentences_length+1), value : int}\n",
        "      sentences_expand = torch.cat((sentences_expand,topk_index),dim = -1)\n",
        "\n",
        "      '''\n",
        "      TO DO :\n",
        "      multipies p_sentences with the probability of top K consequence of each beam(total K beams) in n_beam_output\n",
        "      (so there are also K-square values).\n",
        "\n",
        "      The final step is to choose Top K consequence from K-square sentences by using p_sentences.\n",
        "      '''\n",
        "\n",
        "      # p_sentences : {type : tensor , shape : batch X (beam_num X beam_num),\n",
        "      # value : [P1,P2,P3...] X beam_num times (Pk is log_softmax probability)}\n",
        "      p_sentences = (p_sentences+topk_prob).view(batch,-1)\n",
        "      # p_sentences : {type : tensor , shape : batch X beam_num, value : log_softmax probability}\n",
        "      # p_index : {type : tensor , shape : batch X beam_num, value : int}\n",
        "      p_sentences, p_index = torch.topk(p_sentences, dim = 1, k = beam_num)\n",
        "      p_sentences = p_sentences.unsqueeze(-1)\n",
        "      # row : {type : tensor , shape : batch X 1, value : [[0],[1],[2],...]}\n",
        "      # sentences : {type : tensor , shape : batch X beam_num X (now_sentences_length+1), value : log_softmax probability}\n",
        "      new_sentences = sentences_expand[row, p_index].view(batch*beam_num,-1)\n",
        "\n",
        "      return new_sentences,p_sentences\n",
        "# test Decode_With_Beam_Search\n",
        "# decode_model = Decode_With_Beam_Search(batch_size = 5,\n",
        "#                     model = model,\n",
        "#                     beam_num = 2,\n",
        "#                     max_sentence_length = 12,\n",
        "#                     dictionary_length = 15,\n",
        "#                     bos_id = 2,\n",
        "#                     padding_id = 0)\n",
        "# output = decode_model(False,src,src_mask)\n",
        "\n",
        "# test repeat and sentence select\n",
        "\n",
        "# x = torch.randint(0,10,(2,5))\n",
        "# print(x)\n",
        "# r1 = x.repeat(3,1).view(3,2,-1)\n",
        "# print(r1)\n",
        "# r2 = x.repeat_interleave(torch.tensor([3,3]),dim=0).view(3,2,-1)\n",
        "# print(r2)\n",
        "# row = torch.tensor(range(3)).unsqueeze(1)\n",
        "# p_index = torch.tensor([[1,0],[0,1],[0,1]])\n",
        "# new_sentences = r1[row, p_index].view(6,-1)\n",
        "# print(new_sentences)\n",
        "\n",
        "# test decode_with_beam_search"
      ],
      "metadata": {
        "id": "iMGqrGp4N230"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bleu\n",
        "------"
      ],
      "metadata": {
        "id": "Pt1o-F1Ik8cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torcheval.metrics.functional.text import bleu\n",
        "def get_bleu_score(outputs,tgt,tgt_tokenizer,eos_id):\n",
        "    outputs = np.array(outputs.detach().tolist())\n",
        "    outputs = [x[:np.nonzero(x == eos_id)[0][0]].tolist() if len(np.nonzero(x == eos_id)[0])>0 \\\n",
        "              else x.tolist() for x in outputs ]\n",
        "\n",
        "    outputs_decode = tgt_tokenizer.decode(outputs)\n",
        "    out = [outputs_decode[i] for i in range(len(outputs_decode)) if len(outputs_decode[i])>= 4]\n",
        "    out = [\" \".join(list(x)) for x in out]\n",
        "    tgt_decode = tgt_tokenizer.decode(tgt.detach().tolist())\n",
        "    tgt = [tgt_decode[i] for i in range(len(outputs_decode)) if len(outputs_decode[i])>= 4]\n",
        "    tgt = [\" \".join(list(x)) for x in tgt]\n",
        "    return bleu.bleu_score(out, tgt, n_gram=4).detach().item()\n",
        "# test bleu\n",
        "# test_tokenizer = spm.SentencePieceProcessor(model_file = \"/content/spm_8000_zh.model\")\n",
        "# candidates = torch.tensor([[21,3,9,99,42],[5,78,89,3,31]])\n",
        "# references = torch.tensor([[18,5,9,3,42],[3,5,78,89,50]])\n",
        "# get_bleu_score(candidates,references,test_tokenizer,3)"
      ],
      "metadata": {
        "id": "ISpHxGVQk7dh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train and validation function\n",
        "------"
      ],
      "metadata": {
        "id": "GF4GshwGjz-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "\n",
        "def train_one_batch(device,model,loss_calculator,is_last_batch,\n",
        "          src,tgt,src_mask,tgt_mask,dictionary_length,\n",
        "          optimizer):\n",
        "\n",
        "    outputs = model(is_last_batch,src,tgt,src_mask,tgt_mask)\n",
        "\n",
        "    loss = loss_calculator(is_last_batch,outputs,tgt)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return loss.detach().item(),outputs[0].detach()\n",
        "\n",
        "def valid(device,model,loss_calculator,batch_size_setting,valid_loader,beam_num,max_sentence_length,\n",
        "      dictionary_length,bos_id,eos_id,pad_id,tgt_tokenizer):\n",
        "    batch_loss = []\n",
        "    batch_bleu_score = []\n",
        "    with torch.no_grad():\n",
        "      for val_batch in tqdm(valid_loader,desc=\"valid_step\", unit=\" step\"):\n",
        "        src,tgt,src_mask,tgt_mask = val_batch\n",
        "        src,tgt,src_mask = src.to(device),tgt.to(device),src_mask.to(device)\n",
        "\n",
        "        batch_size = src.size(0)\n",
        "\n",
        "        is_last_batch = False\n",
        "        if batch_size != batch_size_setting:\n",
        "          is_last_batch = True\n",
        "        decode_model = Decode_With_Beam_Search(batch_size,model,beam_num,max_sentence_length,\n",
        "                            dictionary_length,bos_id,pad_id)\n",
        "        decode_model.to(device)\n",
        "        outputs_in_word,outputs = decode_model(is_last_batch,src,src_mask)\n",
        "        # outputs_in_word,outputs = decode_with_beam_search(device,is_last_batch,model,src,src_mask,beam_num,\\\n",
        "        #       max_sentence_length,dictionary_length,bos_id,pad_id)\n",
        "\n",
        "\n",
        "        loss = loss_calculator(is_last_batch,outputs,tgt)\n",
        "\n",
        "        bleu_score = get_bleu_score(outputs_in_word,tgt,tgt_tokenizer,eos_id)\n",
        "\n",
        "        batch_loss.append(loss)\n",
        "        batch_bleu_score.append(bleu_score)\n",
        "\n",
        "      avg_valid_loss = batch_loss.sum()/len(batch_loss).detach().item()\n",
        "      avg_bleu_score = batch_bleu_score.sum()/len(batch_bleu_score).detach().item()\n",
        "\n",
        "    return avg_valid_loss,avg_bleu_score"
      ],
      "metadata": {
        "id": "bwXkljhMFqKV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main function\n",
        "------"
      ],
      "metadata": {
        "id": "R7I8W9cQj7ES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def main(setting,dataset_is_prepare = False,load_model = False,fine_tune = False):\n",
        "\n",
        "    # set random seed\n",
        "    myseed = 1\n",
        "    np.random.seed(myseed)\n",
        "    torch.manual_seed(myseed)\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed_all(myseed)\n",
        "\n",
        "    # data set & tokenizer\n",
        "    if not dataset_is_prepare:\n",
        "        clean_data_and_save(\n",
        "        path_doc = setting[\"data_info\"][\"document\"],\n",
        "        raw_src_path = setting[\"data_info\"][\"source\"][\"raw_data_path\"],\n",
        "        raw_tgt_path = setting[\"data_info\"][\"target\"][\"raw_data_path\"],\n",
        "        clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "        clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"],\n",
        "        threshold = setting[\"tokenized_setting\"][\"max_l\"])\n",
        "\n",
        "        src_tokenizer,tgt_tokenizer = tokenized_data(\n",
        "            vocab_size = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "            tokenized_setting = {k:setting[\"tokenized_setting\"][k] for k in \\\n",
        "                      set(list(setting[\"tokenized_setting\"].keys()))-{\"vocab_size\",\"max_l\"}},\n",
        "            max_l = setting[\"tokenized_setting\"][\"max_l\"],\n",
        "            path_doc = setting[\"data_info\"][\"document\"],\n",
        "            clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "            clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"],\n",
        "            src_lang = setting[\"data_info\"][\"source\"][\"lang\"],\n",
        "            tgt_lang = setting[\"data_info\"][\"target\"][\"lang\"],\n",
        "            st_train_path = setting[\"data_info\"][\"source\"][\"tokenized_train_data\"],\n",
        "            st_val_path = setting[\"data_info\"][\"source\"][\"tokenized_val_data\"],\n",
        "            tt_train_path = setting[\"data_info\"][\"target\"][\"tokenized_train_data\"],\n",
        "            tt_val_path = setting[\"data_info\"][\"target\"][\"tokenized_val_data\"],\n",
        "            bos_id = setting[\"tokenized_setting\"][\"bos_id\"],\n",
        "            eos_id = setting[\"tokenized_setting\"][\"eos_id\"],\n",
        "            pad_id = setting[\"tokenized_setting\"][\"pad_id\"])\n",
        "    else:\n",
        "        src_tokenizer,tgt_tokenizer = get_tokenizers(\n",
        "            path_doc = setting[\"data_info\"][\"document\"],\n",
        "            vocab_size = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "            src_lang = setting[\"data_info\"][\"source\"][\"lang\"],\n",
        "            tgt_lang = setting[\"data_info\"][\"target\"][\"lang\"],)\n",
        "\n",
        "    # data loader\n",
        "    train_batch_size_setting = setting[\"training_hparas\"][\"train_batch_size\"]\n",
        "    valid_batch_size_setting = setting[\"training_hparas\"][\"valid_batch_size\"]\n",
        "    train_loader,valid_loader = get_data_set(\n",
        "        train_batch_size = train_batch_size_setting,\n",
        "        valid_batch_size = valid_batch_size_setting,\n",
        "        num_workers = setting[\"training_hparas\"][\"workers\"],\n",
        "        path_doc = setting[\"data_info\"][\"document\"],\n",
        "        st_train_path = setting[\"data_info\"][\"source\"][\"tokenized_train_data\"],\n",
        "        st_val_path = setting[\"data_info\"][\"source\"][\"tokenized_val_data\"],\n",
        "        tt_train_path = setting[\"data_info\"][\"target\"][\"tokenized_train_data\"],\n",
        "        tt_val_path = setting[\"data_info\"][\"target\"][\"tokenized_val_data\"],\n",
        "        pad_id = setting[\"tokenized_setting\"][\"pad_id\"])\n",
        "    train_iter = iter(train_loader)\n",
        "    valid_iter = iter(valid_loader)\n",
        "\n",
        "    # model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model =  build_model(\n",
        "          max_sentence_length = setting[\"tokenized_setting\"][\"max_l\"],\n",
        "          dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "          padding_idx = setting[\"tokenized_setting\"][\"pad_id\"],\n",
        "          encoder_embedding_dimension = setting[\"model\"][\"encoder_embedding_dimension\"],\n",
        "          decoder_embedding_dimension = setting[\"model\"][\"decoder_embedding_dimension\"],\n",
        "          feedforward_dimension = setting[\"model\"][\"feedforward_dimension\"],\n",
        "          num_heads = setting[\"model\"][\"num_heads\"],\n",
        "          dropout_p = setting[\"model\"][\"dropout_p\"],\n",
        "          layer_num = setting[\"model\"][\"layer_num\"])\n",
        "\n",
        "    if load_model:\n",
        "      checkpoint = torch.load(setting[\"training_hparas\"][\"model_temporary_saving_path\"])\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    apply_mask = True\n",
        "    if fine_tune:\n",
        "     apply_mask = False\n",
        "\n",
        "    train_loss_calculator = LabelSmoothedCrossEntropyCriterion(\n",
        "                batch_size = train_batch_size_setting,\n",
        "                dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "                padding_id = setting[\"tokenized_setting\"][\"pad_id\"],\n",
        "                smoothing = setting[\"training_hparas\"][\"label_smoothing\"],\n",
        "                apply_mask = apply_mask)\n",
        "\n",
        "    valid_loss_calculator = LabelSmoothedCrossEntropyCriterion(\n",
        "            batch_size = valid_batch_size_setting,\n",
        "            dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "            padding_id = setting[\"tokenized_setting\"][\"pad_id\"],\n",
        "            smoothing = 0,\n",
        "            apply_mask = apply_mask)\n",
        "\n",
        "    train_loss_calculator,valid_loss_calculator = \\\n",
        "    train_loss_calculator.to(device),valid_loss_calculator.to(device)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), **(setting[\"training_hparas\"][\"optimization\"][\"optimizer\"]))\n",
        "\n",
        "    Noam_optimizer = NoamOpt(\n",
        "             dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "             factor = setting[\"training_hparas\"][\"optimization\"][\"factor\"],\n",
        "             warmup = setting[\"training_hparas\"][\"optimization\"][\"warmup\"],\n",
        "             optimizer = optimizer)\n",
        "    if load_model:\n",
        "      Noam_optimizer.set_step(checkpoint['step'])\n",
        "      Noam_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # step\n",
        "    step = 0\n",
        "    if load_model:\n",
        "      step = checkpoint['step']+1\n",
        "    print(f\"Start step is {step}\")\n",
        "    total_step = setting[\"training_hparas\"][\"total_step\"]\n",
        "    early_stop_epoch = setting[\"training_hparas\"][\"early_stop_step\"]\n",
        "    do_valid_steps = setting[\"training_hparas\"][\"do_valid_step\"]\n",
        "    early_stop_count = 0\n",
        "    progress_bar = tqdm(total = do_valid_steps-step, desc=\"train_step\", unit=\" step\")\n",
        "\n",
        "    # output datas\n",
        "    train_loss_every_batchs = []\n",
        "    valid_loss = []\n",
        "    bleu_score = []\n",
        "    best_bleu_score = 0\n",
        "\n",
        "    while step <= total_step:\n",
        "\n",
        "      # training\n",
        "      # iter batch\n",
        "      try:\n",
        "        train_batch = next(train_iter)\n",
        "      except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        train_batch = next(train_iter)\n",
        "\n",
        "      # compute batch loss and update parameters in model\n",
        "      model.train()\n",
        "\n",
        "      src,tgt,src_mask,tgt_mask = train_batch\n",
        "      src,tgt,src_mask,tgt_mask = src.to(device),tgt.to(device),\\\n",
        "                     src_mask.to(device),tgt_mask.to(device)\n",
        "      batch_size = src.size(0)\n",
        "\n",
        "      is_last_batch = False\n",
        "      if batch_size != train_batch_size_setting:\n",
        "        is_last_batch = True\n",
        "\n",
        "      train_loss, test_sentence = train_one_batch(\n",
        "              device = device,\n",
        "              model = model,\n",
        "              loss_calculator = train_loss_calculator,\n",
        "              is_last_batch = is_last_batch,\n",
        "              src = src,\n",
        "              tgt = tgt,\n",
        "              src_mask = src_mask,\n",
        "              tgt_mask = tgt_mask,\n",
        "              dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "              optimizer = Noam_optimizer)\n",
        "\n",
        "      train_loss_every_batchs.append(train_loss)\n",
        "      temp_save_and_print = setting[\"training_hparas\"][\"temp_save_step\"]\n",
        "      if (step+1) % (temp_save_and_print) == 0:\n",
        "        loss_list = train_loss_every_batchs[int(-1*(temp_save_and_print)):]\n",
        "        print(sum(loss_list) / len(loss_list))\n",
        "        print(tgt_tokenizer.decode(torch.argmax(test_sentence,dim = -1).tolist()))\n",
        "        print(tgt_tokenizer.decode(tgt[0].detach().tolist()))\n",
        "        torch.save({'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': Noam_optimizer.state_dict(),\n",
        "            'step': step},\n",
        "            setting[\"training_hparas\"][\"model_temporary_saving_path\"])\n",
        "\n",
        "      progress_bar.update()\n",
        "      if (step+1) % do_valid_steps == 0:\n",
        "\n",
        "        print(train_loss_every_batchs[-1])\n",
        "\n",
        "        progress_bar.close()\n",
        "\n",
        "        model.eval()\n",
        "        avg_val_loss,avg_bleu_score = valid(\n",
        "                        device = device,\n",
        "                        model = model,\n",
        "                        loss_calculator = valid_loss_calculator,\n",
        "                        batch_size_setting = valid_batch_size_setting,\n",
        "                        valid_loader = valid_loader,\n",
        "                        beam_num = setting[\"training_hparas\"][\"beam_num\"],\n",
        "                        max_sentence_length = setting[\"tokenized_setting\"][\"max_l\"],\n",
        "                        dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "                        bos_id = setting[\"tokenized_setting\"][\"bos_id\"],\n",
        "                        eos_id = setting[\"tokenized_setting\"][\"eos_id\"],\n",
        "                        pad_id = setting[\"tokenized_setting\"][\"pad_id\"],\n",
        "                        tgt_tokenizer = tgt_tokenizer)\n",
        "        valid_loss.append(avg_val_loss)\n",
        "        bleu_score.append(avg_bleu_score)\n",
        "\n",
        "        # print avg loss\n",
        "        print(f\"average train loss = {sum(train_loss_every_batchs[-1*do_valid_steps:-1])/len(do_valid_steps):.4f}\")\n",
        "        print(f\"average valid loss = {valid_loss[-1]:.4f}\")\n",
        "        print(f\"average valid loss = {bleu_score[-1]:.4f}\")\n",
        "\n",
        "        # saving model and check early stop criterion\n",
        "        if bleu_score[-1] > best_bleu_score:\n",
        "          torch.save(model.state_dict(), setting[\"training_hparas\"][\"model_saving_path\"])\n",
        "        else :\n",
        "          early_stop_count += 1\n",
        "\n",
        "        if early_stop_count == early_stop_epoch:\n",
        "          break\n",
        "\n",
        "        progress_bar = tqdm(total = do_valid_steps, desc=\"train_step\", unit=\" step\")\n",
        "\n",
        "      step += 1\n",
        "    progress_bar.close()\n",
        "\n",
        "    return train_loss_every_batchs,valid_loss,bleu_score"
      ],
      "metadata": {
        "id": "e7hn-NAjHSQ5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(setting, dataset_is_prepare = True, load_model = True)\n",
        "# gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGUYF3WnrI9F",
        "outputId": "57defd1f-82f0-490c-ffc1-227973b22d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 384064/384064 [00:29<00:00, 13084.80it/s]\n",
            "100%|██████████| 384064/384064 [00:31<00:00, 12048.04it/s]\n",
            "100%|██████████| 3879/3879 [00:00<00:00, 17007.35it/s]\n",
            "100%|██████████| 3879/3879 [00:00<00:00, 15920.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now step is 40000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:   5%|▍         | 999/20000 [15:31<4:55:20,  1.07 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86.41491871643066\n",
            "活在在一個觀的中一個。的,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,著著著著著著,著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著著\n",
            "生命結構就在外星空下形成了。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  10%|▉         | 1999/20000 [31:06<4:38:30,  1.08 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85.75711796188355\n",
            "所以,什麼哪裡??????????,,,,的,,,?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
            "所以,他們在哪裡?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  15%|█▍        | 2999/20000 [46:38<4:23:58,  1.07 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85.89685918807983\n",
            "我必須行得了讓他他。。。。,。。。。。。。。。然,,,,。。,,,,我我,我我我我我然我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我\n",
            "我同他接觸,我可以研究他,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  20%|█▉        | 3999/20000 [1:02:10<4:08:42,  1.07 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86.48959498214721\n",
            "對我,,這是能力最民主上最一數二的的主要之一。之的下的的的的的的著著,而著的的的的的創造創造,,我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我\n",
            "因此我認為,超連結是史上數一數二民主的設計了。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  25%|██▍       | 4999/20000 [1:17:42<3:53:13,  1.07 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85.65921239471436\n",
            "他們會把倒入,電,他們覺得覺得覺得感受自己父母讓他們他們父母覺得覺得自己沒有效為力。。。。。。。。。。。。。。。。。。。,。。。,。。。。。,,,,。,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,的的的的的的的的不不不不不不聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲\n",
            "他們會摔鍵盤發洩,他們會真的開始去恨自己,而他們的父母則覺得完全無能為力。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  30%|██▉       | 5999/20000 [1:33:18<3:36:55,  1.08 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85.79260115814209\n",
            "人類體可能更更複雜,但也許可能如此複雜了。。了了。。。。。。。。。。。。。。,。,。。。,。,,。,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
            "在人身上可能要更複雜,但也不至於太複雜\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  35%|███▍      | 6999/20000 [1:48:54<3:21:35,  1.07 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85.64382176589966\n",
            "如果你要你們多斯爾蕾舞蹈合作,你紐約克里和紐約市的士蕾舞者成為好。,,,。。。。。。。。。。聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲聲\n",
            "為了和波修瓦芭蕾舞團競爭,讓傑弗睿和紐約市力芭蕾舞團越來越好。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  40%|███▉      | 7999/20000 [2:04:26<3:05:59,  1.08 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88.2398205833435\n",
            "我稱稱稱為名報陸「冷亮的物\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"的\"的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的。。。。。。。。。。。。。。。。。。。。\n",
            "我將那時的海軍稱之為\"明智之塵\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  45%|████▍     | 8999/20000 [2:19:57<2:49:56,  1.08 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87.83636949539185\n",
            "在知道知道,,在把把把了,克了,他們對來。。。出著。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
            "我們還沒反應過來,他們就已經派來坦克,之後由派進來軍隊,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  50%|████▉     | 9999/20000 [2:35:35<2:34:29,  1.08 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84.98909572982788\n",
            "你如何能你裡面是壓力慮,而不是邊是不會有???而????的??的的?的?的的???的,的,?,的,,,,,的,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
            "怎麼可能身體的一邊很焦慮,另一邊卻不會?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  55%|█████▍    | 10999/20000 [2:51:11<2:19:26,  1.08 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81.38089778900147\n",
            "我們不感謝祝,沒有在在三00年前就的貓蜓就貓王。。。。。的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而而\n",
            "而且我們要慶幸自己不是生活在3億年前,那時的蜻蜓跟貓一樣大。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  60%|█████▉    | 11999/20000 [3:06:43<2:03:57,  1.08 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84.9609408493042\n",
            "這點嚇。。。。。。。。。,,,,,,,,,不我,而,,不而而,,不不,,而,,,,,,,,,,,我我我,我我我我,,我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我\n",
            "有點可怕\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  65%|██████▍   | 12924/20000 [3:21:05<1:50:03,  1.07 step/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "get plot\n",
        "------"
      ],
      "metadata": {
        "id": "s1pWYBDSenBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as plt\n",
        "class get_plot():\n",
        "    def __init__(self,train_loss_every_batchs,valid_loss,bleu_score,\n",
        "           step = setting[\"training_hparas\"][\"do_valid_step\"]):\n",
        "      self.train = train_loss_every_batchs\n",
        "      self.val = valid_loss\n",
        "      self.score = bleu_score\n",
        "      self.step = step\n",
        "    def plot_train_loss(self,figsize = [6,12],ylims = [0,10]):\n",
        "      index = [i for i in range(len(self.train))]\n",
        "      plt.figure(figsize = figsize, layout='constrained')\n",
        "      plt.plot(data = [index, self.train],\n",
        "           color = \"ted:blue\", label = \"train loss\")\n",
        "      plt.ylim(ylims)\n",
        "      plt.xlabel(\"step\")\n",
        "      plt.ylabel(\"loss\")\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "    def plot_loss(self,figsize = [6,12],ylims = [0,10]):\n",
        "      avg_step = min(self.step//10,1)\n",
        "      train_index = list(range(0,len(self.train),self.step))\n",
        "\n",
        "      train_avg = [self.train[:avg_step].mean()] + \\\n",
        "             [self.train[i-avg_step:i+avg_step].mean() for i in train_index[1:-1]] + \\\n",
        "             [self.train[-avg_step:].mean()]\n",
        "\n",
        "      plt.figure(figsize = figsize, layout='constrained')\n",
        "      plt.plot(data = [train_index, train_avg],\n",
        "          color = \"ted:blue\", label = \"train loss\")\n",
        "      plt.plot(data = [train_index, self.val],\n",
        "          color = \"ted:red\", label = \"val loss\")\n",
        "      plt.ylim(ylims)\n",
        "      plt.xlabel(\"step\")\n",
        "      plt.ylabel(\"loss\")\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "    def plot_bleu_score(self,figsize = [6,12],ylims = [0,10]):\n",
        "      train_index = list(range(0,len(self.train),self.step))\n",
        "\n",
        "\n",
        "      plt.figure(figsize = figsize, layout='constrained')\n",
        "      plt.plot(data = [train_index, self.score],\n",
        "          color = \"ted:blue\", label = \"train loss\")\n",
        "\n",
        "      plt.ylim(ylims)\n",
        "      plt.xlabel(\"step\")\n",
        "      plt.ylabel(\"bleu_score\")\n",
        "      plt.legend()\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "WHjjzhzIIXl3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 4 inference\n",
        "======\n",
        "infer dataset\n",
        "------"
      ],
      "metadata": {
        "id": "6xKQ5USkfFU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "# make dataset\n",
        "class infer_dataset(Dataset):\n",
        "    def __init__(self,list_of_sentences,src_tokenizer,bos_id,eos_id,pad_id,max_l):\n",
        "      # clean\n",
        "      infer_list = [s[0] for s in list(map(clean_s_en,list_of_sentences))]\n",
        "      infer_list = [\" \".join(s) for s in list(map(divide_by_END,infer_list))]\n",
        "      # padding and tokenized\n",
        "      padding_src = []\n",
        "      len_s = 0\n",
        "      for sentence in infer_list:\n",
        "        s = src_tokenizer.encode(sentence, out_type=int)\n",
        "        s = np.append(s,[eos_id])\n",
        "        s = np.append([bos_id],np.pad(s,(0, max_l-len(s)-1), constant_values = pad_id))\n",
        "        padding_src.append(s.tolist())\n",
        "      # convert to tensor\n",
        "      self.infer_set = torch.LongTensor(padding_src)\n",
        "    def __getitem__(self, index):\n",
        "      return self.infer_set[index]\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.infer_set)\n",
        "\n",
        "    @classmethod\n",
        "    def padding_mask_batch(cls,batch,pad_id):\n",
        "      \"\"\"Collate a batch of data.\"\"\"\n",
        "      infer = torch.stack(batch)\n",
        "      infer_padding = (infer == pad_id)\n",
        "      return infer, infer_padding"
      ],
      "metadata": {
        "id": "djNxQQSefEcN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "infer function\n",
        "------"
      ],
      "metadata": {
        "id": "DOcJEOa1wd3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(device,model,batch_size_setting,infer_loader,beam_num,max_sentence_length,\n",
        "      dictionary_length,bos_id,eos_id,pad_id):\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for infer_batch in tqdm(infer_loader,desc=\"infer_step\", unit=\" step\"):\n",
        "        infer,infer_mask = infer_batch\n",
        "        infer,infer_mask = infer.to(device),infer_mask.to(device)\n",
        "\n",
        "        batch_size = infer.size(0)\n",
        "\n",
        "        is_last_batch = False\n",
        "        if batch_size != batch_size_setting:\n",
        "          is_last_batch = True\n",
        "        decode_model = Decode_With_Beam_Search(batch_size,model,1,max_sentence_length,\n",
        "                            dictionary_length,bos_id,pad_id)\n",
        "        decode_model.to(device)\n",
        "        outputs_in_word,outputs = decode_model(is_last_batch,infer,infer_mask)\n",
        "\n",
        "\n",
        "\n",
        "    return outputs_in_word.detach().tolist()"
      ],
      "metadata": {
        "id": "HcFXq3wbp6Eu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "inference main function\n",
        "------"
      ],
      "metadata": {
        "id": "oR_JiP1V9tIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def inference_main(setting,list_of_sentences):\n",
        "  # get tokenizer\n",
        "  src_tokenizer,tgt_tokenizer = get_tokenizers(\n",
        "            path_doc = setting[\"data_info\"][\"document\"],\n",
        "            vocab_size = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "            src_lang = setting[\"data_info\"][\"source\"][\"lang\"],\n",
        "            tgt_lang = setting[\"data_info\"][\"target\"][\"lang\"],)\n",
        "\n",
        "  max_l = setting[\"tokenized_setting\"][\"max_l\"]\n",
        "  bos_id = setting[\"tokenized_setting\"][\"bos_id\"]\n",
        "  eos_id = setting[\"tokenized_setting\"][\"eos_id\"]\n",
        "  pad_id = setting[\"tokenized_setting\"][\"pad_id\"]\n",
        "  num_workers = setting[\"training_hparas\"][\"workers\"]\n",
        "  infer_batch_size_setting = setting[\"training_hparas\"][\"valid_batch_size\"]\n",
        "  # make dataloader\n",
        "  infer_set = infer_dataset(list_of_sentences,src_tokenizer,bos_id,eos_id,pad_id,max_l)\n",
        "  infer_loader = DataLoader(\n",
        "    infer_set,\n",
        "    batch_size = infer_batch_size_setting,\n",
        "    shuffle = True,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    collate_fn = lambda x : infer_dataset.padding_mask_batch(x,\n",
        "                   pad_id = pad_id))\n",
        "\n",
        "  # set model and device\n",
        "  max_sentence_length = setting[\"tokenized_setting\"][\"max_l\"]\n",
        "  dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"]\n",
        "\n",
        "  model =  build_model(\n",
        "      max_sentence_length = max_sentence_length,\n",
        "      dictionary_length = dictionary_length,\n",
        "      padding_idx = setting[\"tokenized_setting\"][\"pad_id\"],\n",
        "      encoder_embedding_dimension = setting[\"model\"][\"encoder_embedding_dimension\"],\n",
        "      decoder_embedding_dimension = setting[\"model\"][\"decoder_embedding_dimension\"],\n",
        "      feedforward_dimension = setting[\"model\"][\"feedforward_dimension\"],\n",
        "      num_heads = setting[\"model\"][\"num_heads\"],\n",
        "      dropout_p = setting[\"model\"][\"dropout_p\"],\n",
        "      layer_num = setting[\"model\"][\"layer_num\"])\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  checkpoint = torch.load(setting[\"training_hparas\"][\"model_temporary_saving_path\"])\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  model.to(device)\n",
        "  # using model to translation\n",
        "  model.eval()\n",
        "\n",
        "  beam_num = setting[\"training_hparas\"][\"beam_num\"]\n",
        "\n",
        "  outputs_in_word = infer(\n",
        "                device = device,\n",
        "                model = model,\n",
        "                batch_size_setting = infer_batch_size_setting,\n",
        "                infer_loader = infer_loader,\n",
        "                beam_num = beam_num,\n",
        "                max_sentence_length = max_sentence_length,\n",
        "                dictionary_length = dictionary_length,\n",
        "                bos_id = bos_id,\n",
        "                eos_id = eos_id,\n",
        "                pad_id = pad_id)\n",
        "  outputs_in_word = tgt_tokenizer.decode(outputs_in_word)\n",
        "\n",
        "  # print and save output\n",
        "  outputs = [(list_of_sentences[i],outputs_in_word[i]) for i in range(len(outputs_in_word))]\n",
        "  print(outputs)\n",
        "  save_infer_path = setting[\"inference_out_path\"]\n",
        "  with open(path_doc + save_infer_path, 'w') as out_f:\n",
        "      for index,line_pair in enumerate(tqdm(outputs)):\n",
        "        infer_dict = {\"index\":index,\"original\":line_pair[0],\"translation\":line_pair[1]}\n",
        "        json.dump(infer_dict, out_f)"
      ],
      "metadata": {
        "id": "8ErevRWLp_FR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_main(setting,[\"Hello, world!\",\"this is a pen\"])"
      ],
      "metadata": {
        "id": "A_uu2yjzHnZO",
        "outputId": "bc1b8990-dc20-45df-e5bf-29d3bf13d21f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "infer_step: 100%|██████████| 1/1 [00:08<00:00,  8.56s/ step]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello, world!', '!!!!!!!!!!!!!的!,,,你!!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————'), ('this is a pen', ',,。。。。的子,,,,,,,子,,著,,,,子,,,,子,,,,子,,,,子,,子,,,,,,子,,,,,,,,,子,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 10824.01it/s]\n"
          ]
        }
      ]
    }
  ]
}