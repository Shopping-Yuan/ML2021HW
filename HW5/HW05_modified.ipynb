{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW5/HW05_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "###Part 0 setting and installing package\n",
        "------\n",
        "###Part 1 preparing data set\n",
        "------\n",
        "######load data file\n",
        "######clean data\n",
        "######pick up line pairs\n",
        "######tokenize : using sentencepiece\n",
        "######make data set\n",
        "------\n",
        "###Part 2 make model\n",
        "------\n",
        "######positional encoding layer\n",
        "######multihead attention layer\n",
        "######encoder layer(s)\n",
        "######decoder layer(s)\n",
        "######transformer layer\n",
        "------\n",
        "###Part 3 training and validation process\n",
        "------\n",
        "######Noam optimizer\n",
        "######label smoothing\n",
        "######beam search\n",
        "######bleu\n",
        "######training and validation function\n",
        "######main function\n",
        "------\n"
      ],
      "metadata": {
        "id": "WTv4XN2qB_fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "setting\n",
        "======\n",
        ">Here are all parameters using in this project."
      ],
      "metadata": {
        "id": "iVQ2D_mLcx4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "setting = {\n",
        "# information of the path of dataset\n",
        "\"data_info\" : {\n",
        "    \"document\":\"/content\",\n",
        "    \"raw_file_name\":\"/ted2020.tgz\",\n",
        "    \"unzip_path\":\"/train_dev/\",\n",
        "    \"source\":{\n",
        "        \"lang\":\"en\",\n",
        "        \"raw_data_path\":\"/train_dev/raw.en\",\n",
        "        \"clean_data_path\":\"/train_dev/clean_en.txt\",\n",
        "        \"tokenized_train_data\":\"/train_dev/tokenized_train_data_en.txt\",\n",
        "        \"tokenized_val_data\":\"/train_dev/tokenized_val_data_en.txt\"\n",
        "        },\n",
        "    \"target\":{\n",
        "        \"lang\":\"zh\",\n",
        "        \"raw_data_path\":\"/train_dev/raw.zh\",\n",
        "        \"clean_data_path\":\"/train_dev/clean_zh.txt\",\n",
        "        \"tokenized_train_data\":\"/train_dev/tokenized_train_data_zh.txt\",\n",
        "        \"tokenized_val_data\":\"/train_dev/tokenized_val_data_zh.txt\"\n",
        "        }\n",
        "},\n",
        "# tokenized setting for spm\n",
        "\"tokenized_setting\" : {\n",
        "    \"vocab_size\" : 8000,\n",
        "    \"character_coverage\" : 1,\n",
        "    \"model_type\" : \"bpe\", # \"unigram\",\n",
        "    \"input_sentence_size\" : 400000,\n",
        "    \"shuffle_input_sentence\" : True,\n",
        "    \"normalization_rule_name\" : \"nmt_nfkc_cf\",\n",
        "    \"pad_id\":0,\n",
        "    \"unk_id\":1,\n",
        "    \"bos_id\":2,\n",
        "    \"eos_id\":3,\n",
        "    \"max_l\":400\n",
        "},\n",
        "# model structure setting\n",
        "\"model\" : {\n",
        "      \"encoder_embedding_dimension\" : 256,\n",
        "      \"decoder_embedding_dimension\" : 256,\n",
        "      \"feedforward_dimension\" : 2048,\n",
        "      \"num_heads\" : 2,\n",
        "      \"dropout_p\" : 0.0,\n",
        "      \"layer_num\" : 6\n",
        "},\n",
        "\n",
        "# setting in training and validation process ,\n",
        "# including optimization setting.\n",
        "\"training_hparas\" : {\n",
        "    \"total_step\" : 40000,\n",
        "    \"do_valid_step\" : 4000,\n",
        "    \"early_stop_step\" : 2,\n",
        "    \"train_batch_size\" : 40,\n",
        "    \"valid_batch_size\" : 100,\n",
        "    \"workers\" : 0,\n",
        "    \"label_smoothing\" : 0.1,\n",
        "    \"beam_num\" : 2,\n",
        "    \"optimization\":{\n",
        "        \"factor\" : 2,\n",
        "        \"warmup\"  : 4000,\n",
        "        \"optimizer\" : {\n",
        "                \"lr\" : 0,\n",
        "                \"betas\" : (0.9, 0.98),\n",
        "                \"eps\" : 1e-9,\n",
        "                \"weight_decay\" : 0.0001\n",
        "                }\n",
        "            },\n",
        "    \"model_saving_path\" : \"/content/model.pth\"\n",
        "}\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "-WoR-01STMor"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "installing package\n",
        "------"
      ],
      "metadata": {
        "id": "cdURO12Sntrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# used in part 1\n",
        "!pip install sentencepiece\n",
        "# used in part 1 and 3\n",
        "!pip install tqdm\n",
        "# used in part 2\n",
        "!pip install torchinfo\n",
        "# used in part 3\n",
        "!pip install torcheval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ti-UGuHNhh",
        "outputId": "a8c079d0-eaea-4ebf-c7b3-087f3046e4cf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing data set\n",
        "=============\n",
        "\n",
        "load data file\n",
        "-------------\n",
        ">Here I load dataset from my drive,  \n",
        ">but it also can be download from the link below."
      ],
      "metadata": {
        "id": "mMkT3K4JXs60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 : download dataset from drive to google colab\n",
        "# original dataset is in \"https://mega.nz/#!vEcTCISJ!3Rw0eHTZWPpdHBTbQEqBDikDEdFPr7fI8WxaXK9yZ9U\"\n",
        "\n",
        "path_doc = setting[\"data_info\"][\"document\"]\n",
        "rawdata_file_name = setting[\"data_info\"][\"raw_file_name\"]\n",
        "rawdata_file_path = path_doc + rawdata_file_name\n",
        "unzip_path = path_doc + setting[\"data_info\"][\"unzip_path\"]\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive_path = path_doc + \"/drive\"\n",
        "drive_name = \"/MyDrive\"\n",
        "drive.mount(drive_path)\n",
        "\n",
        "# copy file from drive\n",
        "import shutil\n",
        "shutil.copyfile(drive_path + drive_name + rawdata_file_name, rawdata_file_path)\n",
        "\n",
        "# step 2 : unzip dataset\n",
        "import tarfile\n",
        "# open file\n",
        "file = tarfile.open(rawdata_file_path)\n",
        "# extracting file\n",
        "file.extractall(unzip_path)\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X23RaTWe9hoU",
        "outputId": "617a6169-bcd6-4362-ed74-348faaa5a662"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean data\n",
        "------\n",
        ">First each dataset (source or target) is clean  \n",
        ">seperately, change to halfwidth and remove/replace  \n",
        ">some kind of punctuations.\n",
        "\n",
        ">Also because the number of sentences in one line may be  \n",
        ">different in line pairs of source and target set (its an error),  \n",
        ">some special punctuations is add to the end of sentences  \n",
        ">for the next process dealing with these problem by  \n",
        ">using sentence pairs instead of lines pairs to form datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "-y54N2UNimor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "# convert fullwidth to halfwidth\n",
        "def to_halfwidth(string):\n",
        "  return \"\".join(unicodedata.normalize('NFKC',letter) for letter in string)\n",
        "def clean_s_zh(s):\n",
        "    s = to_halfwidth(s)\n",
        "    # step 1 : delete — _\n",
        "    delete = \" _()[]\"\n",
        "    delete_rules = s.maketrans(\"\",\"\",delete)\n",
        "    s = s.translate(delete_rules)\n",
        "\n",
        "    # step 2 : replace “” with \"\"\n",
        "    to_be_replace = '“”'\n",
        "    replace = '\"\"'\n",
        "    replace_dict = dict(zip(to_be_replace,replace))\n",
        "\n",
        "    # step 3 : add **END** before and after punctuation\n",
        "\n",
        "    \"\"\"\n",
        "    The number of sentences in one line may be different\n",
        "    in line pairs of source and target set.\n",
        "    \"**END**\" is add after \"。!?\" and \".!?\", which can be used\n",
        "    to check if the number of sentence in the pair are equal\n",
        "    in the next process.\n",
        "    also in english, \".\" may be use in abbreviation,\n",
        "    these different use must be identified.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    punctuation = \"。!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    zh_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return zh_list\n",
        "\n",
        "def clean_s_en(s):\n",
        "    s = to_halfwidth(s)\n",
        "\n",
        "    replace_dict = {}\n",
        "\n",
        "    delete = \"-()[]\"\n",
        "    for char in delete:\n",
        "      replace_dict[char] = \"\"\n",
        "\n",
        "    punctuation = \"!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    # Identify if \".\" is used in abbreviation,\n",
        "    # if not, add \"**END**\" after it.\n",
        "    pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "    s = pattern.sub(\"**END**\",s)\n",
        "\n",
        "    # test pattern\n",
        "    # pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "    # result = pattern.sub(\"**END**\",\"There are many people in U.S. w.r.t. in Taiwan.Thank you.\")\n",
        "\n",
        "    en_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return en_list"
      ],
      "metadata": {
        "id": "5dwWeVz-sPI2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pick up line pairs\n",
        "------\n",
        ">pick up line pairs has equal number of sentences and  \n",
        ">split them into sentences to form sourse/target dataset.  \n",
        ">Remove sentences with too many words for training and validation."
      ],
      "metadata": {
        "id": "M2EfnpIXV91l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using \"**END**\" to split line pairs to check if they have equal sentence\n",
        "def divide_by_END(s):\n",
        "    list_s = []\n",
        "    for line_string in s.strip(\"**END**\").split(\"**END**\"):\n",
        "      if line_string not in [\"\",\" \"]:\n",
        "         list_s.append(line_string)\n",
        "    return(list_s)\n",
        "'''\n",
        "warning : devide_en_again function is apply just beacause\n",
        "in \"this\" dataset english sentences end with \":\" or \";\"\n",
        "sometimes not splited well.\n",
        "If the dataset is change, this part may need to be\n",
        "eliminated or modified.\n",
        "'''\n",
        "def devide_en_again(s,punctuation = \":;\"):\n",
        "    replace_dict = {}\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules_src = s.maketrans(replace_dict)\n",
        "    new_s = divide_by_END(s.translate(replace_rules_src))\n",
        "    return new_s\n",
        "\n",
        "# remove \"sentence\" if it is too long.\n",
        "def remove_too_long(src_list,tgt_list,threshold):\n",
        "    too_long_src = 0\n",
        "    too_long_tgt = 0\n",
        "    remove = False\n",
        "    new_s = []\n",
        "    new_t = []\n",
        "    for i in range(len(src_list)):\n",
        "      if ((len(src_list[i])>threshold)):\n",
        "        remove = True\n",
        "        too_long_src += 1\n",
        "      if (len(tgt_list[i])>threshold):\n",
        "        remove = True\n",
        "        too_long_tgt += 1\n",
        "      if remove == False:\n",
        "        new_s.append(src_list[i])\n",
        "        new_t.append(tgt_list[i])\n",
        "      else :\n",
        "        remove = False\n",
        "    return(new_s,new_t,too_long_src,too_long_tgt)\n",
        "\n",
        "# pick up good line pairs for traning and validation model\n",
        "def check_data_pairs(src_list,tgt_list,threshold):\n",
        "    index = 0\n",
        "    new_src_list = []\n",
        "    new_tgt_list = []\n",
        "\n",
        "    same = 0\n",
        "    add_next = 0\n",
        "    split_again = 0\n",
        "    not_use = 0\n",
        "\n",
        "    while(index < len(src_list)):\n",
        "\n",
        "      src = divide_by_END(src_list[index])\n",
        "      tgt = divide_by_END(tgt_list[index])\n",
        "\n",
        "      # case 1 : src is as long as tgt , finished.\n",
        "      if len(src) == len(tgt):\n",
        "        new_src_list += src\n",
        "        new_tgt_list += tgt\n",
        "        same += 1\n",
        "        index += 1\n",
        "\n",
        "      else :\n",
        "        # if it is not the last one : both src and tgt add next sentence\n",
        "        if index != len(src_list)-1:\n",
        "          src_add_next = divide_by_END(src_list[index] + src_list[index+1])\n",
        "          tgt_add_next = divide_by_END(tgt_list[index] + tgt_list[index+1])\n",
        "          # case 2 : src_add_next is as long as tgt_add_next , finished.\n",
        "          if len(src_add_next) == len(tgt_add_next):\n",
        "            new_src_list += src_add_next\n",
        "            new_tgt_list += tgt_add_next\n",
        "            add_next += 2\n",
        "            index += 2\n",
        "\n",
        "          # using new punctuation to divide tgt (english) sentence.\n",
        "          # note that this part could cause negative effects if the dataset is change.\n",
        "          else :\n",
        "            src_add_next = devide_en_again(src_list[index] + src_list[index+1])\n",
        "            # case 3 : src_add_next is as long as tgt_add_next , finished.\n",
        "            if len(src_add_next) == len(tgt_add_next):\n",
        "              new_src_list += src_add_next\n",
        "              new_tgt_list += tgt_add_next\n",
        "              split_again +=2\n",
        "              index += 2\n",
        "\n",
        "            # case 4 : sentence will not be used.\n",
        "            else :\n",
        "              not_use += 1\n",
        "              # if to_do == 1 :\n",
        "              #   print(index,src_add_next,tgt_add_next,len(src_add_next),len(tgt_add_next))\n",
        "              index += 1\n",
        "\n",
        "        # if it is the last one\n",
        "        else :\n",
        "          not_use += 1\n",
        "          index += 1\n",
        "    # print information\n",
        "    print(f\"The original total number of line is {index}.\")\n",
        "    print(f\"The number of line pairs have the equal sentences is {same}.\")\n",
        "    print(f\"The number of line pairs have the equal sentences after combine the next lines is {add_next}.\")\n",
        "    print(f\"The number of line pairs have the equal sentences after combine the next lines\"+\\\n",
        "       f\"and resplit english lines using :; is {split_again}.\")\n",
        "    print(f\"The number of line we don't use is {not_use}.\")\n",
        "    print(f\"Note that {index} = {same}+{add_next}+{split_again}+{not_use}.\")\n",
        "\n",
        "    # remove long lines\n",
        "    print(f\"The total number of sentence pairs before remove long sentences is {len(new_src_list)}.\")\n",
        "    new_src_list,new_tgt_list,too_long_src,too_long_tgt = remove_too_long(new_src_list,new_tgt_list,threshold)\n",
        "    print(f\"The finally total number of sentence pairs using is {len(new_src_list)}.\")\n",
        "    print(f\"Note that {len(new_src_list)} are the number of sentence pairs, not line pairs\")\n",
        "\n",
        "    return(new_src_list,new_tgt_list)"
      ],
      "metadata": {
        "id": "UVYvYCpuDQDa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load and clean data\n",
        "------"
      ],
      "metadata": {
        "id": "1KPMwJwck437"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load and clean data\n",
        "def load_file(path,function):\n",
        "    with open(path, \"r\") as f:\n",
        "      data = f.read()\n",
        "      return function(data)\n",
        "# saving to new path\n",
        "def clean_data_and_save(\n",
        "    path_doc,raw_src_path,raw_tgt_path,\n",
        "    clean_src_path,clean_tgt_path,threshold\n",
        "    ):\n",
        "    raw_src_path = path_doc + raw_src_path\n",
        "    raw_tgt_path = path_doc + raw_tgt_path\n",
        "    src = load_file(raw_src_path,clean_s_en),\n",
        "    tgt = load_file(raw_tgt_path,clean_s_zh),\n",
        "    # src , tgt are tuples with only one term : src_list, tgt_list\n",
        "    src_list = src[0]\n",
        "    tgt_list = tgt[0]\n",
        "    clean_src_list, clean_tgt_list = check_data_pairs(src_list,tgt_list,threshold)\n",
        "    with open(path_doc + clean_src_path, \"w\") as f:\n",
        "      f.write(\"\\n\".join(clean_src_list))\n",
        "    with open(path_doc + clean_tgt_path, \"w\") as f:\n",
        "      f.write(\"\\n\".join(clean_tgt_list))\n",
        "# test clean_data_and_save\n",
        "# clean_data_and_save(\n",
        "#     path_doc = setting[\"data_info\"][\"document\"],\n",
        "#     raw_src_path = setting[\"data_info\"][\"source\"][\"raw_data_path\"],\n",
        "#     raw_tgt_path = setting[\"data_info\"][\"target\"][\"raw_data_path\"],\n",
        "#     clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "#     clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"],\n",
        "#     threshold = setting[\"tokenized_setting\"][\"max_l\"]\n",
        "# )"
      ],
      "metadata": {
        "id": "DhVI2xylk1tr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenize\n",
        "------\n",
        ">using sentencepiece to tokenize sentences,  \n",
        ">first make the english/chinese dictionary separately,  \n",
        ">then use these dict to encode sentence pair in dataset,  \n",
        ">including add bos/eos/padding to tokenized sentences.  \n",
        ">Finally split then into train/val set and save."
      ],
      "metadata": {
        "id": "S_-APGe5okQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.utils.data as data\n",
        "def tokenized(clean_data_path,\n",
        "       vocab_size,\n",
        "       lang,\n",
        "       tokenized_setting\n",
        "       ):\n",
        "  model_prefix = f\"spm_{vocab_size}_{lang}\"\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      input=clean_data_path,\n",
        "      **tokenized_setting,\n",
        "      model_prefix=model_prefix,\n",
        "  )\n",
        "  return(model_prefix)\n",
        "\n",
        "def get_tokenizers(path_doc,vocab_size,src_lang,tgt_lang):\n",
        "  src_tokenizer = spm.SentencePieceProcessor(model_file = path_doc + f\"/spm_{vocab_size}_{src_lang}\" +\".model\")\n",
        "  tgt_tokenizer = spm.SentencePieceProcessor(model_file = path_doc + f\"/spm_{vocab_size}_{tgt_lang}\" +\".model\")\n",
        "  return src_tokenizer,tgt_tokenizer\n",
        "\n",
        "def bos_eos_padding(dataset,\n",
        "          max_l,\n",
        "          src_tokenizer,\n",
        "          tgt_tokenizer\n",
        "          ):\n",
        "\n",
        "\n",
        "  padding_src = []\n",
        "  padding_tgt = []\n",
        "  len_s = 0\n",
        "  len_t = 0\n",
        "  for src,tgt in dataset:\n",
        "    s = src_tokenizer.encode(src, out_type=int)\n",
        "    s = np.append(s,[3])\n",
        "    s = np.append([2],np.pad(s,(0, max_l-len(s)-1), constant_values = 0))\n",
        "    padding_src.append(s)\n",
        "\n",
        "    t = tgt_tokenizer.encode(tgt, out_type=int)\n",
        "    t = np.append(t,[3])\n",
        "    t = np.append([2],np.pad(t,(0, max_l-len(t)-1), constant_values = 0))\n",
        "    padding_tgt.append(t)\n",
        "\n",
        "  return(list(zip(padding_src,padding_tgt)))\n",
        "# test SentencePieceProcessor and bos_eos_padding\n",
        "# s_src = spm.SentencePieceProcessor(model_file=\"/content/spm8000_en.model\")\n",
        "# s_src.encode(\"hello world!\", out_type=int)\n",
        "# bos_eos_padding([(\"hello world\",\"_哈囉\")],5,10)\n",
        "\n",
        "def data_set_preparing(path_doc,\n",
        "            clean_src_path,\n",
        "            clean_tgt_path,\n",
        "            max_l,\n",
        "            src_tokenizer,\n",
        "            tgt_tokenizer,\n",
        "            st_train_path,\n",
        "            st_val_path,\n",
        "            tt_train_path,\n",
        "            tt_val_path,\n",
        "            ):\n",
        "    src_set = []\n",
        "    tgt_set = []\n",
        "\n",
        "    with open(path_doc+clean_src_path,\"r\") as in_f :\n",
        "      for line in tqdm(in_f):\n",
        "        src_set.append(line)\n",
        "    with open(path_doc+clean_tgt_path,\"r\") as in_f :\n",
        "      for line in tqdm(in_f):\n",
        "        tgt_set.append(line)\n",
        "\n",
        "    dataset = list(zip(src_set,tgt_set))\n",
        "    dataset = bos_eos_padding(dataset,max_l,src_tokenizer,tgt_tokenizer)\n",
        "    train_set, valid_set = data.random_split(dataset,[0.99,0.01])\n",
        "    # print(train_set[0][0])\n",
        "\n",
        "    with open(path_doc + st_train_path, 'w') as out_f:\n",
        "      for line_pair in tqdm(train_set):\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[0])+\"\\n\")\n",
        "    with open(path_doc + st_val_path, 'w') as out_f:\n",
        "      for line_pair in tqdm(valid_set):\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[0])+\"\\n\")\n",
        "    with open(path_doc + tt_train_path, 'w') as out_f:\n",
        "      for line_pair in tqdm(train_set):\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[1])+\"\\n\")\n",
        "    with open(path_doc + tt_val_path, 'w') as out_f:\n",
        "      for line_pair in tqdm(valid_set):\n",
        "        out_f.write(\" \".join(str(x) for x in line_pair[1])+\"\\n\")"
      ],
      "metadata": {
        "id": "ByrUmAvFkKk9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenized_data(vocab_size,tokenized_setting,max_l,path_doc,clean_src_path,\n",
        "          clean_tgt_path,src_lang,tgt_lang,st_train_path,st_val_path,\n",
        "          tt_train_path,tt_val_path,):\n",
        "  tokenized(path_doc + clean_src_path,vocab_size,src_lang,tokenized_setting)\n",
        "  tokenized(path_doc + clean_tgt_path,vocab_size,tgt_lang,tokenized_setting)\n",
        "  src_tokenizer,tgt_tokenizer = get_tokenizers(path_doc,vocab_size,src_lang,tgt_lang)\n",
        "  data_set_preparing(path_doc,clean_src_path,clean_tgt_path,max_l,src_tokenizer,\n",
        "           tgt_tokenizer,st_train_path,st_val_path,tt_train_path,tt_val_path)\n",
        "  return src_tokenizer,tgt_tokenizer\n",
        "\n",
        "# test tokenized_data()\n",
        "# src_tokenizer,tgt_tokenizer = tokenized_data(\n",
        "#     vocab_size = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "#     tokenized_setting = {k:setting[\"tokenized_setting\"][k] for k in \\\n",
        "#               set(list(setting[\"tokenized_setting\"].keys()))-{\"vocab_size\",\"max_l\"}},\n",
        "#     max_l = setting[\"tokenized_setting\"][\"max_l\"],\n",
        "#     path_doc = setting[\"data_info\"][\"document\"],\n",
        "#     clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "#     clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"],\n",
        "#     src_lang = setting[\"data_info\"][\"source\"][\"lang\"],\n",
        "#     tgt_lang = setting[\"data_info\"][\"target\"][\"lang\"],\n",
        "#     st_train_path = setting[\"data_info\"][\"source\"][\"tokenized_train_data\"],\n",
        "#     st_val_path = setting[\"data_info\"][\"source\"][\"tokenized_val_data\"],\n",
        "#     tt_train_path = setting[\"data_info\"][\"target\"][\"tokenized_train_data\"],\n",
        "#     tt_val_path = setting[\"data_info\"][\"target\"][\"tokenized_val_data\"])"
      ],
      "metadata": {
        "id": "RkIHpc6qkRqh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make data set\n",
        "------\n",
        "> Using tokenized data to make dataset.  \n",
        "> Classmethod : padding_mask_batch which  \n",
        "> where the key padding mask is constucted  \n",
        "> also defined here."
      ],
      "metadata": {
        "id": "FV3kHGqlggdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class myDataset(Dataset):\n",
        "  def __init__(self,src_path,tgt_path):\n",
        "\n",
        "    self.src_path = src_path\n",
        "    self.tgt_path = tgt_path\n",
        "\n",
        "    src_list = []\n",
        "    with open(self.src_path,\"r\") as f :\n",
        "      d_l = f.readlines()\n",
        "      for line in tqdm(d_l):\n",
        "        int_list = [int(i) for i in line.split()]\n",
        "        src_list.append(int_list)\n",
        "    self.src = torch.LongTensor(src_list)\n",
        "\n",
        "    tgt_list = []\n",
        "    with open(self.tgt_path,\"r\") as f :\n",
        "      l_l = f.readlines()\n",
        "      for line in tqdm(l_l):\n",
        "        int_list = [int(i) for i in line.split()]\n",
        "        tgt_list.append(int_list)\n",
        "    self.tgt = torch.LongTensor(tgt_list)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.src)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.src[index], self.tgt[index]\n",
        "\n",
        "  # make key padding mask\n",
        "  @classmethod\n",
        "  def padding_mask_batch(cls,batch,pad_id):\n",
        "    \"\"\"Collate a batch of data.\"\"\"\n",
        "    src, tgt = zip(*batch)\n",
        "    src = torch.stack(src)\n",
        "    tgt = torch.stack(tgt)\n",
        "    src_padding = (src == pad_id)\n",
        "    tgt_padding = (tgt == pad_id)\n",
        "\n",
        "    return src, tgt , src_padding, tgt_padding\n",
        "# test myDataset\n",
        "# data = []\n",
        "# with open(\"/content/train_dev/tokenized_train_data_en.txt\",\"r\") as f :\n",
        "#   d_l = f.readlines()\n",
        "#   for line in tqdm(d_l):\n",
        "#     int_list = [int(i) for i in line.split()]\n",
        "#     data.append(int_list)\n",
        "# print(data[0])"
      ],
      "metadata": {
        "id": "dbfi-DvrlDhI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "def get_data_set(train_batch_size,valid_batch_size,num_workers,path_doc,\n",
        "         st_train_path,st_val_path,tt_train_path,tt_val_path,pad_id):\n",
        "\n",
        "  train_set = myDataset(src_path = path_doc + st_train_path,\n",
        "              tgt_path = path_doc + tt_train_path,\n",
        "              )\n",
        "  valid_set = myDataset(src_path = path_doc + st_val_path,\n",
        "              tgt_path = path_doc + tt_val_path,\n",
        "              )\n",
        "  train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size = train_batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    collate_fn = lambda x : myDataset.padding_mask_batch(x,\n",
        "                   pad_id = pad_id)\n",
        "  )\n",
        "  valid_loader = DataLoader(\n",
        "    valid_set,\n",
        "    batch_size = valid_batch_size,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    collate_fn = lambda x : myDataset.padding_mask_batch(x,\n",
        "                   pad_id = pad_id)\n",
        "  )\n",
        "  del train_set,valid_set\n",
        "  gc.collect()\n",
        "  return train_loader,valid_loader\n",
        "# test get_data_set()\n",
        "# train_set,valid_set = get_data_set()\n",
        "# batch = next(iter(valid_set))\n",
        "# src,tgt,src_mask,tgt_mask = batch\n",
        "# print(src.shape)"
      ],
      "metadata": {
        "id": "YBFKMSM45ppM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make model\n",
        "======\n",
        "positional encoding layer\n",
        "------\n",
        ">The first layer is embedding layer, where each integers  \n",
        ">in encoder sentence will be represent by a vector.   \n",
        ">I use build-in class in pytorch to finish these part,    \n",
        ">and combine it with encoder layers to form my encoder.\n",
        "\n",
        ">The layer below is the second layer :positional encoding layer  \n",
        ">in this layer the position infomation is add to each \"word\"  \n",
        ">in the sentence.\n",
        ">Here I use parameters instead of constant as  \n",
        ">position infomation so they will change during training process."
      ],
      "metadata": {
        "id": "RZAzqY3bwds8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Positional_Encoding(nn.Module):\n",
        "    def __init__(self,max_sentence_length,embedding_dimension):\n",
        "      super().__init__()\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.encoding_values = nn.Parameter(nn.init.normal_(torch.empty(max_sentence_length,1, embedding_dimension)))\n",
        "    def forward(self, x):\n",
        "        # the shape of x : [batch,length,e_dim]\n",
        "        # the shape of self.encoding_values : [batch,length,e_dim]\n",
        "        x = x + self.encoding_values.unsqueeze(0)\n",
        "        x = x.squeeze(-2)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "EoKH9m1LznWO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "multihead attention layer\n",
        "------\n"
      ],
      "metadata": {
        "id": "OURB2Fg-4lE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import math\n",
        "from torchinfo import summary\n",
        "# This part is modify from pytorch : torch.nn.functional.scaled_dot_product_attention\n",
        "# Efficient implementation equivalent to the following:\n",
        "class Scaled_Dot_Product_Attention(nn.Module):\n",
        "    def __init__(self,max_sentence_length,dropout_p):\n",
        "      super().__init__()\n",
        "      self.dropout_p = dropout_p\n",
        "      self.max_l = max_sentence_length\n",
        "      attn_bias = torch.zeros(self.max_l, self.max_l)\n",
        "      temp_mask = torch.ones(self.max_l, self.max_l, dtype=torch.bool).tril(diagonal=0)\n",
        "      attn_bias = attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
        "      self.register_buffer(\"attn_bias\",attn_bias)\n",
        "\n",
        "    def forward(self, is_last_batch, query, key, value, padding_mask=None, is_causal=False, scale=None) -> torch.Tensor:\n",
        "      # Efficient implementation equivalent to the following:\n",
        "\n",
        "      scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
        "      attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
        "\n",
        "      if is_causal:\n",
        "        if is_last_batch:\n",
        "          self.attn_bias = self.attn_bias[:query.size(-2),:query.size(-2)]\n",
        "        self.attn_bias.to(query.dtype)\n",
        "        attn_weight += self.attn_bias\n",
        "\n",
        "      if padding_mask is not None:\n",
        "          if padding_mask.dtype == torch.bool:\n",
        "            padding_mask = torch.zeros_like(padding_mask,dtype = float).masked_fill_(padding_mask, (float(\"-inf\")))\n",
        "\n",
        "          padding_mask = padding_mask.unsqueeze(0).unsqueeze(0)\n",
        "          padding_mask.to(query.dtype)\n",
        "\n",
        "          attn_weight = attn_weight.transpose(-4,-2)\n",
        "          attn_weight += padding_mask\n",
        "          attn_weight = attn_weight.transpose(-4,-2)\n",
        "\n",
        "      attn_weight = torch.softmax(attn_weight, dim=-1)\n",
        "      attn_weight = torch.dropout(attn_weight, self.dropout_p, train=True)\n",
        "      return attn_weight @ value\n",
        "# test scaled_dot_product_attention\n",
        "# t = torch.rand([2,3,4,5])\n",
        "# mask = torch.tensor([[False,False,True,True],[False,True,False,True]],dtype = torch.bool)\n",
        "# print(scaled_dot_product_attention(\"cpu\",t,t,t,padding_mask= mask, is_causal=True))\n",
        "# from torch.nn.functional import scaled_dot_product_attention\n",
        "class My_MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, max_sentence_length, kv_input_dimension, embedding_dimension, num_heads, dropout_p, if_decoder = False):\n",
        "        '''\n",
        "        embedding_dimension = input dimension\n",
        "        note that there are residual sublayers in MultiHeadedAttention\n",
        "        '''\n",
        "        super().__init__()\n",
        "        assert embedding_dimension % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.max_l = max_sentence_length\n",
        "        self.kv_d = kv_input_dimension\n",
        "        self.d = embedding_dimension\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_p = dropout_p\n",
        "        self.is_causal = if_decoder\n",
        "        self.sdpa = Scaled_Dot_Product_Attention(self.max_l,self.dropout_p)\n",
        "        self.linear_for_q = nn.Linear(self.d, self.d)\n",
        "        self.linear_for_kv = nn.Linear(self.kv_d, 2 * self.d)\n",
        "        self.linear_out_project = nn.Linear(self.d, self.d)\n",
        "\n",
        "    def forward(self, is_last_batch, q_input_data, kv_input_data , padding_mask = None):\n",
        "\n",
        "        query = self.linear_for_q(q_input_data)\n",
        "        key, value = self.linear_for_kv(kv_input_data).split(self.d,dim = -1)\n",
        "\n",
        "        query,key,value = \\\n",
        "          map(lambda x : x.view(x.size(0),x.size(1),self.num_heads,self.d//self.num_heads),[query,key,value])\n",
        "        query,key,value = \\\n",
        "          map(lambda x : x.transpose(-2,-3),[query,key,value])\n",
        "\n",
        "        x = self.sdpa(is_last_batch,query,key,value,padding_mask = padding_mask,is_causal = self.is_causal)\n",
        "        x = x.transpose(-2,-3).contiguous()\n",
        "        x = x.view(x.size(0),x.size(1),self.d)\n",
        "        x = self.linear_out_project(x)\n",
        "\n",
        "        return x\n",
        "# test My_MultiHeadedAttention\n",
        "# model = My_MultiHeadedAttention(64,128,2,0.0)\n",
        "# q_input = torch.rand(32,400,128)\n",
        "# kv_input = torch.rand(32,400,64)\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(q_input,kv_input,mask).size())\n",
        "# print(summary(model,device = \"cpu\",q_input_data = q_input, kv_input_data = kv_input,padding_mask = mask))"
      ],
      "metadata": {
        "id": "PLWLkr9UaKFD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "encoder layer(s)\n",
        "------"
      ],
      "metadata": {
        "id": "T0IjI-zC5fqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class My_Encoder_Layer(nn.Module):\n",
        "  def __init__(self,max_sentence_length,embedding_dimension,feedforward_dimension,num_heads,dropout_p):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "\n",
        "    self.attention = My_MultiHeadedAttention(self.max_l, self.emb_dim, self.emb_dim, self.num_heads, self.dropout_p)\n",
        "    self.layer_norm_attn = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_attn_layernorm = nn.Dropout(self.dropout_p)\n",
        "\n",
        "    self.feedforward = nn.Sequential(\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    )\n",
        "    self.layer_norm_feedforward = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_feedforward_layernorm = nn.Dropout(self.dropout_p)\n",
        "\n",
        "\n",
        "  def forward(self,is_last_batch,x,padding_mask):\n",
        "    x = x + self.attention(is_last_batch,x,x,padding_mask)\n",
        "    x = self.layer_norm_attn(x)\n",
        "\n",
        "    x = self.drop_out_attn_layernorm(x)\n",
        "\n",
        "    x = x + self.feedforward(x)\n",
        "    x = self.layer_norm_feedforward(x)\n",
        "    x = self.drop_out_feedforward_layernorm(x)\n",
        "\n",
        "    return x\n",
        "# test My_Encoder_Layer\n",
        "# model = My_Encoder_Layer(\"cpu\",128,256,2,0.0)\n",
        "# input = torch.rand((32,400,128))\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(input,mask).size())\n",
        "# print(summary(model,input_data = input,padding_mask = mask))\n",
        "# print(model.state_dict().keys())\n",
        "class My_Encoder(nn.Module):\n",
        "  def __init__(self,max_sentence_length,dictionary_length,embedding_dimension,feedforward_dimension,\n",
        "         padding_idx, num_heads, dropout_p, layer_num):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.dict_l = dictionary_length\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.padding_idx = padding_idx\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "    self.layer_num = layer_num\n",
        "\n",
        "    self.encoder_embedding = nn.Embedding(self.dict_l,self.emb_dim,self.padding_idx)\n",
        "    self.positional_encoding = Positional_Encoding(self.max_l,self.emb_dim)\n",
        "    self.encoder = nn.ModuleList([My_Encoder_Layer(self.max_l,self.emb_dim,self.fwd_dim,\\\n",
        "                    self.num_heads,self.dropout_p) for i in range(layer_num)])\n",
        "\n",
        "  def forward(self,is_last_batch,input,padding_mask):\n",
        "    x = self.encoder_embedding(input.unsqueeze(-1))* math.sqrt(self.emb_dim)\n",
        "    x = self.positional_encoding(x)\n",
        "\n",
        "    for index,module in enumerate(self.encoder):\n",
        "      if index == 0:\n",
        "        x = module(is_last_batch,x,padding_mask)\n",
        "      else:\n",
        "        x = module(is_last_batch,x,None)\n",
        "    return x\n",
        "# test My_Encoder\n",
        "# model = My_Encoder(\"cpu\",400,8000,128,256,0,2,0.0,2)\n",
        "# input = torch.randint(0,7999,(32,400),dtype = torch.long)\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(input,mask).size())\n",
        "# print(summary(model,input_data = input,padding_mask = mask))\n",
        "# print(model.state_dict().keys())"
      ],
      "metadata": {
        "id": "PYhd7muASnrY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "decoder layer(s)\n",
        "------"
      ],
      "metadata": {
        "id": "4O_rI3QV55Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class My_Decoder_Layer(nn.Module):\n",
        "  def __init__(self,max_sentence_length,encoder_embedding_dimension,embedding_dimension,feedforward_dimension,num_heads,dropout_p):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.encoder_dim = encoder_embedding_dimension\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "\n",
        "    self.self_attention = My_MultiHeadedAttention \\\n",
        "     (self.max_l,self.emb_dim,self.emb_dim, num_heads = self.num_heads,\\\n",
        "     dropout_p = self.dropout_p, if_decoder = True)\n",
        "    self.layer_norm_sa = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_sa = nn.Dropout(0)\n",
        "\n",
        "    self.feedforward_sa = nn.Sequential(\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    )\n",
        "    self.layer_norm_sa_fw = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_sa_fw = nn.Dropout(0)\n",
        "\n",
        "    self.cross_attention = My_MultiHeadedAttention \\\n",
        "    (self.max_l,self.encoder_dim, self.emb_dim, num_heads = self.num_heads,\n",
        "    dropout_p = self.dropout_p, if_decoder = True)\n",
        "    self.layer_norm_ca = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_ca = nn.Dropout(0)\n",
        "\n",
        "    self.feedforward_ca = nn.Sequential(\n",
        "    nn.Linear(self.emb_dim,self.fwd_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(self.fwd_dim,self.emb_dim)\n",
        "    )\n",
        "    self.layer_norm_ca_fw = nn.LayerNorm(self.emb_dim)\n",
        "    self.drop_out_ca_fw = nn.Dropout(0)\n",
        "\n",
        "  def forward(self,is_last_batch,encoder_input,input,padding_mask):\n",
        "\n",
        "    x = input + self.self_attention(is_last_batch,input,input,padding_mask)\n",
        "    x = self.layer_norm_sa(x)\n",
        "    x = self.drop_out_sa(x)\n",
        "\n",
        "    x = x + self.feedforward_sa(x)\n",
        "    x = self.layer_norm_sa_fw(x)\n",
        "    x = self.drop_out_sa_fw(x)\n",
        "\n",
        "    x = x + self.cross_attention(is_last_batch,x,encoder_input,padding_mask)\n",
        "    x = self.layer_norm_ca(x)\n",
        "    x = self.drop_out_ca(x)\n",
        "\n",
        "    x = x + self.feedforward_ca(x)\n",
        "    x = self.layer_norm_ca_fw(x)\n",
        "    x = self.drop_out_ca_fw(x)\n",
        "\n",
        "    return x\n",
        "class My_Decoder(nn.Module):\n",
        "  def __init__(self,max_sentence_length, dictionary_length, encoder_embedding_dimension,\n",
        "         embedding_dimension, feedforward_dimension, padding_idx, num_heads, dropout_p, layer_num):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.dict_l = dictionary_length\n",
        "    self.encoder_dim = encoder_embedding_dimension\n",
        "    self.emb_dim = embedding_dimension\n",
        "    self.fwd_dim = feedforward_dimension\n",
        "    self.padding_idx = padding_idx\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "    self.layer_num = layer_num\n",
        "\n",
        "    self.decoder_embedding = nn.Embedding(self.dict_l,self.emb_dim,padding_idx=self.padding_idx)\n",
        "    self.positional_encoding = Positional_Encoding(self.max_l,self.emb_dim)\n",
        "    self.decoder = nn.ModuleList([My_Decoder_Layer(self.max_l,self.encoder_dim,self.emb_dim,\\\n",
        "                    self.fwd_dim,self.num_heads,self.dropout_p) for i in range(self.layer_num)])\n",
        "    # self.encoder = My_Encoder_Layer(self.emb_dim,self.fwd_dim)\n",
        "\n",
        "    self.generator = nn.Linear(self.emb_dim,self.dict_l)\n",
        "\n",
        "  def forward(self,is_last_batch,encoder_input,input,padding_mask):\n",
        "    x = self.decoder_embedding(input.unsqueeze(-1))* math.sqrt(self.emb_dim)\n",
        "    x = self.positional_encoding(x)\n",
        "    # x = self.encoder(x,padding_mask)\n",
        "    for index,module in enumerate(self.decoder):\n",
        "      if index == 0:\n",
        "        x = module(is_last_batch,encoder_input,x,padding_mask)\n",
        "      else:\n",
        "        x = module(is_last_batch,encoder_input,x,None)\n",
        "    x = self.generator(x)\n",
        "    x = F.log_softmax(x,dim = -1)\n",
        "    return x\n",
        "# test My_Decoder\n",
        "# model = My_Decoder(\"cpu\",400,8000,128,64,256,0,2,0.0,2)\n",
        "# encoder_input = torch.rand(32,400,128)\n",
        "# input = torch.randint(0,7999,(32,400),dtype = torch.long)\n",
        "# mask = (torch.FloatTensor(32,400).uniform_() > 0.8)\n",
        "# print(model(encoder_input = encoder_input,input = input, padding_mask = mask).size())\n",
        "# print(summary(model,encoder_input = encoder_input,input = input, padding_mask = mask))\n",
        "# print(model.state_dict().keys())"
      ],
      "metadata": {
        "id": "rEjaTbhyBmEV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer layer\n",
        "------"
      ],
      "metadata": {
        "id": "1gcz18nz6QTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class My_Transformer(nn.Module):\n",
        "  def __init__(self,max_sentence_length,dictionary_length,padding_idx,\n",
        "         encoder_embedding_dimension,decoder_embedding_dimension,\n",
        "         feedforward_dimension,num_heads,dropout_p,layer_num):\n",
        "    super().__init__()\n",
        "    self.max_l = max_sentence_length\n",
        "    self.dict_l = dictionary_length\n",
        "    self.padding_idx = padding_idx\n",
        "    self.en_dim = encoder_embedding_dimension\n",
        "    self.de_dim = decoder_embedding_dimension\n",
        "    self.fw_dim = feedforward_dimension\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_p = dropout_p\n",
        "    self.layer_num = layer_num\n",
        "    self.encoder = My_Encoder \\\n",
        "     (self.max_l,self.dict_l,self.en_dim,self.fw_dim,\n",
        "      self.padding_idx,self.num_heads,self.dropout_p,self.layer_num)\n",
        "    self.decoder = My_Decoder \\\n",
        "     (self.max_l,self.dict_l,self.en_dim,self.de_dim,self.fw_dim,\n",
        "      self.padding_idx,self.num_heads,self.dropout_p,self.layer_num)\n",
        "\n",
        "  def forward(self,is_last_batch,src,tgt,src_mask,tgt_mask):\n",
        "    memory = self.encoder(is_last_batch,src,src_mask)\n",
        "    outputs = self.decoder(is_last_batch,memory,tgt,tgt_mask)\n",
        "    return outputs\n",
        "\n",
        "def build_model(max_sentence_length,dictionary_length,padding_idx,encoder_embedding_dimension,\n",
        "         decoder_embedding_dimension,feedforward_dimension,num_heads,dropout_p,layer_num):\n",
        "  return My_Transformer(max_sentence_length,dictionary_length,padding_idx,\n",
        "              encoder_embedding_dimension,decoder_embedding_dimension,\n",
        "              feedforward_dimension,num_heads,dropout_p,layer_num)\n",
        "# test My_Transformer\n",
        "# model = My_Transformer(\"cpu\",400,8000,0,128,64,256,2,0,2)\n",
        "# src = torch.randint(0,8000,(32,400),dtype = torch.long)\n",
        "# tgt = torch.randint(0,8000,(32,400),dtype = torch.long)\n",
        "# src_mask = torch.cat(((torch.FloatTensor(32,200).uniform_() > 1),(torch.FloatTensor(32,200).uniform_() > 0.15)),dim =1)\n",
        "# tgt_mask = torch.cat(((torch.FloatTensor(32,100).uniform_() > 1),(torch.FloatTensor(32,300).uniform_() > 0.15)),dim =1)\n",
        "# out = model(src,tgt,src_mask,tgt_mask)\n",
        "# print(out.size(),out.dim(),out[0][0])\n",
        "# print(summary(model,src = src,tgt = tgt,src_mask = src_mask,tgt_mask = tgt_mask))\n",
        "# print(model.state_dict().keys())\n",
        "\n",
        "# test build_model\n",
        "# model = build_model()\n",
        "# batch = next(iter(train_set))\n",
        "# src, tgt, src_mask, tgt_mask = batch\n",
        "# print(type(src),src.shape)\n",
        "# print(summary(model,src = src,tgt = tgt,src_mask = src_mask,tgt_mask = tgt_mask))\n",
        "# outputs = model(src,tgt,src_mask,tgt_mask)\n",
        "# print(outputs.shape)"
      ],
      "metadata": {
        "id": "PBDq_h1jKCo7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training and validation process\n",
        "======\n",
        "Noam optimizer\n",
        "------"
      ],
      "metadata": {
        "id": "sPbMfCre6cSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reference : https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "class NoamOpt:\n",
        "    def __init__(self,dictionary_length,factor,warmup,optimizer):\n",
        "        self.dict_len = dictionary_length\n",
        "        self.factor = factor\n",
        "        self.warmup = warmup\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self._rate = 0\n",
        "    def step(self):\n",
        "        self._step += 1\n",
        "        self._rate = self.factor *(self.dict_len ** (-0.5) * \\\n",
        "        min(self._step ** (-0.5), self._step * self.warmup ** (-1.5)))\n",
        "\n",
        "        self.optimizer.param_groups[0][\"lr\"] = self._rate\n",
        "        self.optimizer.step()\n",
        "    def zero_grad(self):\n",
        "        return self.optimizer.zero_grad()\n",
        "# test NoamOpt:\n",
        "# x = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "# x.param_groups[0][\"lr\"]"
      ],
      "metadata": {
        "id": "Lb8BnuysNCOQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "label smoothing\n",
        "------"
      ],
      "metadata": {
        "id": "3TyyebUNOeI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "  def __init__(self,batch_size,dictionary_length,padding_id,smoothing):\n",
        "        super().__init__()\n",
        "        self.dict_len = dictionary_length\n",
        "        self.smoothing = smoothing\n",
        "        self.padding_id = padding_id\n",
        "        shift = torch.full(size = (batch_size,1), dtype = torch.long, fill_value = self.padding_id)\n",
        "        self.register_buffer(\"shift\",shift)\n",
        "  def forward(self, is_last_batch, outputs , label):\n",
        "\n",
        "    # step1 : when using label in validation, shift is needed.\n",
        "    # label_shift : {type : tensor , shape : batch  X (max_sentence_length-1)\n",
        "    # value : int}\n",
        "    label_shift = label[:,1:]\n",
        "    # shift : {type : tensor , shape : batch  X 1 ,value : self.padding_id}\n",
        "\n",
        "    # label_shift : {type : tensor , shape : batch  X max_sentence_length\n",
        "    # value : int}\n",
        "    if is_last_batch:\n",
        "      label_shift = torch.cat((label_shift,self.shift[:label.size(0),:]),dim = 1)\n",
        "\n",
        "    else:\n",
        "      label_shift = torch.cat((label_shift,self.shift),dim = 1)\n",
        "\n",
        "    # step2 : convert label to onehot tensor, then apply label smoothing\n",
        "    # label_onehot : {type : tensor , shape : batch  X max_sentence_length X dictionary_length\n",
        "    # value : 0 or 1}\n",
        "    label_onehot = F.one_hot(label_shift,self.dict_len).float()\n",
        "    # add : {type : float}\n",
        "    add = self.smoothing / (self.dict_len)\n",
        "    # label_onehot : {type : tensor , shape : batch  X max_sentence_length X dictionary_length\n",
        "    # value : add or 1+add}\n",
        "    label_onehot += add\n",
        "    # label_smoothed : {type : tensor , shape : batch  X max_sentence_length X dictionary_length\n",
        "    # value : add or 1+add-self.smoothing}\n",
        "    label_smoothed = label_onehot.masked_fill_((label_onehot > 1),float(1-self.smoothing+add))\n",
        "\n",
        "    '''\n",
        "    Question : Is padding really needed?\n",
        "    '''\n",
        "    # step3 : use padding mask to ignore to loss from padding id, then calculate loss.\n",
        "    # loss : {type : tensor , shape : batch  X max_sentence_length X 1, value : float}\n",
        "    loss = -1*torch.sum((outputs*label_smoothed),dim = -1)\n",
        "    # label_padding_mask {type : tensor , shape : batch  X max_sentence_length, value : bool}\n",
        "    label_padding_mask = (label == self.padding_id)\n",
        "    # mask_loss : {type : tensor , shape : batch  X max_sentence_length,\n",
        "    # value : 0 or add or 1+add-self.smoothing}\n",
        "    mask_loss = loss.masked_fill_(label_padding_mask,0)\n",
        "    # # ignore_index_number : {type : int}\n",
        "    # ignore_index_number = (mask_loss == 0).sum().item()\n",
        "    # avg_loss : {type : int}\n",
        "    # avg_loss = mask_loss.sum()/(mask_loss.size(0)*mask_loss.size(1)-ignore_index_number)\n",
        "    avg_loss = mask_loss.sum()/mask_loss.size(0)\n",
        "    return(avg_loss)\n",
        "\n",
        "# test LabelSmoothedCrossEntropyCriterion\n",
        "# cal1 = LabelSmoothedCrossEntropyCriterion()\n",
        "# print(cal1(outputs,tgt))\n",
        "\n",
        "# ignore_index not work correctly\n",
        "# def LabelSmoothedCrossEntropy(outputs , label,dictionary_length,smooth,padding_id):\n",
        "#   print(outputs.shape)\n",
        "#   print(label.shape)\n",
        "#   label_onehot = label.transpose(-1,-2).squeeze()\n",
        "#   outputs = outputs.transpose(-1,-2)\n",
        "#   cal_loss = nn.CrossEntropyLoss(ignore_index = padding_idx,reduction = \"mean\", label_smoothing=smooth)\n",
        "#   return cal_loss(outputs,label_onehot)"
      ],
      "metadata": {
        "id": "8JltOQM_wq4m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://arxiv.org/pdf/1512.00567.pdf page 7\n",
        "\n",
        "#Ref 1 : Hong-Yi Li ML2021 HW5\n",
        "\n",
        "# class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "#     def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "#         super().__init__()\n",
        "#         self.smoothing = smoothing\n",
        "#         self.ignore_index = ignore_index\n",
        "#         self.reduce = reduce\n",
        "\n",
        "#     def forward(self, lprobs, target):\n",
        "#         if target.dim() == lprobs.dim() - 1:\n",
        "#             target = target.unsqueeze(-1)\n",
        "#         # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
        "#         nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "#         #  reserve some probability for other labels. thus when calculating cross-entropy,\n",
        "#         # equivalent to summing the log probs of all labels\n",
        "#         smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "#         if self.ignore_index is not None:\n",
        "#             pad_mask = target.eq(self.ignore_index)\n",
        "#             nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "#             smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "#         else:\n",
        "#             nll_loss = nll_loss.squeeze(-1)\n",
        "#             smooth_loss = smooth_loss.squeeze(-1)\n",
        "#         if self.reduce:\n",
        "#             nll_loss = nll_loss.sum()\n",
        "#             smooth_loss = smooth_loss.sum()\n",
        "#         # when calculating cross-entropy, add the loss of other labels\n",
        "#         eps_i = self.smoothing / lprobs.size(-1)\n",
        "#         loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "#         return loss\n",
        "\n",
        "#Ref 2 : By hemingkx : https://github.com/hemingkx/ChineseNMT\n",
        "\n",
        "# class LabelSmoothing(nn.Module):\n",
        "#     \"\"\"Implement label smoothing.\"\"\"\n",
        "\n",
        "#     def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "#         super(LabelSmoothing, self).__init__()\n",
        "#         self.criterion = nn.KLDivLoss(size_average=False)\n",
        "#         self.padding_idx = padding_idx\n",
        "#         self.confidence = 1.0 - smoothing\n",
        "#         self.smoothing = smoothing\n",
        "#         self.size = size\n",
        "#         self.true_dist = None\n",
        "\n",
        "\n",
        "#     def forward(self, x, target):\n",
        "#         assert x.size(1) == self.size\n",
        "#         true_dist = x.data.clone()\n",
        "#         true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "#         true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "#         true_dist[:, self.padding_idx] = 0\n",
        "#         mask = torch.nonzero(target.data == self.padding_idx)\n",
        "#         if mask.dim() > 0:\n",
        "#             true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "#         self.true_dist = true_dist\n",
        "#         return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "metadata": {
        "id": "uaE0-tA9Q9cq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "beam search\n",
        "------"
      ],
      "metadata": {
        "id": "33m5daxZ7Cpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "class Decode_With_Beam_Search(nn.Module):\n",
        "    def __init__(self,batch_size,model,beam_num,max_sentence_length,\n",
        "           dictionary_length,bos_id,padding_id):\n",
        "      super().__init__()\n",
        "      self.batch_size = batch_size\n",
        "      self.model = model\n",
        "      self.beam_num = beam_num\n",
        "      self.max_sentence_length = max_sentence_length\n",
        "      self.dictionary_length = dictionary_length\n",
        "      self.bos_id = bos_id\n",
        "      self.padding_id = padding_id\n",
        "      # decoder_input : {type : tensor , shape : Batch X 1 , value : bos_id}\n",
        "      decoder_input = torch.full(size = (self.batch_size,1),fill_value = self.bos_id)\n",
        "      self.register_buffer(\"decoder_input\",decoder_input)\n",
        "      # repeat : {type : tensor , shape : Batch ,value : beam_num}\n",
        "      # each row repeat beam_num times before concatenate\n",
        "      repeat = torch.full([self.batch_size],fill_value = self.beam_num)\n",
        "      self.register_buffer(\"repeat\",repeat)\n",
        "      # decoder_probability {type : tensor , shape : Batch X beam_num X 1, value : 0.1}\n",
        "      decoder_probability = torch.full(size = (self.batch_size,self.beam_num,1),fill_value = 0.0)\n",
        "      self.register_buffer(\"decoder_probability\",decoder_probability)\n",
        "\n",
        "      # padding : {type : tensor , shape : (Batch X beam_num) X (max_sentence_length-(id+1)) ,value : int}\n",
        "      padding = torch.full(size = (batch_size*self.beam_num,self.max_sentence_length),fill_value = self.padding_id)\n",
        "      self.register_buffer(\"padding\",padding)\n",
        "\n",
        "      # row : {type : tensor , shape : batch X 1, value : [[0],[1],[2],...]}\n",
        "      row = torch.tensor(range(self.batch_size)).unsqueeze(1)\n",
        "      self.register_buffer(\"row\",row)\n",
        "\n",
        "    def forward(self,is_last_batch,src,src_mask):\n",
        "\n",
        "      if is_last_batch:\n",
        "        batch = src.size(0)\n",
        "      else :\n",
        "        batch = self.batch_size\n",
        "\n",
        "      if self.beam_num > batch:\n",
        "        beam_num = batch\n",
        "      else :\n",
        "        beam_num = self.beam_num\n",
        "\n",
        "      decoder_input = self.decoder_input[:batch,:]\n",
        "      repeat = self.repeat[:batch]\n",
        "      decoder_probability = self.decoder_probability[:batch,:,]\n",
        "      padding = self.padding[:batch*beam_num,:]\n",
        "      row = self.row[:batch,:]\n",
        "\n",
        "      # decoder_beam_expand : {type : tensor , shape : (Batch X beam_num) X 1 ,value : bos_id}\n",
        "      decoder_beam_expand = torch.repeat_interleave(decoder_input,repeat,dim=0)\n",
        "\n",
        "      # memory : {type : tensor , shape : Batch X max_sentence_length X encoder_output_dim ,value : arbitary float}\n",
        "      memory = self.model.encoder(is_last_batch,src,src_mask)\n",
        "      # memory_beam_expand : {type : tensor ,\n",
        "      # shape : (Batch X n_beam) X max_sentence_length X encoder_output_dim ,value : float}\n",
        "      memory_beam_expand = torch.repeat_interleave(memory,repeat,dim=0)\n",
        "\n",
        "      gc.collect()\n",
        "\n",
        "      for id in range(self.max_sentence_length-1):\n",
        "\n",
        "        # decoder_n_beam : {type : tensor , shape : (Batch X beam_num) X (id+1) ,value : int}\n",
        "        # decoder_probability {type : tensor , shape : Batch X beam_num X 1 , value : log_softmax probability}\n",
        "        new_decoder_beam_expand , new_decoder_probability = self.get_next_word(is_last_batch,memory_beam_expand,\n",
        "        decoder_beam_expand,decoder_probability,id,batch,beam_num,padding,repeat,row)\n",
        "\n",
        "        decoder_beam_expand,decoder_probability = new_decoder_beam_expand , new_decoder_probability\n",
        "        if id%10 ==0:\n",
        "          print(new_decoder_beam_expand[0])\n",
        "        gc.collect()\n",
        "\n",
        "      # out_beam_expand : {type : tensor , shape : Batch X beam_num X (max_sentence_length) ,value : 0 or 1}\n",
        "      decoder_beam_expand = decoder_beam_expand.view(batch,beam_num,self.max_sentence_length)\n",
        "      # max_probability : {type : tensor , shape :  Batch  X 1 ,value : int(max prob index)}\n",
        "      max_probability = torch.argmax(input = decoder_probability,dim = 1)\n",
        "      # max_probability_expand : {type : tensor , shape :  Batch  X 1 X max_sentence_length ,\n",
        "      # value : [[A,A,A....],[B,B,B...],...](A,B are int)}\n",
        "      max_probability_expand = max_probability.expand(batch, self.max_sentence_length).unsqueeze(1)\n",
        "      # decoder_out : {type : tensor , shape :  Batch X max_sentence_length ,\n",
        "      # value : [[int,int,...],[int,int...],...]}\n",
        "      decoder_out =  torch.gather(input = decoder_beam_expand ,dim = 1, index = max_probability_expand).squeeze(1)\n",
        "\n",
        "      print(decoder_out[0])\n",
        "      return decoder_out,F.one_hot(decoder_out,self.dictionary_length).float()\n",
        "\n",
        "    def get_next_word(self,is_last_batch,memory,out,out_probability,id,batch,beam_num,padding,repeat,row):\n",
        "      # padding : {type : tensor , shape : (Batch X beam_num) X (max_sentence_length-(id+1)) ,value : int}\n",
        "      padding = padding[:,:self.max_sentence_length-(id+1)]\n",
        "      # out_padding : {type : tensor , shape : (Batch X beam_num) X max_sentence_length,\n",
        "      # value : [[bos_id],[any_id],...[padding_id],....] X Batch}\n",
        "      out_padding = torch.cat((out,padding),dim = 1)\n",
        "      # tgt_padding : {type : tensor , shape : (Batch X beam_num) X max_sentence_length ,value: bool}\n",
        "      tgt_padding = (out_padding == self.padding_id).squeeze(-1)\n",
        "      # out_add : {type : tensor , shape : Batch X beam_num X dictionary_length ,value : int}\n",
        "      out_add = self.model.decoder(is_last_batch,memory,out_padding,tgt_padding)[:,id,:]\\\n",
        "            .view(batch,beam_num,self.dictionary_length)\n",
        "      # out_n_beam : {type : tensor , shape : (Batch X beam_num) X (id+1) ,value : int}\n",
        "      # out_probability {type : tensor , shape : Batch X beam_num X 1 , value : log_softmax probability}\n",
        "      out , out_probability = self.beam_search_one_step(batch,beam_num,repeat,row,out,out_probability,out_add)\n",
        "\n",
        "      gc.collect()\n",
        "      return(out , out_probability)\n",
        "\n",
        "    def beam_search_one_step(self,batch,beam_num,repeat,row,sentences,p_sentences,n_beam_output):\n",
        "    # sentences : {type : tensor , shape : (batch X beam_num) X now_sentences_length X 1 value : int}\n",
        "    # p_sentences : {type : tensor , shape : batch X beam_num X 1 value : log_softmax probability}\n",
        "    # n_beam_output : {type : tensor , shape : batch X beam_num X dictionary_length,\n",
        "    # value : [P1,P2,P3...] X beam_num times (Pk in [0,1])}\n",
        "\n",
        "      '''\n",
        "      TO DO : (set beam num = K)\n",
        "      for every batch:\n",
        "      expand sentences(total number = K) K times (so there are K-square sentences),then concat with\n",
        "      the index of top K consequence of each beam(total K beams) in n_beam_output (so there are also K-square values).\n",
        "      '''\n",
        "      # sentences : {type : tensor , shape : batch X beam_num X now_sentences_length value : int}\n",
        "      sentences = sentences.view(batch,beam_num,-1)\n",
        "      # repeat : {type : tensor , shape : beam_num ,value : beam_num}\n",
        "      # each row repeat beam_num times before concatenate\n",
        "      repeat = repeat[:beam_num]\n",
        "      # sentences_expand : {type : tensor , shape : batch X (beam_num X beam_num) X now_sentences_length ,\n",
        "      # value : [[[A,B...] X beam_num times,[C,D...] X beam_num times}...] A,B,C,D...are int}\n",
        "      sentences_expand = torch.repeat_interleave(sentences,repeat,dim=1)\n",
        "\n",
        "      # topk_prob : {type : tensor , shape : batch X beam_num X beam_num, value : log_softmax probability}\n",
        "      # topk_index : {type : tensor , shape : batch X beam_num X beam_num, value : int}\n",
        "      topk_prob, topk_index = torch.topk(n_beam_output,dim = -1,k = beam_num)\n",
        "\n",
        "      # topk_index : {type : tensor , shape : batch X (beam_num X beam_num) X 1, value : int}\n",
        "      topk_index = topk_index.view(batch,-1,1)\n",
        "      # sentences : {type : tensor , shape : batch X (beam_num X beam_num) X (now_sentences_length+1), value : int}\n",
        "      sentences_expand = torch.cat((sentences_expand,topk_index),dim = -1)\n",
        "      '''\n",
        "      TO DO :\n",
        "      multipies p_sentences with the probability of top K consequence of each beam(total K beams) in n_beam_output\n",
        "      (so there are also K-square values).\n",
        "\n",
        "      The final step is to choose Top K consequence from K-square sentences by using p_sentences.\n",
        "      '''\n",
        "\n",
        "      # p_sentences : {type : tensor , shape : batch X (beam_num X beam_num),\n",
        "      # value : [P1,P2,P3...] X beam_num times (Pk is log_softmax probability)}\n",
        "      p_sentences = (p_sentences+topk_prob).view(batch,-1)\n",
        "      # p_sentences : {type : tensor , shape : batch X beam_num, value : log_softmax probability}\n",
        "      # p_index : {type : tensor , shape : batch X beam_num, value : int}\n",
        "      p_sentences, p_index = torch.topk(p_sentences, dim = 1, k = beam_num)\n",
        "      p_sentences = p_sentences.unsqueeze(-1)\n",
        "      # row : {type : tensor , shape : batch X 1, value : [[0],[1],[2],...]}\n",
        "      # sentences : {type : tensor , shape : batch X beam_num X (now_sentences_length+1), value : log_softmax probability}\n",
        "      new_sentences = sentences_expand[row, p_index].view(batch*beam_num,-1)\n",
        "      sentences.data = new_sentences.data\n",
        "      gc.collect()\n",
        "      return sentences,p_sentences\n",
        "\n",
        "# test decode_with_beam_search\n",
        "# batch = 3\n",
        "# beam_num = 2\n",
        "# sentences = torch.randint(0,8000,(batch*beam_num,5))\n",
        "# p_sentences = torch.log(torch.rand((batch , beam_num , 1)))\n",
        "# n_beam_output = torch.rand((batch , beam_num , 8000))\n",
        "# print(sentences,p_sentences,n_beam_output)\n",
        "# print(beam_search_one_step(sentences,p_sentences,n_beam_output))\n",
        "# repeat = torch.full([beam_num],fill_value = beam_num)\n",
        "# sentences_expand = torch.repeat_interleave(sentences.view(batch,beam_num,-1),repeat,dim=1)\n",
        "# print(sentences_expand,sentences_expand.shape)\n",
        "# decode_model = Decode_With_Beam_Search(32,model,2,400,8000,2,0)\n",
        "# outputs_in_word,outputs = decode_model(False,src,src_mask)\n",
        "# print(output[0])"
      ],
      "metadata": {
        "id": "6hrE6wYFxTIs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def beam_search_one_step(device,sentences,p_sentences,n_beam_output):\n",
        "    # sentences : {type : tensor , shape : (batch X beam_num) X now_sentences_length X 1 value : int}\n",
        "    # p_sentences : {type : tensor , shape : batch X beam_num X 1 value : log_softmax probability}\n",
        "    # n_beam_output : {type : tensor , shape : batch X beam_num X dictionary_length,\n",
        "    # value : [P1,P2,P3...] X beam_num times (Pk in [0,1])}\n",
        "\n",
        "    '''\n",
        "    TO DO : (set beam num = K)\n",
        "    for every batch:\n",
        "    expand sentences(total number = K) K times (so there are K-square sentences),then concat with\n",
        "    the index of top K consequence of each beam(total K beams) in n_beam_output (so there are also K-square values).\n",
        "    '''\n",
        "    batch = n_beam_output.size(0)\n",
        "    beam_num = n_beam_output.size(1)\n",
        "    # sentences : {type : tensor , shape : batch X beam_num X now_sentences_length value : int}\n",
        "    sentences = sentences.view(batch,beam_num,-1)\n",
        "    # repeat : {type : tensor , shape : beam_num ,value : beam_num}\n",
        "    # each row repeat beam_num times before concatenate\n",
        "    repeat = torch.full([beam_num],fill_value = beam_num)\n",
        "    repeat = repeat.to(device)\n",
        "    # sentences_expand : {type : tensor , shape : batch X (beam_num X beam_num) X now_sentences_length ,\n",
        "    # value : [[[A,B...] X beam_num times,[C,D...] X beam_num times}...] A,B,C,D...are int}\n",
        "    sentences_expand = torch.repeat_interleave(sentences,repeat,dim=1)\n",
        "\n",
        "    # topk_prob : {type : tensor , shape : batch X beam_num X beam_num, value : log_softmax probability}\n",
        "    # topk_index : {type : tensor , shape : batch X beam_num X beam_num, value : int}\n",
        "    topk_prob, topk_index = torch.topk(n_beam_output,dim = -1,k = beam_num)\n",
        "\n",
        "    # topk_index : {type : tensor , shape : batch X (beam_num X beam_num) X 1, value : int}\n",
        "    topk_index = topk_index.view(batch,-1,1)\n",
        "    # sentences : {type : tensor , shape : batch X (beam_num X beam_num) X (now_sentences_length+1), value : int}\n",
        "    sentences_expand = torch.cat((sentences_expand,topk_index),dim = -1)\n",
        "    '''\n",
        "    TO DO :\n",
        "    multipies p_sentences with the probability of top K consequence of each beam(total K beams) in n_beam_output\n",
        "    (so there are also K-square values).\n",
        "\n",
        "    The final step is to choose Top K consequence from K-square sentences by using p_sentences.\n",
        "    '''\n",
        "\n",
        "    # p_sentences : {type : tensor , shape : batch X (beam_num X beam_num),\n",
        "    # value : [P1,P2,P3...] X beam_num times (Pk is log_softmax probability)}\n",
        "    p_sentences = (p_sentences+topk_prob).view(batch,-1)\n",
        "    # p_sentences : {type : tensor , shape : batch X beam_num, value : log_softmax probability}\n",
        "    # p_index : {type : tensor , shape : batch X beam_num, value : int}\n",
        "    p_sentences, p_index = torch.topk(p_sentences, dim = 1, k = beam_num)\n",
        "    p_sentences = p_sentences.unsqueeze(-1)\n",
        "    # row : {type : tensor , shape : batch X 1, value : [[0],[1],[2],...]}\n",
        "    row = torch.tensor(range(batch)).unsqueeze(1)\n",
        "    row = row.to(device)\n",
        "    # sentences : {type : tensor , shape : batch X beam_num X (now_sentences_length+1), value : log_softmax probability}\n",
        "    new_sentences = sentences_expand[row, p_index].view(batch*beam_num,-1)\n",
        "    sentences.data = new_sentences.data\n",
        "    gc.collect()\n",
        "    return sentences,p_sentences\n",
        "\n",
        "def get_next_word(model,is_last_batch,device,memory,out,out_probability,id,batch,beam_num,max_sentence_length,dictionary_length,padding_id):\n",
        "    # padding : {type : tensor , shape : (Batch X beam_num) X (max_sentence_length-(id+1)) ,value : int}\n",
        "    padding = torch.full(size = (batch*beam_num,max_sentence_length-(id+1)),fill_value = padding_id)\n",
        "    padding = padding.to(device)\n",
        "    # out_padding : {type : tensor , shape : (Batch X beam_num) X max_sentence_length,\n",
        "    # value : [[bos_id],[any_id],...[padding_id],....] X Batch}\n",
        "    out_padding = torch.cat((out,padding),dim = 1)\n",
        "    # tgt_padding : {type : tensor , shape : (Batch X beam_num) X max_sentence_length ,value: bool}\n",
        "    tgt_padding = (out_padding == padding_id).squeeze(-1)\n",
        "    # out_add : {type : tensor , shape : Batch X beam_num X dictionary_length ,value : int}\n",
        "    out_add = model.decoder(is_last_batch,memory,out_padding,tgt_padding)[:,id,:].view(batch,beam_num,dictionary_length)\n",
        "    # out_n_beam : {type : tensor , shape : (Batch X beam_num) X (id+1) ,value : int}\n",
        "    # out_probability {type : tensor , shape : Batch X beam_num X 1 , value : log_softmax probability}\n",
        "    out , out_probability = beam_search_one_step(device,out,out_probability,out_add)\n",
        "\n",
        "    gc.collect()\n",
        "    return(out , out_probability)\n",
        "\n",
        "def decode_with_beam_search(device,is_last_batch,model,src,src_mask,beam_num,max_sentence_length,\n",
        "               dictionary_length,bos_id,padding_id):\n",
        "    with torch.no_grad():\n",
        "      batch = src.size(0)\n",
        "      # decoder_input : {type : tensor , shape : Batch X 1 , value : bos_id}\n",
        "      decoder_input = torch.full(size = (batch,1),fill_value = bos_id)\n",
        "      decoder_input = decoder_input.to(device)\n",
        "      # repeat : {type : tensor , shape : Batch ,value : beam_num}\n",
        "      # each row repeat beam_num times before concatenate\n",
        "      repeat = torch.full([batch],fill_value = beam_num)\n",
        "      repeat = repeat.to(device)\n",
        "      # decoder_beam_expand : {type : tensor , shape : (Batch X beam_num) X 1 ,value : bos_id}\n",
        "      decoder_beam_expand = torch.repeat_interleave(decoder_input,repeat,dim=0)\n",
        "\n",
        "      # decoder_probability {type : tensor , shape : Batch X beam_num X 1, value : 0.1}\n",
        "      decoder_probability = torch.full(size = (batch,beam_num,1),fill_value = 0.0)\n",
        "      decoder_probability = decoder_probability.to(device)\n",
        "\n",
        "      # memory : {type : tensor , shape : Batch X max_sentence_length X encoder_output_dim ,value : arbitary float}\n",
        "      memory = model.encoder(is_last_batch,src,src_mask)\n",
        "      # memory_beam_expand : {type : tensor ,\n",
        "      # shape : (Batch X n_beam) X max_sentence_length X encoder_output_dim ,value : float}\n",
        "      memory_beam_expand = torch.repeat_interleave(memory,repeat,dim=0)\n",
        "\n",
        "      gc.collect()\n",
        "\n",
        "      for id in range(max_sentence_length-1):\n",
        "\n",
        "        # decoder_n_beam : {type : tensor , shape : (Batch X beam_num) X (id+1) ,value : int}\n",
        "        # decoder_probability {type : tensor , shape : Batch X beam_num X 1 , value : log_softmax probability}\n",
        "        new_decoder_beam_expand , new_decoder_probability = \\\n",
        "        get_next_word(model,is_last_batch,device,memory_beam_expand,decoder_beam_expand,\n",
        "               decoder_probability,id,batch,beam_num,max_sentence_length,dictionary_length,padding_id)\n",
        "\n",
        "        decoder_beam_expand,decoder_probability = new_decoder_beam_expand , new_decoder_probability\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "      # out_beam_expand : {type : tensor , shape : Batch X beam_num X (max_sentence_length) ,value : 0 or 1}\n",
        "      decoder_beam_expand = decoder_beam_expand.view(batch,beam_num,max_sentence_length)\n",
        "      # max_probability : {type : tensor , shape :  Batch  X 1 ,value : int(max prob index)}\n",
        "      max_probability = torch.argmax(input = decoder_probability,dim = 1)\n",
        "      # max_probability_expand : {type : tensor , shape :  Batch  X 1 X max_sentence_length ,\n",
        "      # value : [[A,A,A....],[B,B,B...],...](A,B are int)}\n",
        "      max_probability_expand = max_probability.expand(batch, max_sentence_length).unsqueeze(1)\n",
        "      # decoder_out : {type : tensor , shape :  Batch X max_sentence_length ,\n",
        "      # value : [[int,int,...],[int,int...],...]}\n",
        "      decoder_out =  torch.gather(input = decoder_beam_expand ,dim = 1, index = max_probability_expand).squeeze(1)\n",
        "    print(decoder_out[0])\n",
        "    return decoder_out,F.one_hot(decoder_out,dictionary_length).float()\n",
        "\n",
        "# test decode_with_beam_search\n",
        "# batch = 3\n",
        "# beam_num = 2\n",
        "# sentences = torch.randint(0,8000,(batch*beam_num,5))\n",
        "# p_sentences = torch.log(torch.rand((batch , beam_num , 1)))\n",
        "# n_beam_output = torch.rand((batch , beam_num , 8000))\n",
        "# print(sentences,p_sentences,n_beam_output)\n",
        "# print(beam_search_one_step(sentences,p_sentences,n_beam_output))\n",
        "# repeat = torch.full([beam_num],fill_value = beam_num)\n",
        "# sentences_expand = torch.repeat_interleave(sentences.view(batch,beam_num,-1),repeat,dim=1)\n",
        "# print(sentences_expand,sentences_expand.shape)\n",
        "# output = decode_with_beam_search(model,src,src_mask,2,400,8000,)\n",
        "# print(output[0])"
      ],
      "metadata": {
        "id": "sz_zI5fDh9UU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bleu\n",
        "------"
      ],
      "metadata": {
        "id": "Pt1o-F1Ik8cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torcheval.metrics.functional.text import bleu\n",
        "def get_bleu_score(outputs,tgt,tgt_tokenizer,eos_id):\n",
        "    outputs = np.array(outputs.detach().tolist())\n",
        "    outputs = [x[:np.nonzero(x == eos_id)[0][0]].tolist() if len(np.nonzero(x == eos_id)[0])>0 \\\n",
        "              else x.tolist() for x in outputs ]\n",
        "\n",
        "    outputs_decode = tgt_tokenizer.decode(outputs)\n",
        "    out = [outputs_decode[i] for i in range(len(outputs_decode)) if len(outputs_decode[i])>= 4]\n",
        "    print(out)\n",
        "    out = [\" \".join(list(x)) for x in out]\n",
        "    print(out)\n",
        "    tgt_decode = tgt_tokenizer.decode(tgt.detach().tolist())\n",
        "    tgt = [tgt_decode[i] for i in range(len(outputs_decode)) if len(outputs_decode[i])>= 4]\n",
        "    print(tgt)\n",
        "    tgt = [\" \".join(list(x)) for x in tgt]\n",
        "    print(tgt)\n",
        "    return bleu.bleu_score(out, tgt, n_gram=4).detach().item()\n",
        "# test bleu\n",
        "# test_tokenizer = spm.SentencePieceProcessor(model_file = \"/content/spm_8000_zh.model\")\n",
        "# candidates = torch.tensor([[21,3,9,99,42],[5,78,89,3,31]])\n",
        "# references = torch.tensor([[18,5,9,3,42],[3,5,78,89,50]])\n",
        "# get_bleu_score(candidates,references,test_tokenizer,3)"
      ],
      "metadata": {
        "id": "ISpHxGVQk7dh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train and validation function\n",
        "------"
      ],
      "metadata": {
        "id": "GF4GshwGjz-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "\n",
        "def train_one_epoch(device,model,loss_calculator,is_last_batch,\n",
        "          src,tgt,src_mask,tgt_mask,dictionary_length,\n",
        "          optimizer):\n",
        "\n",
        "    outputs = model(is_last_batch,src,tgt,src_mask,tgt_mask)\n",
        "\n",
        "    loss = loss_calculator(is_last_batch,outputs,tgt)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return loss.detach().item(),outputs[0].detach()\n",
        "\n",
        "def valid(device,model,loss_calculator,batch_size_setting,valid_loader,beam_num,max_sentence_length,\n",
        "      dictionary_length,bos_id,eos_id,pad_id,tgt_tokenizer):\n",
        "    batch_loss = []\n",
        "    batch_bleu_score = []\n",
        "    with torch.no_grad():\n",
        "      for val_batch in tqdm(valid_loader,desc=\"valid_step\", unit=\" step\"):\n",
        "        src,tgt,src_mask,tgt_mask = val_batch\n",
        "        src,tgt,src_mask = src.to(device),tgt.to(device),src_mask.to(device)\n",
        "\n",
        "        batch_size = src.size(0)\n",
        "\n",
        "        is_last_batch = False\n",
        "        if batch_size != batch_size_setting:\n",
        "          is_last_batch = True\n",
        "        decode_model = Decode_With_Beam_Search(batch_size,model,beam_num,max_sentence_length,\n",
        "                            dictionary_length,bos_id,pad_id)\n",
        "        decode_model.to(device)\n",
        "        outputs_in_word,outputs = decode_model(is_last_batch,src,src_mask)\n",
        "        # outputs_in_word,outputs = decode_with_beam_search(device,is_last_batch,model,src,src_mask,beam_num,\\\n",
        "        #       max_sentence_length,dictionary_length,bos_id,pad_id)\n",
        "\n",
        "\n",
        "        loss = loss_calculator(is_last_batch,outputs,tgt)\n",
        "\n",
        "        bleu_score = get_bleu_score(outputs_in_word,tgt,tgt_tokenizer,eos_id)\n",
        "\n",
        "        batch_loss.append(loss)\n",
        "        batch_bleu_score.append(bleu_score)\n",
        "\n",
        "      avg_valid_loss = batch_loss.sum()/len(batch_loss).detach().item()\n",
        "      avg_bleu_score = batch_bleu_score.sum()/len(batch_bleu_score).detach().item()\n",
        "\n",
        "    return avg_valid_loss,avg_bleu_score"
      ],
      "metadata": {
        "id": "bwXkljhMFqKV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main function\n",
        "------"
      ],
      "metadata": {
        "id": "R7I8W9cQj7ES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def main(setting,dataset_is_prepare = False):\n",
        "\n",
        "    # set random seed\n",
        "    myseed = 1\n",
        "    np.random.seed(myseed)\n",
        "    torch.manual_seed(myseed)\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed_all(myseed)\n",
        "\n",
        "    # data set & tokenizer\n",
        "    if not dataset_is_prepare:\n",
        "        clean_data_and_save(\n",
        "        path_doc = setting[\"data_info\"][\"document\"],\n",
        "        raw_src_path = setting[\"data_info\"][\"source\"][\"raw_data_path\"],\n",
        "        raw_tgt_path = setting[\"data_info\"][\"target\"][\"raw_data_path\"],\n",
        "        clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "        clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"],\n",
        "        threshold = setting[\"tokenized_setting\"][\"max_l\"])\n",
        "\n",
        "        src_tokenizer,tgt_tokenizer = tokenized_data(\n",
        "            vocab_size = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "            tokenized_setting = {k:setting[\"tokenized_setting\"][k] for k in \\\n",
        "                      set(list(setting[\"tokenized_setting\"].keys()))-{\"vocab_size\",\"max_l\"}},\n",
        "            max_l = setting[\"tokenized_setting\"][\"max_l\"],\n",
        "            path_doc = setting[\"data_info\"][\"document\"],\n",
        "            clean_src_path = setting[\"data_info\"][\"source\"][\"clean_data_path\"],\n",
        "            clean_tgt_path = setting[\"data_info\"][\"target\"][\"clean_data_path\"],\n",
        "            src_lang = setting[\"data_info\"][\"source\"][\"lang\"],\n",
        "            tgt_lang = setting[\"data_info\"][\"target\"][\"lang\"],\n",
        "            st_train_path = setting[\"data_info\"][\"source\"][\"tokenized_train_data\"],\n",
        "            st_val_path = setting[\"data_info\"][\"source\"][\"tokenized_val_data\"],\n",
        "            tt_train_path = setting[\"data_info\"][\"target\"][\"tokenized_train_data\"],\n",
        "            tt_val_path = setting[\"data_info\"][\"target\"][\"tokenized_val_data\"])\n",
        "    else:\n",
        "        src_tokenizer,tgt_tokenizer = get_tokenizers(\n",
        "            path_doc = setting[\"data_info\"][\"document\"],\n",
        "            vocab_size = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "            src_lang = setting[\"data_info\"][\"source\"][\"lang\"],\n",
        "            tgt_lang = setting[\"data_info\"][\"target\"][\"lang\"],)\n",
        "\n",
        "    # data loader\n",
        "    train_batch_size_setting = setting[\"training_hparas\"][\"train_batch_size\"]\n",
        "    valid_batch_size_setting = setting[\"training_hparas\"][\"valid_batch_size\"]\n",
        "    train_loader,valid_loader = get_data_set(\n",
        "        train_batch_size = train_batch_size_setting,\n",
        "        valid_batch_size = valid_batch_size_setting,\n",
        "        num_workers = setting[\"training_hparas\"][\"workers\"],\n",
        "        path_doc = setting[\"data_info\"][\"document\"],\n",
        "        st_train_path = setting[\"data_info\"][\"source\"][\"tokenized_train_data\"],\n",
        "        st_val_path = setting[\"data_info\"][\"source\"][\"tokenized_val_data\"],\n",
        "        tt_train_path = setting[\"data_info\"][\"target\"][\"tokenized_train_data\"],\n",
        "        tt_val_path = setting[\"data_info\"][\"target\"][\"tokenized_val_data\"],\n",
        "        pad_id = setting[\"tokenized_setting\"][\"pad_id\"])\n",
        "    train_iter = iter(train_loader)\n",
        "    valid_iter = iter(valid_loader)\n",
        "\n",
        "    # model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model =  build_model(\n",
        "          max_sentence_length = setting[\"tokenized_setting\"][\"max_l\"],\n",
        "          dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "          padding_idx = setting[\"tokenized_setting\"][\"pad_id\"],\n",
        "          encoder_embedding_dimension = setting[\"model\"][\"encoder_embedding_dimension\"],\n",
        "          decoder_embedding_dimension = setting[\"model\"][\"decoder_embedding_dimension\"],\n",
        "          feedforward_dimension = setting[\"model\"][\"feedforward_dimension\"],\n",
        "          num_heads = setting[\"model\"][\"num_heads\"],\n",
        "          dropout_p = setting[\"model\"][\"dropout_p\"],\n",
        "          layer_num = setting[\"model\"][\"layer_num\"])\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_loss_calculator = LabelSmoothedCrossEntropyCriterion(\n",
        "                batch_size = train_batch_size_setting,\n",
        "                dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "                padding_id = setting[\"tokenized_setting\"][\"pad_id\"],\n",
        "                smoothing = setting[\"training_hparas\"][\"label_smoothing\"])\n",
        "\n",
        "    valid_loss_calculator = LabelSmoothedCrossEntropyCriterion(\n",
        "            batch_size = valid_batch_size_setting,\n",
        "            dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "            padding_id = setting[\"tokenized_setting\"][\"pad_id\"],\n",
        "            smoothing = 0)\n",
        "\n",
        "    train_loss_calculator,valid_loss_calculator = \\\n",
        "    train_loss_calculator.to(device),valid_loss_calculator.to(device)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), **(setting[\"training_hparas\"][\"optimization\"][\"optimizer\"]))\n",
        "\n",
        "    Noam_optimizer = NoamOpt(\n",
        "             dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "             factor = setting[\"training_hparas\"][\"optimization\"][\"factor\"],\n",
        "             warmup = setting[\"training_hparas\"][\"optimization\"][\"warmup\"],\n",
        "             optimizer = optimizer)\n",
        "\n",
        "    # step\n",
        "    total_step = setting[\"training_hparas\"][\"total_step\"]\n",
        "    early_stop_epoch = setting[\"training_hparas\"][\"early_stop_step\"]\n",
        "    do_valid_steps = setting[\"training_hparas\"][\"do_valid_step\"]\n",
        "    early_stop_count = 0\n",
        "    progress_bar = tqdm(total = do_valid_steps, desc=\"train_step\", unit=\" step\")\n",
        "\n",
        "    # output datas\n",
        "    train_loss_every_batchs = []\n",
        "    valid_loss = []\n",
        "    bleu_score = []\n",
        "    best_bleu_score = 0\n",
        "\n",
        "    for step in range(total_step):\n",
        "\n",
        "      # training\n",
        "      # iter batch\n",
        "      try:\n",
        "        train_batch = next(train_iter)\n",
        "      except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        train_batch = next(train_iter)\n",
        "\n",
        "      # compute batch loss and update parameters in model\n",
        "      model.train()\n",
        "\n",
        "      src,tgt,src_mask,tgt_mask = train_batch\n",
        "      src,tgt,src_mask,tgt_mask = src.to(device),tgt.to(device),\\\n",
        "                     src_mask.to(device),tgt_mask.to(device)\n",
        "      batch_size = src.size(0)\n",
        "\n",
        "      is_last_batch = False\n",
        "      if batch_size != train_batch_size_setting:\n",
        "        is_last_batch = True\n",
        "\n",
        "      train_loss, test_sentence = train_one_epoch(\n",
        "              device = device,\n",
        "              model = model,\n",
        "              loss_calculator = train_loss_calculator,\n",
        "              is_last_batch = is_last_batch,\n",
        "              src = src,\n",
        "              tgt = tgt,\n",
        "              src_mask = src_mask,\n",
        "              tgt_mask = tgt_mask,\n",
        "              dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "              optimizer = Noam_optimizer)\n",
        "\n",
        "      train_loss_every_batchs.append(train_loss)\n",
        "      if (step+1) % (do_valid_steps//20) == 0:\n",
        "        print(train_loss_every_batchs[-1])\n",
        "        print(tgt_tokenizer.decode(torch.argmax(test_sentence,dim = -1).tolist()))\n",
        "        print(tgt_tokenizer.decode(tgt.detach().tolist()))\n",
        "\n",
        "      progress_bar.update()\n",
        "      if (step+1) % do_valid_steps == 0:\n",
        "\n",
        "        print(train_loss_every_batchs[-1])\n",
        "\n",
        "        progress_bar.close()\n",
        "\n",
        "        model.eval()\n",
        "        avg_val_loss,avg_bleu_score = valid(\n",
        "                        device = device,\n",
        "                        model = model,\n",
        "                        loss_calculator = valid_loss_calculator,\n",
        "                        batch_size_setting = valid_batch_size_setting,\n",
        "                        valid_loader = valid_loader,\n",
        "                        beam_num = setting[\"training_hparas\"][\"beam_num\"],\n",
        "                        max_sentence_length = setting[\"tokenized_setting\"][\"max_l\"],\n",
        "                        dictionary_length = setting[\"tokenized_setting\"][\"vocab_size\"],\n",
        "                        bos_id = setting[\"tokenized_setting\"][\"bos_id\"],\n",
        "                        eos_id = setting[\"tokenized_setting\"][\"eos_id\"],\n",
        "                        pad_id = setting[\"tokenized_setting\"][\"pad_id\"],\n",
        "                        tgt_tokenizer = tgt_tokenizer)\n",
        "        valid_loss.append(avg_val_loss)\n",
        "        bleu_score.append(avg_bleu_score)\n",
        "\n",
        "        # print avg loss\n",
        "        print(f\"average train loss = {sum(train_loss_every_batchs[-1*do_valid_steps:-1])/len(do_valid_steps):.4f}\")\n",
        "        print(f\"average valid loss = {valid_loss[-1]:.4f}\")\n",
        "        print(f\"average valid loss = {bleu_score[-1]:.4f}\")\n",
        "\n",
        "        # saving model and check early stop criterion\n",
        "        if bleu_score[-1] > best_bleu_score:\n",
        "          torch.save(model.state_dict(), setting[\"tokenized_setting\"][\"model_saving_path\"])\n",
        "        else :\n",
        "          early_stop_count += 1\n",
        "\n",
        "        if early_stop_count == early_stop_epoch:\n",
        "          break\n",
        "\n",
        "        progress_bar = tqdm(total = do_valid_steps, desc=\"train_step\", unit=\" step\")\n",
        "\n",
        "    progress_bar.close()\n",
        "\n",
        "    return train_loss_every_batchs,valid_loss,bleu_score"
      ],
      "metadata": {
        "id": "e7hn-NAjHSQ5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(setting,dataset_is_prepare = True)\n",
        "# gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGUYF3WnrI9F",
        "outputId": "09642365-da2c-41f7-a00e-9f0aa1d6bbcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 384064/384064 [00:31<00:00, 12375.50it/s]\n",
            "100%|██████████| 384064/384064 [00:34<00:00, 11268.07it/s]\n",
            "100%|██████████| 3879/3879 [00:00<00:00, 13999.97it/s]\n",
            "100%|██████████| 3879/3879 [00:00<00:00, 16465.11it/s]\n",
            "train_step:   5%|▌         | 200/4000 [02:59<59:46,  1.06 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "137.8590087890625\n",
            ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
            "['他是一位年輕的丈夫,一位半職業的棒球選手,也是一個紐約市的消防員。', '所以,我們反而必須多花時間發展人文、社會學和社會科學,修辭、哲學、倫理,因為這些知識構成我們的背景涵養,對大數據非常重要,也因為這能幫助我們更會思辨,', '我就是無法相信,我的父親,我年輕時的阿多尼斯,我親愛的朋友,會認為這樣的生命還值得活下去。', '第一天:「把它放到你的口袋中。', '我覺得這件事也一樣。', '很遺憾,這只是眾多事件的其中一件。', '而也不是所有的遷移都是自主的。', '第一:我沒有印度口音,我有的是巴勒斯坦的,好嗎?', '你們喜歡這種被加上標籤的感覺嗎?', '」', '我們開了另一個會議', '我們要讓選擇「左」的「協調者」獲得比較高的獎勵', '公約文件定義「難民」為:國民離鄉背景、無法回歸家園,是出於被迫害的恐懼,', '嫌惡是一種情緒,它結合了驚訝、尷尬,還有一些厭惡感,就像是不知道你的雙手要做什麼好。', '但我想我們忽略了某些告訴我們可以做的事的事實', '觀察使我們現在能夠', '」', '各位可以試想一下,如果我不這麽想,可能我們便沒有辦法從哥本哈根的爛攤子裏走出來。', '那麼─所有這些東拉西扯的東西應該歸到何處呢?', '我試著消失到jr的眼睛裏面,但是jr的所有作品裏面的模特眼睛都特別大,', '更糟的是,當你問:「你是否聽過媒體討論過這個?', '她們有著吃重的工作-有時候是自家的事業、有自己的孩子或者是父母要照顧、參與地方事務...大忙人!', '我們會叫他們\"生物修改工具\"', '我們在1979年十月寄出visicalc,', '我們可以滿懷期待我們會將兩岸的橋搭起,並且期待彼此相連', '他們進了會議室,我們把一位建築師帶到角落代表建築,另一個代表設計。', '如果再過個幾年每個壓力大的醫學院預科生也都能做到呢?', '」', '沒有,知道嗎?', '要我說,長期而言,在g0的世界中,歐洲的擴張太快了。', '想想看這其實是地球,多麼神奇!', '我們有個評論功能在那裡人們可以詢問議員問題', '因為它利用了意願來做正確的事。', '事實上,我訪問的第一個人georgeabbott,他已經九十七歲了,但他仍然充滿生命的力量我就是這樣想著,充滿力量。', '大部分人站在邊沁這一邊。', '是意外發現的', '而且那裡沒有人真正受貧窮壓迫,', '如果燃燒化石燃料這麼不好,以至於威脅到我們的生存,我們怎能持續以前的做法?', '他們總是很瞧不起我合作的公司。', '太不真實了。']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  10%|█         | 400/4000 [06:06<56:14,  1.07 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "138.47463989257812\n",
            ",,,,,,,,,,,,,,,,。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
            "['所以我們花了九年的時間,讓政府相信當地有很多被石油污染的企鵝。', '公司、銀行,', '第一:從最重要的部分開始寫。', '時間是早上三點。', '與其等待,我乾脆自己做一個,只用了一張紙', '他的守靈日剛好發生交通罷工。', '但那就是次要的了,', '男性文化對於持續不斷的男性對女性及兒童施暴的悲劇已經有太多的沉默,不是嗎?', '我,很幸運地,幾年以後遇到了zackieachmat\"治療\"活動的創辦人他是個很棒的遊説家和社會運動領導', '有馬爾地夫的人跟你聯繫時,你要做的第一件事,就是告訴他,你要現場勘查。', '克:我的意思是,有些其他計畫是從零開始。', '抗瘧疾蚊帳其實是功臣之一', '現在,你仍然可以看到她在販售各種口味的烤玉米和不同的零食等等。', '所以,千禧世代黑人、褐色人種、所有有色人種的故事,都需要被說出來,也需要被傾聽。', '這六年來,我懷抱著不屈不饒的態度,向這個體系注入樂觀主義,無論媒體會提出甚麼樣的質疑——我現在變得更會處理這些質疑了——而且,無論對立面有著怎樣的證據。', '至少,我希望他們只是在賺取廣告收入。', '如果一點漆的行進速度恰好,它撞上其中的任一物體都可能令其作廢。', '事實證明,常常。', '因此在地球最為寒冷的地區他們實質上是在冰箱裏面工作', '這讓我想到稍早michael在做的平衡動作因為你很努力的保持平衡並同時要彈奏和保持音調你不想要一直在注意音調的控制你想要感受音樂', '我們從農業社會轉換到工業社會,用了一百年的時間,然後用了六十年的時間,才完整服務業經濟的轉型。', '最主要的風力是來自於高度三百英呎以上的地方,我們目前還沒有可以到達那個高度所需的技術。', '現在我來介紹一下這個故事中的幾位主角這邊講幾個就好', '人們還會改變他們的觀點,所以他們的觀點不是恆久不變的。', '在這個光譜上,我們都能彼此聯繫,一起爲人類的所有痛苦尋求療癒。', '謝謝各位。', '你不能這樣測量人類。', '可以嗎?', '不過這不適用於所有人', '一個好故事帶出了更深刻的討論,在討論中,我的訪客也在他們自己的人生做了有意義的連結。', '這是在人類干預自然以前。', '身邊都是破舊的小屋。', '想想吧。', '直等到這一刻的觸動,否則,你實在不會開始第一個鏡頭', '你剛提到了非常強大且不容易的事,就是,你談到當你五歲時遇到的那次意外,你被性侵的事。', '那就是為什麼我在這裡,雖然我其實是頂著大頭來的。', '要是這麼簡單就好了。', '一路上,我得到了許多協助和大力支持。', '這裡的區域劃分規模龐大,因此我有責任在這些濱水區上打造優美的公園,我花了非常多的時間在這些計畫中的每一吋土地上。', '因為數據本身不會提供解答。']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  15%|█▌        | 600/4000 [09:13<53:11,  1.07 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124.4197769165039\n",
            ",,,,,,???。???。。。?。。。???。。。。?????。?。。?。。?。。?。?。。。。?。?。。。。。?。???????????。?。。?????。???。???。。??????????????????????????????????。????。???????。??????????????????。。??。??????????\n",
            "['你能想出方法讓你們全都脫困嗎?', '你們能夠想像這樣的畫面嗎?', '非洲還在發生些什麽?', '其中一件事情是,當我們把事情從簡單變成複雜是當我們想要更多。', '現代的女人比男人要負擔更多的家務,例如烹飪和打掃。', '但了解crispr技術提供了我們一個工具可以來做這些改變是很重要的一旦那些知識變成能取得後。', '這只是要告訴你在腦幹紅色的區塊,那裏有,簡單來說,所有的小方塊會對應到模組那是可以真的做出腦圖譜的我們內部不同的面向,及我們身體不同的面向。', '貓鼬就住在那裡。', '嗯,可能大家都會叫做法蘭克福-黑比諾', '我們如何辦到呢?', '數位id掃瞄器取代了人工驗票,加速了上車的流程,網路上還採用了人工智慧,讓旅行路線能最佳化。', '他們確實創造了新領域,探索一些領域你可能認為無人駕駛飛機只有軍隊在製造其實有一整群人在製造無人駕駛飛機或車輛就是你可以利用程式控制,讓它自主飛行而不用控制桿之類的,就可以控制它的飛行路徑', '正當我在繪圖板上醞釀這想法的時候,電話鈴響了。', '謝謝。', '」或者,「他住在哪裡?', '在漫長的太空旅程中,根本無法沿途取得任何碳,所以必須想辦法在艙裡,將碳回收再利用。', '所以要真正了解多世代職場的美好,我想,我們只需要認識人們的真我。', '或是疾病的表徵', '兩人也同時創造「放射性」一詞', '應該這樣說,我們能夠多早預知事件的發生,取決於幾個主要的因素。', '幾天後,我們在他家見面。', '我一直說,向本地診所提供藥物的非政府組織可能會幫他兒子恢復健康。', '這是地球的尺寸。', '」', '破懷你人際關係的方式,甚至是憤怒恐怖的模樣。', '我的病人可以因此受惠', '因為它無所爭,也就不會有所怨尤。', '但這點將會改變。', '統整這份報告的是國際山地綜合開發中心,縮寫為icimod,位在尼泊爾。', '我們把它稱為隱性社會單位。', '我的意思是,這是—想到連這個都還需要人去做實驗證明真的有點奇怪,但那就是當時事情的常態。', '美國出版商把我們的書名改成了《我們的末日》。', '這都是美食。', '當然事實上沒有微笑也無法忍受', '當國際貨幣組織在四零年代創立時', '這是我們的標誌。', '至此仍有一個重要的關鍵待解。', '並在這些技術變貴或被競爭對手掌握之前,盡快地取得它們,之後再發展該項技術,即使有失敗的風險或是研究中斷的風險。', '同樣極少量的科技也轉化了整個世界。', '關於女性在商業社群中帶來的衝擊效應,就又是另一篇演講了,']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  20%|██        | 800/4000 [12:20<50:03,  1.07 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140.36119079589844\n",
            "我們,,,的的的的的的的的的。。。。。。。。。。。。。。。。。。。。。。\n",
            "['我們引進財政法規讓財政預算與石油價格脫勾', '5點鐘:任務該是英雄面對最大考驗惡夢成真的時候了', '這是被變換成藝術的醫療工具。', '我們創造出了小型自駕式機器人,能在安靜的巷弄及人行道中找到方向,用行走的速度行進,還有安全的貨倉可以運送你的食物和用品。', '沒人來,動力不會出現,你永遠不會感覺想去做。', '他的女朋友大概也蠻生氣他的。', '和別人相處時要投入、心要在。', '不是去想說要製造什麼,而是為了思考而製造。', '歷史上,經過衝突後重建的地區,有40%在十年之內又再次發生了衝突', '我看看--搞一下牌,我才不能--', 'rives:所以當我回神的時候,我發現我有了一個興趣,我不知道我想不想要,但它得到大眾迴響。', '它在水中。', '我曾經有過許多的答案去試著回答這個問題。', '看看這些生物,它們到處游,它們在找地方吃東西和繁殖。', '現在,你們並不需要我給太多合成快樂的例子', '幾件事情會隨之而生。', '像死胡同,原地打轉一樣像塞車,談話遇到僵局一樣', '然而,我必須說,對我而言,溝通的過程並不是輕而易舉的。', '崔西寫信說,她是五個孩子的媽,也很享受居家的時間,但她剛經歷一場離婚必需回去工作賺錢,加上她真的很想把工作帶回她的生命中,因為她熱愛工作。', '完全的線上評分機制、同儕互動以及討論版都是我們必須努力的。', '好,讓我們弄清楚吧。', '瓦拉杰村座落的位置非常接近耶路撒冷那裡的人面臨跟布德鲁斯村非常相似的困境', '我們實際上是在做社會裡的道德選擇說我們不要珊瑚礁', '但這只是整個生意裏面的一丁點。', '什麼會令你恐懼?', '要有這麼多直行的機率,對有個c的每種物種,或有個t的每種物種,在隨機的狀況下,是無限小的。', '沒有什麽真正的你。', '是時候突破那些錯誤的詮釋了這些詮釋把不平等歸結於個人問題卻大幅地忽略了財富的優勢', '有這麼一個\"名人錄\"。', '如果你乘飛機橫跨美國,這就相當於你的飛行高度。', '去問任何一位天文物理學。', '我會把這個當作我的科學遺產', '我們建造了一個小型的工作台高度可以調整,矮小的學生也能參與', '我出生就帶了一種罕見的視覺障礙就是\"全色盲\"我從未見過顏色我不知道顏色看起來是什麼樣子因為我眼見的世界都是灰色', '抽菸者抽完最後一根香菸之後二十分鐘,他們的心跳和血壓就會開始回到正常。', '現在,這真的是一門生意而且真有公司提供這樣的投資標的,但是,我對它有個很不一樣的興趣。', '而我們做大約30萬次', '畢竟,我的故事是從一株垂死的菠菜開始的,而這個故事越來越棒。', '但有個問題世界上每個醫生,每間醫院的管理人都看過電視,咒罵《急診室的春天》又在重播', '但正當它們炸開的時候,更多蛋白質連接到尾端並將其增長,所以它不斷地在變化才能維持原狀。']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  25%|██▌       | 1000/4000 [15:27<46:57,  1.06 step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "155.09397888183594\n",
            ",,,,,,,,,,,,,,,,,,,,的,,的,的\n",
            "['2005年十月的時候第一批的七個貸款都還清後我和matt將網站的\"試用版\"字樣移除', '大家都是如此,許多隨我們過活的故事甚至不是源於我們。', '我們得要支持這些人,他們現在不只是在拯救人命,也要靠他們,才能在衝突結束後把受傷的社區重新縫合,協助它們療癒。', '計算著所量測到水落下的概念可能使您忽然有所心得\"oh,布魯克林是多麼的大,從布魯克林到曼哈頓的距離是個明顯的例子,這東邊流域下游是如此之大。', '他選擇用這樣的結構來設計屋頂的其中一個原因,便是他驚訝的發現,你竟然可以用這樣少的材料建造這樣強壯的結構,而且只需要靠幾個點來支撐。', '把最大矩形的面積減掉最小矩形的面積。', '釷是天然產生的核燃料,在地球的地殼中,比鈾還要常見四倍。', '當我是小孩子時,明確地說是在高中時,我被告知我會困在新的世界經濟大海中,除非我懂日文。', '比較易揮發的啡色芥末種子、一些白酒,', '之後,我們的政府保證永遠全額資助。', '有往上跑的氣泡,然後最上面是用凸起的磁磚做成的泡沫。', '到了第三天,迦納變得喜怒無常', '帶著正確的工具和正確的方法到一個國家,並衝滿活力地去執行防疫工作,那麼你可以做到局部的根除,', '或許他沒有足夠的錢或許他有家庭問題或許他喜歡的女孩不喜歡他', '不過有件事情一直在困擾著我,', '你也能改變世界', '黎巴嫩人請鼓掌黎巴嫩人。', 'ah:當然,請。', '我們強調,靠單一國家的力量是沒用的你必須動員所有的國家。', '想想你的家鄉。', '這是我們的信仰,我們會對我們的信仰忠誠。', '沒錯,化身人物是一個表現真實自我的方式我們可以成為的最英勇且理想化的樣子', '我們已經騎了五個半小時,我們來到我喜愛的部份:爬坡,我愛爬坡。', '那是100年來這個城市最低的投票率,', '所以我開始尋找更有創意的方式來把科技知識介紹給學生。', '親愛的朋友們,咱們鼓起勇氣吧。', '我們一同分析了西岸局勢,挑選出100個身處險境的家庭:它們在關卡旁,在軍隊基地邊,緊挨著定居點。', '都是因為石油;這是事實你知我知天下人都知道。', '」', '但是受到創傷的人會感覺不到這種不朽感。', '你為什麼不該去惹未接觸之印地安人,這就是他們的觀點。', '如果第十版下週推出那就沒必要更新第九版', '我們先在x軸上翻轉,然後逆時針旋轉三分之一圈。', '然後你又看到一條綠色的褲子,也很不錯。', '同樣的,英國退歐的公投選舉,僅有少部份人,數百萬的英國人,參與投票,而這投票的結果,不論是退或不退,都必然顯著影響到世界各地幾十億人的生活。', '到下個世紀末,預測會有超過一億八千萬人會因氣候變遷而被迫遷移,而在南路易斯安那州,能走的人早就走了。', '就是這種建造機器的想法,讓我們能航向天空,而這個想法在往後世世代代的人心中播下種子。', '」', '我們詢問、爭論「健康照護」,但選民對「健康」的看法呢?', '愛因斯坦對史瓦西精確的解答實在印象深刻我應該期望自己有科學家這樣全心全力']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_step:  28%|██▊       | 1132/4000 [17:31<44:46,  1.07 step/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save({\n",
        "#             'epoch': epoch,\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'loss': loss,\n",
        "#             ...\n",
        "#             }, PATH)\n",
        "# model = TheModelClass(*args, **kwargs)\n",
        "# optimizer = TheOptimizerClass(*args, **kwargs)\n",
        "\n",
        "# checkpoint = torch.load(PATH)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# epoch = checkpoint['epoch']\n",
        "# loss = checkpoint['loss']\n",
        "\n",
        "# model.eval()\n",
        "# # - or -\n",
        "# model.train()"
      ],
      "metadata": {
        "id": "NAQywvDycPXJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}