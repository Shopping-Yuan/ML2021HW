{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW03_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNZgP7lEqTbE"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW2/ML2021HW2_modified.ipynb\" target=\"_parent\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDo4n9HGqTbH",
        "outputId": "60d1dc45-c720-40c5-b3f1-0c8434a3e795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1awF7pZ9Dz7X1jn1_QAiKN-_v56veCEKy\n",
            "To: /content/food-11.zip\n",
            "100% 963M/963M [00:14<00:00, 68.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "# You may choose where to download the data.\n",
        "\n",
        "# Google Drive\n",
        "!gdown --id '1awF7pZ9Dz7X1jn1_QAiKN-_v56veCEKy' --output food-11.zip\n",
        "\n",
        "# Dropbox\n",
        "# !wget https://www.dropbox.com/s/m9q6273jl3djall/food-11.zip -O food-11.zip\n",
        "\n",
        "# MEGA\n",
        "# !sudo apt install megatools\n",
        "# !megadl \"https://mega.nz/#!zt1TTIhK!ZuMbg5ZjGWzWX1I6nEUbfjMZgCmAgeqJlwDkqdIryfg\"\n",
        "\n",
        "# Unzip the dataset.\n",
        "# This may take some time.\n",
        "!unzip -q food-11.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fopoO1C4qTbJ",
        "outputId": "ece954da-eabe-436c-810a-1769909f54d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "#import pytorch\n",
        "import torch\n",
        "\n",
        "# torch.backends.cudnn: set CNN algorithmtorch.backends.cudnn\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# get the current available device ('cpu' or 'cuda')\n",
        "def get_device():\n",
        "#    return 'cpu'\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = get_device()\n",
        "print(torch.cuda.is_available())\n",
        "#set random variable\n",
        "import numpy as np\n",
        "myseed = 1\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "24NsmYkgqTbK"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# m = nn.Conv2d(1, 1, 2 , stride=2, padding=(9,9),bias = False)\n",
        "# test = torch.tensor([[[1.0,2.0,3.0],[4.0,5.0,6.0],[7.0,8.0,9.0]]])\n",
        "# print(test)\n",
        "# print(m(test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class Label_Changable_Folder(DatasetFolder):\n",
        "#     def __init__(self,root,loader,extensions,transform,target_transform,is_valid_file):\n",
        "#             super().__init__(root, loader, extensions , transform, target_transform,is_valid_file)\n",
        "#     def change_targets(self , index ,new_target):\n",
        "#       self.targets[index] = new_target\n"
      ],
      "metadata": {
        "id": "5_8etr9QAD0x"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6RhmTvYcqTbL"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import v2\n",
        "from torchvision.datasets import DatasetFolder\n",
        "from torch.utils.data import ConcatDataset\n",
        "from PIL import Image\n",
        "def get_tfm(data_info):\n",
        "    tfm = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Resize(data_info[\"size\"]),\n",
        "    ])\n",
        "    return tfm\n",
        "def get_tfm_flip(data_info):\n",
        "    tfm_flip = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "#      v2.RandomResizedCrop(size=data_info[\"size\"],scale=(0.8, 1.0), antialias=True),\n",
        "    v2.Resize(data_info[\"size\"]),\n",
        "    v2.RandomHorizontalFlip(p=1),\n",
        "    ])\n",
        "    return tfm_flip\n",
        "def food_f(mode,data_info):\n",
        "    dataset = DatasetFolder(data_info[mode][\"path\"], \\\n",
        "          loader=lambda x: Image.open(x), \\\n",
        "          extensions=\"jpg\", transform=get_tfm(data_info))\n",
        "    if mode in [\"train_origin\"]:\n",
        "      dataset_flip = DatasetFolder(data_info[mode][\"path\"], \\\n",
        "          loader=lambda x: Image.open(x), \\\n",
        "          extensions=\"jpg\", transform=get_tfm_flip(data_info))\n",
        "      dataset = ConcatDataset([dataset,dataset_flip])\n",
        "    print('Size of {} data: {}'.format(mode,len(list(dataset))))\n",
        "    return dataset\n",
        "#create a dict of functions and path w.r.t. different mode\n",
        "data_info = {\n",
        "    \"train_origin\":{\"path\":\"./food-11/training/labeled\"},\n",
        "    \"train\":{\"path\":\"./food-11/training/unlabeled\"},\n",
        "    \"val\":{\"path\":\"./food-11/validation\"},\n",
        "    \"test\":{\"path\":\"./food-11/testing\"},\n",
        "    \"size\" :(128,128)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTaGe8WoqTbM"
      },
      "outputs": [],
      "source": [
        "# tfm = v2.Compose([\n",
        "#       v2.ToImage(),\n",
        "#       v2.ToDtype(torch.float32, scale=True),\n",
        "#       v2.Resize(data_info[\"size\"]),\n",
        "#       ])\n",
        "# print(type(tfm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RImDMzdqTbM"
      },
      "outputs": [],
      "source": [
        "# x = 500# x = int(max(prep))\n",
        "# print(list(food_f(\"val\"))[x][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OIekqubSqTbN"
      },
      "outputs": [],
      "source": [
        "h_paras = {\n",
        "    # maximum number of epochs\n",
        "    'n_epochs': 100,\n",
        "    # mini-batch size for dataloader\n",
        "    'batch_size': 128,\n",
        "    # optimization algorithm (optimizer in torch.optim)\n",
        "    'optimizer': 'Adam',\n",
        "    # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "    'optim_hparas': {\n",
        "        # learning rate of Adam\n",
        "        'lr': 0.0003,\n",
        "        \"weight_decay\" : 1e-5\n",
        "    },\n",
        "    # your model will be saved here\n",
        "    'save_path': './model.pth',\n",
        "    'early_stop': 5,\n",
        "    'threshold': 0.8,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5ktb7bVqTbN",
        "outputId": "093eb053-7a23-4a79-ad96-eba961ec1f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of train_origin data: 6160\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Construct data loaders.\n",
        "train_loader = DataLoader(food_f(\"train_origin\",data_info), batch_size=h_paras[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n",
        "valid_loader = DataLoader(food_f(\"val\",data_info), batch_size=h_paras[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(food_f(\"test\",data_info), batch_size=h_paras[\"batch_size\"], shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IGIUgFEqTbN",
        "outputId": "30056e4f-c5c4-456d-92e6-920cbf1dd7e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 3, 128, 128])\n",
            "torch.Size([16, 3, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# for i , t in enumerate(valid_set):\n",
        "#     if i ==0:\n",
        "#         test_t = t[0]\n",
        "# #        print(type(t[0]),t[0].size())\n",
        "#         print(test_t.size())\n",
        "#         x = test_t[list(range(16))]\n",
        "#         print(x.size())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import copy\n",
        "# a = [1,[2,3]]\n",
        "# b =  copy.deepcopy(a)\n",
        "# b[1][1] = 2\n",
        "# print(a[1][1])"
      ],
      "metadata": {
        "id": "v_x708ltotqJ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_uSxJ1bqTbO"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        # The arguments for commonly used modules:\n",
        "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
        "\n",
        "        # input image size: [3, 32, 32]\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(4, 4, 0),\n",
        "        )\n",
        "        self.flat_layer = nn.Flatten()\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(int(256 * data_info[\"size\"][0]/16 * data_info[\"size\"][1]/16), 256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(256, 11)\n",
        "        )\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "    def forward(self, x):\n",
        "        # input (x): [batch_size, 3, 128, 128]\n",
        "        # output: [batch_size, 11]\n",
        "\n",
        "        # Extract features by convolutional layers.\n",
        "        x = self.cnn_layers(x)\n",
        "\n",
        "        # The extracted feature map must be flatten before going to fully-connected layers.\n",
        "        x = self.flat_layer(x)\n",
        "\n",
        "        # The features are transformed by fully-connected layers to obtain the final logits.\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "    def cal_loss(self, pred, target):\n",
        "        return self.criterion(pred, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N08f_-X_qTbO"
      },
      "outputs": [],
      "source": [
        "#%conda install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class Semi_Dataset_preparation(Dataset):\n",
        "  def __init__(self,mode,datas,labels):\n",
        "      self.mode = mode\n",
        "      self.datas = torch.FloatTensor(datas)\n",
        "      self.labels = torch.LongTensor(labels)\n",
        "      self.dim = self.datas.shape[0]\n",
        "      print('Finished reading the {mode} set of Dataset ({len} samples found, each dim = {dim})'\n",
        "              .format(mode = self.mode, len =len(self.datas), dim=self.dim))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      return self.datas[index], self.labels[index]\n",
        "  def __len__(self):\n",
        "      # Returns the size of the dataset\n",
        "      return len(self.datas)"
      ],
      "metadata": {
        "id": "eLvQxom-Yw6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import ConcatDataset\n",
        "def get_pseudo_labels(model, training_loader , data_origin , label_origin ,h_paras , epoch):\n",
        "    threshold = h_paras[\"threshold\"]\n",
        "    softmax = nn.Softmax(dim=-1)\n",
        "    model.eval()\n",
        "    # Construct a data loader.\n",
        "\n",
        "    train_unlabeled_loader = \\\n",
        "    DataLoader(food_f(\"train\",data_info), \\\n",
        "               batch_size=h_paras[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "    datas = np.array([])\n",
        "    labels = np.array([])\n",
        "    label_dict = {}\n",
        "    update = False\n",
        "    for batch in tqdm(train_unlabeled_loader):\n",
        "        img = batch[0]\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            img_d = img.to(device)\n",
        "            prep = model(img_d)\n",
        "        predicts = softmax(prep)\n",
        "        max_prob , max_column_index = predicts.max(dim = -1)\n",
        "        max_prob = max_prob.tolist()\n",
        "        max_column_index = max_column_index.tolist()\n",
        "        for row_index in range(len(predicts)):\n",
        "\n",
        "          if max_prob[row_index] > threshold:\n",
        "            if len(datas) ==0:\n",
        "              datas = torch.unsqueeze(img[row_index],0).numpy()\n",
        "              labels = np.array([max_column_index[row_index]])\n",
        "            else:\n",
        "              datas = np.concatenate((datas, torch.unsqueeze(img[row_index],0).numpy()), axis=0)\n",
        "              labels = np.concatenate((labels, np.array([max_column_index[row_index]])), axis=0)\n",
        "            if epoch%10 == 0:\n",
        "              label_dict[int(max_column_index[row_index])] = \\\n",
        "              label_dict.get(int(max_column_index[row_index]),0)+1\n",
        "    add_num = len(datas)\n",
        "    if epoch%10 == 0:\n",
        "      print(\"add total {} datas to training dataset, each label numbers as follow {}\".format(add_num,label_dict))\n",
        "    else :\n",
        "      print(\"add total {} datas to training dataset\".format(add_num))\n",
        "    if (add_num)>0:\n",
        "      update = True\n",
        "      datas = np.concatenate((datas, data_origin), axis=0)\n",
        "      labels = np.concatenate((labels, label_origin), axis=0)\n",
        "\n",
        "      new_train_set = Semi_Dataset_preparation(\"unlabel_training\",datas,labels)\n",
        "#      new_train_set = ConcatDataset([train_set_origin , add_set])\n",
        "      # for i in DataLoader(add_set,batch_size=h_paras[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True):\n",
        "      #   print(i)\n",
        "      # for index,i in enumerate(DataLoader(new_train_set,batch_size=h_paras[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)):\n",
        "      #   if index >25:\n",
        "      #     print(i)\n",
        "      training_loader = DataLoader(new_train_set, batch_size=h_paras[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n",
        "    model.train()\n",
        "    return training_loader , update"
      ],
      "metadata": {
        "id": "fh9ppO585cd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1,2]]).max(dim = -1)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "pIOuHYMYrhvg",
        "outputId": "55e58112-2edb-4e64-dc9f-cf1ec3444072",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.return_types.max(\n",
            "values=tensor([2]),\n",
            "indices=tensor([1]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_unlabeled_loader = \\\n",
        "#     DataLoader(food_f(\"train\",data_info), \\\n",
        "#                batch_size=h_paras[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n",
        "# for batch in tqdm(train_unlabeled_loader):\n",
        "#     img = batch[0]\n",
        "#     print(img.size())\n"
      ],
      "metadata": {
        "id": "xV1h_SFXPUL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUDbUl7lqTbO"
      },
      "outputs": [],
      "source": [
        "# from tqdm.auto import tqdm\n",
        "# from torch.utils.data import ConcatDataset\n",
        "# def get_pseudo_labels(model, threshold=0.65):\n",
        "#     # Make sure the model is in eval mode.\n",
        "#     model.eval()\n",
        "#     # Define softmax function.\n",
        "#     softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "#     # Iterate over the dataset by batches.\n",
        "#     datas = []\n",
        "#     labels = []\n",
        "#     for batch in tqdm(train_unlabeled_set):\n",
        "#         img = batch[0]\n",
        "#         # Forward the data\n",
        "#         # Using torch.no_grad() accelerates the forward process.\n",
        "#         with torch.no_grad():\n",
        "#             img_d = img.to(device)\n",
        "#             prep = model(img_d)\n",
        "\n",
        "#         # Obtain the probability distributions by applying softmax on logits.\n",
        "#         predicts = softmax(prep)\n",
        "#         for  row_index , probs in enumerate(predicts):\n",
        "#             max_prob , max_column_index = probs.max(dim = -1)\n",
        "#             if max_prob > threshold:\n",
        "#                 datas.append(img[row_index].tolist())\n",
        "#                 labels += (max_column_index.tolist())\n",
        "#     new_train_set = torch.FloatTensor(datas)\n",
        "#     new_train_label = torch.LongTensor(labels)\n",
        "#     model.train()\n",
        "#     return new_train_set,new_train_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgdT2JdtqTbP"
      },
      "outputs": [],
      "source": [
        "def train(train_loader,model_d,optimizer,device):\n",
        "    # set model to training mode\n",
        "    model_d.train()\n",
        "    total_correct_number = 0\n",
        "    train_loss_list = []\n",
        "#    print(type(train_loader))\n",
        "    # iterate through the dataloader\n",
        "    for data , label in tqdm(train_loader):\n",
        "      # move data to device (cpu/cuda)\n",
        "      data_d , label_d = data.to(device), label.to(device)\n",
        "      # forward pass (compute output tensor)\n",
        "      pred = (model_d(data_d))\n",
        "      # get the index of the class with the highest probability\n",
        "      max_prob_values, max_prob_indexs = torch.max(pred, dim = 1)\n",
        "      correct_number = (max_prob_indexs.cpu() == label_d.cpu()).sum().item()\n",
        "#      print(correct_number)\n",
        "      total_correct_number += correct_number\n",
        "      # compute loss\n",
        "      loss = model_d.cal_loss(pred , label_d)\n",
        "      # compute gradient (backpropagation)\n",
        "      loss.backward()\n",
        "      # Clip the gradient norms for stable training.\n",
        "      nn.utils.clip_grad_norm_(model_d.parameters(), max_norm=10)\n",
        "      # update model with optimizer\n",
        "      optimizer.step()\n",
        "      # set optimizer gradient to zero\n",
        "      optimizer.zero_grad()\n",
        "      train_loss_list.append(loss.detach().cpu().item())\n",
        "    acc = total_correct_number/len(train_loader.dataset)\n",
        "#    print(acc , train_loss_list)\n",
        "    return acc , train_loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEn8YBfVqTbP"
      },
      "outputs": [],
      "source": [
        "def val(val_loader,model_d,device):\n",
        "    # set model to evalutation mode\n",
        "    model_d.eval()\n",
        "    total_correct_number = 0\n",
        "    total_loss = 0\n",
        "    # iterate through the dataloader\n",
        "    for data , label in tqdm(val_loader):\n",
        "    # move data to device (cpu/cuda)\n",
        "      data_d, label_d = data.to(device), label.to(device)\n",
        "      # disable gradient calculation\n",
        "      with torch.no_grad():\n",
        "        # forward pass (compute output)\n",
        "        pred = model_d(data_d)\n",
        "        # get the index of the class with the highest probability\n",
        "        max_prob_values, max_prob_indexs = torch.max(pred, dim = 1)\n",
        "        total_correct_number += (max_prob_indexs.cpu() == label_d.cpu()).sum().item()\n",
        "        # compute loss\n",
        "        mse_loss = model_d.cal_loss(pred, label_d)\n",
        "      # accumulate loss\n",
        "      batch_size = len(data_d)\n",
        "      total_loss += mse_loss.detach().cpu().item() * batch_size\n",
        "\n",
        "    # compute averaged loss\n",
        "    totol_size = len(val_loader.dataset)\n",
        "    acc = total_correct_number/totol_size\n",
        "    avg_loss =  total_loss/totol_size\n",
        "    return acc, avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5PRQVp6qTbP"
      },
      "outputs": [],
      "source": [
        "def train_val_process(training_loader, validation_loader, data_info, model, h_paras, device , do_semi = False):\n",
        "    # Initialize a model, and put it on the device specified.\n",
        "    model_d = model.to(device)\n",
        "    model_d.device = device\n",
        "    optimizer = torch.optim.Adam(model_d.parameters(), **h_paras[\"optim_hparas\"])\n",
        "\n",
        "    # The number of training epochs.\n",
        "    n_epochs = h_paras[\"n_epochs\"]\n",
        "\n",
        "    # record training accuracy\n",
        "    acc_record = {'train': [], \"val\": []}\n",
        "    # record training loss\n",
        "    loss_record = {'train': [], \"val\": []}\n",
        "\n",
        "    # accuracy paras\n",
        "    best_acc = 0\n",
        "    # early-stoping paras\n",
        "    early_stop_cnt = 0\n",
        "    # set training_set\n",
        "    training_set_now = food_f(\"train_origin\",data_info)\n",
        "    #for semi-supervised learning\n",
        "    if do_semi == True:\n",
        "      data_origin = np.array([])\n",
        "      label_origin = np.array([])\n",
        "      for data , label in tqdm(training_loader):\n",
        "        if len(data_origin) ==0:\n",
        "          data_origin = data.numpy()\n",
        "          label_origin = np.array(label)\n",
        "        else:\n",
        "          data_origin = np.concatenate((data_origin, data.numpy()), axis=0)\n",
        "          label_origin = np.concatenate((label_origin, np.array(label)), axis=0)\n",
        "\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # ---------- Training ----------\n",
        "        train_acc , train_loss_list = train(training_loader,model_d,optimizer,device)\n",
        "        # save accuracy to acc_record['train']\n",
        "        acc_record['train'].append(train_acc)\n",
        "      # save loss to loss_record['train']\n",
        "        loss_record['train'].append(train_loss_list)\n",
        "\n",
        "        # ---------- Do semi-supervised learning ----------\n",
        "        if ((do_semi == True) & (epoch >= 5 )):\n",
        "          new_training_loader, update = get_pseudo_labels(model_d, training_loader ,data_origin, label_origin, h_paras ,epoch)\n",
        "          if update == True:\n",
        "            training_loader = new_training_loader\n",
        "\n",
        "\n",
        "        # ---------- Validation ----------\n",
        "        val_acc , val_loss = val(validation_loader,model_d,device)\n",
        "        # Print the information.\n",
        "        acc_record[\"val\"].append(val_acc)\n",
        "        # save loss to loss_record[\"val\"]\n",
        "        loss_record[\"val\"].append(val_loss)\n",
        "\n",
        "        print(' model epoch = {:4d}, train_loss = {:.4f} , val_loss = {:.4f})'\\\n",
        "        .format(epoch+1 , train_loss_list[-1] , val_loss))\n",
        "        print('train set accuracy = {:.3f}'.format(train_acc))\n",
        "\n",
        "        # ---------- Early Stop ----------\n",
        "        if val_acc > best_acc:\n",
        "          best_acc = val_acc\n",
        "          torch.save(model.state_dict(), h_paras[\"save_path\"])\n",
        "          print('saving model with acc {:.3f}'.format(best_acc))\n",
        "          early_stop_cnt = 0\n",
        "        else:\n",
        "          early_stop_cnt += 1\n",
        "        # Check early stop criteria\n",
        "        if early_stop_cnt > h_paras['early_stop']:\n",
        "            # Stop training if your model stops improving\n",
        "            # for \"h_paras['early_stop']\" epochs.\n",
        "            break\n",
        "\n",
        "    return acc_record , loss_record\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "099mNJXHqTbQ"
      },
      "outputs": [],
      "source": [
        "# Construct model\n",
        "model = Classifier()\n",
        "model_acc_record, model_loss_record = train_val_process(train_loader, valid_loader, data_info, model, h_paras, device, do_semi = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r2xj_0FqTbQ"
      },
      "outputs": [],
      "source": [
        "def test(test_loader, model_d, device):\n",
        "    # set model to evalutation mode\n",
        "    model_d.eval()\n",
        "    pred_index = []\n",
        "    pred_loss = []\n",
        "    # iterate through the dataloader\n",
        "    for imgs, incorrect_labels in tqdm(test_loader):\n",
        "      # move data to device (cpu/cuda)\n",
        "      imgs_d = imgs.to(device)\n",
        "      # disable gradient calculation\n",
        "      with torch.no_grad():\n",
        "        # forward pass (compute output)\n",
        "        pred = model_d(imgs_d)\n",
        "        # get the index of the class with the highest probability\n",
        "        max_prob_values, max_prob_indexs = torch.max(pred, dim = 1)\n",
        "        # collect prediction\n",
        "        pred_index += (max_prob_indexs.tolist())\n",
        "        pred_loss += (pred.mean(dim = 1).tolist())\n",
        "    # concatenate all predictions and convert to a numpy array\n",
        "    np_pred_index , np_pred_loss = np.array(pred_index), np.array(pred_loss)\n",
        "    pred_dict = { \"pred_index\" : np_pred_index , \"pred_loss\" : np_pred_loss }\n",
        "    return pred_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P50yBDcnqTbQ"
      },
      "outputs": [],
      "source": [
        "def save_pred(preds, file_path):\n",
        "    import pandas as pd\n",
        "    print('Saving results to {}'.format(file_path))\n",
        "    df = pd.DataFrame(preds,columns=[\"tested_positive\"])\n",
        "    df.to_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = test(test_loader, model, device)  # predict COVID-19 cases with your model\n",
        "save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
      ],
      "metadata": {
        "id": "_bY7s9ASVri4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%conda install -c conda-forge matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics\n",
        "def plot_learning_curve_loss(loss_record, title=''):\n",
        "\n",
        "    #plot1 : train loss\n",
        "    #plot2 : val loss\n",
        "    train_data = []\n",
        "    t = np.array(model_loss_record['train'])\n",
        "    for i in t:\n",
        "      train_data.append(statistics.mean(i))\n",
        "    val_data = np.array(model_loss_record['val'])\n",
        "\n",
        "    #setting index range\n",
        "    total_steps = len(train_data)\n",
        "    x_1 = range(total_steps)\n",
        "\n",
        "    x_2 = x_1[::len(train_data) // len(val_data)]\n",
        "    #figure size setting\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    #plot train data\n",
        "    plt.plot(x_1, train_data, c='tab:red', label='train')\n",
        "    #plot val data\n",
        "    plt.plot(x_2, val_data, c='tab:cyan', label='val')\n",
        "\n",
        "    plt.ylim(0.0, 4.)\n",
        "    #title & label setting\n",
        "    plt.title('Learning curve of {}'.format(title))\n",
        "    plt.xlabel('Training steps')\n",
        "    plt.ylabel('Cross Entropy loss')\n",
        "    #show legend\n",
        "    plt.legend()\n",
        "    #show plot\n",
        "    plt.show()\n",
        "def plot_learning_curve_acc(acc_record, title=''):\n",
        "\n",
        "    #plot1 : train acc\n",
        "    #plot2 : val acc\n",
        "    train_data = np.array(acc_record['train'])\n",
        "    val_data = np.array(acc_record['val'])\n",
        "\n",
        "    #setting index range\n",
        "    x_1 = range(len(train_data))\n",
        "    x_2 = range(len(val_data))\n",
        "\n",
        "    #figure size setting\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    #plot train data\n",
        "    plt.plot(x_1, train_data, c='tab:red', label='train')\n",
        "    #plot val data\n",
        "    plt.plot(x_2, val_data, c='tab:cyan', label='val')\n",
        "\n",
        "    plt.ylim(0.0, 1.0)\n",
        "    #title & label setting\n",
        "    plt.title('Learning curve of {}'.format(title))\n",
        "    plt.xlabel('Training steps')\n",
        "    plt.ylabel('Cross Entropy loss')\n",
        "    #show legend\n",
        "    plt.legend()\n",
        "    #show plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kWsS_dCRT5Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curve_loss(model_loss_record, title='deep model')\n",
        "plot_learning_curve_acc(model_acc_record, title='deep model')"
      ],
      "metadata": {
        "id": "AzPNljlMXxEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels_map = {\n",
        "#     0: \"T-Shirt\",\n",
        "#     1: \"Trouser\",\n",
        "#     2: \"Pullover\",\n",
        "#     3: \"Dress\",\n",
        "#     4: \"Coat\",\n",
        "#     5: \"Sandal\",\n",
        "#     6: \"Shirt\",\n",
        "#     7: \"Sneaker\",\n",
        "#     8: \"Bag\",\n",
        "#     9: \"Ankle Boot\",\n",
        "# }\n",
        "# figure = plt.figure(figsize=(8, 8))\n",
        "# cols, rows = 3, 3\n",
        "# for i in range(1, cols * rows + 1):\n",
        "#     sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "#     img, label = training_data[sample_idx]\n",
        "#     figure.add_subplot(rows, cols, i)\n",
        "#     plt.title(labels_map[label])\n",
        "#     plt.axis(\"off\")\n",
        "#     plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "6I8wGL7NTqDe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}