{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW6/HW06_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hw6\n",
        "###Part 0 : setting & install package\n",
        "###Part 1 : data cleaning\n",
        "###Part 2 : build model\n",
        "###Part 3 : training and validation\n",
        "###Part 4 : inference"
      ],
      "metadata": {
        "id": "iR-_Z01Xkjiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###setting"
      ],
      "metadata": {
        "id": "NF6NBgMgl_Y7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dDuJitOPW69-"
      },
      "outputs": [],
      "source": [
        "setting = {\n",
        "# information of the path of dataset\n",
        "\"documents\" : {\n",
        "    \"workspace_dir\" : \"/content/drive/MyDrive/HW6_data\",\n",
        "    \"log_doc_path\" : \"/log\",\n",
        "    \"ckpt_file_path\" : \"/model\",\n",
        "    \"model_temp_save_path\" : \"/temporary_model\",\n",
        "    \"out_doc_path\" : \"/sn_gan_out\",\n",
        "},\n",
        "\"dataset\" : {\n",
        "},\n",
        "\"dataloader\" : {\n",
        "    \"batch_size\" : 64,\n",
        "    \"num_workers\" : 2,\n",
        "    \"shuffle\" : True,\n",
        "},\n",
        "# model setting\n",
        "\"model\" : {\n",
        "    \"sample_number\" : 100,\n",
        "    \"generator_in_dimension\" : 100\n",
        "},\n",
        "# setting in training and validation process ,\n",
        "# including optimization setting.\n",
        "\"training_hparas\" : {\n",
        "    \"total_epoch\" : 50, # 50\n",
        "    \"generator_step\" : 5, # 5\n",
        "    \"temp_save_epoch\" : 1,\n",
        "    \"wgan_clip_value\" : 0.01,\n",
        "    },\n",
        "\"generator_optimizer\" : {\n",
        "    \"lr\" : 1e-4,\n",
        "    \"betas\" : (0.5, 0.999)\n",
        "    },\n",
        "\"discriminator_optimizer\" : {\n",
        "    \"lr\" : 1e-4,\n",
        "    \"betas\" : (0.5, 0.999)\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###install package"
      ],
      "metadata": {
        "id": "LRia-UVamFIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qqdm\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "OYFCGoI7XgrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 : download dataset from drive to google colab\n",
        "# original dataset is in \"https://mega.nz/#!vEcTCISJ!3Rw0eHTZWPpdHBTbQEqBDikDEdFPr7fI8WxaXK9yZ9U\"\n",
        "\n",
        "workspace_dir = \"/content/drive/MyDrive/HW6_data\"\n",
        "rawdata_file_path = workspace_dir + \"/crypko_data.zip\"\n",
        "unzip_path = \"/content/unzip_image/\"\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive_path = \"/content/drive\"\n",
        "drive.mount(drive_path)\n",
        "\n",
        "# step 2 : unzip dataset\n",
        "import zipfile\n",
        "with zipfile.ZipFile(rawdata_file_path, 'r') as zip_f:\n",
        "    zip_f.extractall(unzip_path)"
      ],
      "metadata": {
        "id": "P9wHCD-ZYDl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def seeds(seed):\n",
        "    # Python built-in random module\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Torch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "8V1sAzE2YQXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import os\n",
        "import glob\n",
        "import torchvision\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "class CrypkoDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.imgs = glob.glob(os.path.join(unzip_path,\"faces\",\"*\"))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = torchvision.io.read_image(self.imgs[index])\n",
        "    return self.crypko_transform(img)\n",
        "    # return img\n",
        "\n",
        "  def crypko_transform(self,img):\n",
        "    compose = [\n",
        "          v2.ToPILImage(),\n",
        "          v2.Resize((64, 64)),\n",
        "          v2.ToTensor(),\n",
        "          v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "          ]\n",
        "    transform =  v2.Compose(compose)\n",
        "    return transform(img)\n",
        "\n",
        "def get_dataloader(dataset,batch_size,shuffle,num_workers):\n",
        "  return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "nfWGElInr_bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CrypkoDataset()\n",
        "images = [dataset[i] for i in range(100)]\n",
        "# print(images[0][0][0][0],type(images[0][0][0][0].item()))\n",
        "grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
        "plt.figure(figsize=(10,10))\n",
        "out = grid_img.permute(1, 2, 0)\n",
        "plt.imshow(out)\n",
        "plt.show()\n",
        "\n",
        "# from qqdm.notebook import qqdm\n",
        "# loader = DataLoader(dataset, batch_size=12, shuffle=True, num_workers=0)\n",
        "# step = 0\n",
        "# for data in qqdm(loader):\n",
        "#   if step<2:\n",
        "#     images = [data[i] for i in range(10)]\n",
        "#     grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
        "#     plt.figure(figsize=(10,10))\n",
        "#     out = grid_img.permute(1, 2, 0)\n",
        "#     plt.imshow(out)\n",
        "#     plt.show()\n",
        "#   step += 1"
      ],
      "metadata": {
        "id": "yvdMuE6dkh9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "def weights_init(layer):\n",
        "    classname = layer.__class__.__name__\n",
        "    # if hasattr(layer,\"weight\"):\n",
        "    #   print(classname,layer.weight.shape)\n",
        "    if classname.find('Conv') != -1:\n",
        "        layer.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        layer.weight.data.normal_(1.0, 0.02)\n",
        "        layer.bias.data.fill_(0)\n",
        "class Spectral_Normalization(nn.Module):\n",
        "    def __init__(self,layer,eps = 1e-12,power_iteration = 1):\n",
        "        super().__init__()\n",
        "        # if hasattr(layer,\"weight\"):\n",
        "        self.layer = layer\n",
        "        self.classname = self.layer.__class__.__name__\n",
        "        self.eps = eps\n",
        "        self.power_iteration = power_iteration\n",
        "        # print(f\"apply spectral normalization to layer {classname}\")\n",
        "\n",
        "        # reshape weight to 2D-matrix\n",
        "        self.weight_2D = self.layer.weight.view(self.layer.weight.shape[0],-1)\n",
        "\n",
        "        # u , v is SVD matrixs of weight\n",
        "        self.u = torch.empty(self.weight_2D.shape[0]).normal_(0, 1)\n",
        "        self.v = torch.mv(torch.transpose(self.weight_2D, 0, 1),self.u)\n",
        "        # compute w_max_sv using u,v\n",
        "        self.w_max_sv = self.u@(torch.mv(self.weight_2D,self.v)).data\n",
        "\n",
        "        self.layer.register_buffer(self.classname+'_u', self.u)\n",
        "        self.layer.register_buffer(self.classname+'_v', self.v)\n",
        "        self.layer.register_buffer(self.classname+\"w\",self.w_max_sv)\n",
        "        # print(f\"u shape {u.shape}\")\n",
        "        # print(f\"v shape {v.shape}\")\n",
        "    def forward(self,x):\n",
        "        self.get_new_u_v()\n",
        "        self.w_max_sv = self.u@(torch.mv(self.weight_2D,self.v)).data\n",
        "        for weight in self.layer.weight:\n",
        "          weight = weight/self.w_max_sv\n",
        "        return(self.layer.forward(x))\n",
        "\n",
        "    def get_new_u_v(self):\n",
        "        for times in range(self.power_iteration):\n",
        "          self.v = torch.mv(torch.transpose(self.weight_2D, 0, 1),self.u)\n",
        "          self.u = torch.mv(self.weight_2D,self.v)\n",
        "          self.u.data = self.u.data/(self.u.data.norm() + self.eps)\n",
        "          self.v.data = self.v.data/(self.v.data.norm() + self.eps)\n",
        "\n",
        "\n",
        "class Print(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "    def forward(self,x):\n",
        "      print(x.shape)\n",
        "      return x\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_dim, dim=64):\n",
        "      super().__init__()\n",
        "      self.indim = in_dim\n",
        "      self.dim = dim\n",
        "      def dconv_bn_relu(in_dim, out_dim):\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_dim, out_dim, 5, 2,\n",
        "                          padding=2, output_padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_dim),\n",
        "                nn.ReLU())\n",
        "      self.l1 = nn.Sequential(\n",
        "            nn.Linear(self.indim, self.dim * 8 * 4 * 4, bias=False),\n",
        "            nn.BatchNorm1d(self.dim * 8 * 4 * 4),\n",
        "            nn.ReLU())\n",
        "      self.l2_5 = nn.Sequential(\n",
        "            dconv_bn_relu(self.dim * 8, self.dim * 4),\n",
        "            dconv_bn_relu(self.dim * 4, self.dim * 2),\n",
        "            dconv_bn_relu(self.dim * 2, self.dim),\n",
        "            nn.ConvTranspose2d(self.dim, 3, 5, 2, padding=2, output_padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "      self.apply(weights_init)\n",
        "    def forward(self,x):\n",
        "      y = self.l1(x)\n",
        "      batch_size = y.size(0)\n",
        "      dconv_dim = self.dim*8\n",
        "      y = y.view(batch_size,dconv_dim,4,4)\n",
        "      y = self.l2_5(y)\n",
        "      return y\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Input shape: (N, 3, 64, 64)\n",
        "    Output shape: (N, )\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        def conv_bn_lrelu(in_dim, out_dim):\n",
        "            return nn.Sequential(\n",
        "                Spectral_Normalization(nn.Conv2d(in_dim, out_dim, 5, 2, 2)),\n",
        "                Spectral_Normalization(nn.BatchNorm2d(out_dim)),\n",
        "                nn.LeakyReLU(0.2))\n",
        "\n",
        "        \"\"\" Medium: Remove the last sigmoid layer for WGAN. \"\"\"\n",
        "        self.ls = nn.Sequential(\n",
        "            Spectral_Normalization(nn.Conv2d(in_dim, dim, 5, 2, 2)),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            conv_bn_lrelu(dim, dim * 2),\n",
        "            conv_bn_lrelu(dim * 2, dim * 4),\n",
        "            conv_bn_lrelu(dim * 4, dim * 8),\n",
        "            Spectral_Normalization(nn.Conv2d(dim * 8, 1, 4)),\n",
        "            # nn.Sigmoid(),\n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.ls(x)\n",
        "        y = y.view(-1)\n",
        "        return y"
      ],
      "metadata": {
        "id": "x-GS6dFHo4LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchinfo import summary\n",
        "model_G = Generator(100)\n",
        "# print(summary(model_G,(32,100)))\n",
        "# input = torch.rand(32,100)\n",
        "# print(model_G(input).shape)\n",
        "model_D = Discriminator(3)\n",
        "print(model_D.state_dict().keys())\n",
        "\n",
        "# print(summary(model_D,(32,3,64,64)))\n",
        "# input = torch.rand(32,3,64,64)\n",
        "# print(model_D(input).shape)"
      ],
      "metadata": {
        "id": "yZh-Rnqu637V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(list(setting[\"discriminator_optimizer\"].values())[0])"
      ],
      "metadata": {
        "id": "o9d16rSM27Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GP_term(G_out,ref,D,l=10):\n",
        "  epison = torch.rand((1)).item()\n",
        "  G_out_detach = G_out.detach()\n",
        "  ref_detach = ref.detach()\n",
        "  random_point = epison*ref_detach + (1-epison)*G_out_detach\n",
        "  random_point.requires_grad = True\n",
        "  D.zero_grad()\n",
        "  for p in D.parameters():\n",
        "    p.requires_grad = False\n",
        "  diff = D(random_point)\n",
        "  diff.backward(torch.ones(diff.size(0)))\n",
        "  grad = random_point.grad\n",
        "  for p in D.parameters():\n",
        "    p.requires_grad = True\n",
        "  grad_flat = grad.view(32,-1)\n",
        "  return(l*torch.square(torch.torch.norm(grad_flat,dim = -1)-1))\n",
        "\n",
        "# from torchinfo import summary\n",
        "# dataset = CrypkoDataset()\n",
        "# ref = torch.stack([dataset[i] for i in range(32)])\n",
        "# print(ref.shape)\n",
        "# model_G = Generator(100)\n",
        "# G_input = torch.rand(32,100)\n",
        "# G_out = model_G(input)\n",
        "# print(model_G(input).shape)\n",
        "# model_D = Discriminator(3)\n",
        "# GP_term(G_out,ref,model_D,l=10).shape"
      ],
      "metadata": {
        "id": "BmzH7jpcjBED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from qqdm.notebook import qqdm\n",
        "def train_val(documents,dataset,dataloader,model,training_hparas,\n",
        "        generator_optimizer,discriminator_optimizer):\n",
        "\n",
        "  #doc paths\n",
        "  paths = list(documents.values())\n",
        "  workspace_dir, log_doc_path, ckpt_file_path, model_temp_save_path, out_doc_path = paths\n",
        "  print(workspace_dir)\n",
        "  for path in paths[1:]:\n",
        "    full_path = workspace_dir+path\n",
        "    os.makedirs(full_path,exist_ok=True)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  # Dataset\n",
        "  dataset = CrypkoDataset()\n",
        "  # DataLoader\n",
        "  # data_loader = get_dataloader(dataset, **dataloader)\n",
        "  data_loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "  # test sample\n",
        "  sample_number, generator_in_dimension = list(model.values())\n",
        "  sample = torch.randn(sample_number, generator_in_dimension)\n",
        "  sample = sample.to(device)\n",
        "\n",
        "  # Model\n",
        "  G = Generator(in_dim = generator_in_dimension).to(device)\n",
        "  D = Discriminator(3).to(device)\n",
        "  G.train()\n",
        "  D.train()\n",
        "\n",
        "  # Loss\n",
        "  criterion = nn.BCELoss()\n",
        "  # set seeds\n",
        "  seeds(2021)\n",
        "  \"\"\" Medium: Use RMSprop for WGAN. \"\"\"\n",
        "  # Optimizer\n",
        "  # opt_D = torch.optim.Adam(D.parameters(), **discriminator_optimizer)\n",
        "  # opt_G = torch.optim.Adam(G.parameters(), **generator_optimizer)\n",
        "  opt_D = torch.optim.RMSprop(D.parameters(), lr = list(discriminator_optimizer.values())[0])\n",
        "  opt_G = torch.optim.RMSprop(G.parameters(), lr = list(generator_optimizer.values())[0])\n",
        "\n",
        "  # training_hparas\n",
        "  total_epoch, generator_step, temp_save_epoch, wgan_clip_value = \\\n",
        "  list(training_hparas.values())\n",
        "\n",
        "\n",
        "  steps = 0\n",
        "  for e, epoch in enumerate(range(total_epoch)):\n",
        "      progress_bar = qqdm(data_loader)\n",
        "      for i, data in enumerate(progress_bar):\n",
        "          imgs = data\n",
        "          imgs = imgs.to(device)\n",
        "          batch_size = imgs.size(0)\n",
        "          # if steps % 100 == 0:\n",
        "          #   images = [data[j] for j in range(int(batch_size/5))]\n",
        "          #   grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
        "          #   plt.figure(figsize=(10,10))\n",
        "          #   out = grid_img.permute(1, 2, 0)\n",
        "          #   plt.imshow(out)\n",
        "          #   plt.show()\n",
        "\n",
        "          # ============================================\n",
        "          #  Train D\n",
        "          # ============================================\n",
        "          train_sample = torch.randn(batch_size, generator_in_dimension)\n",
        "          train_sample = train_sample.to(device)\n",
        "          r_imgs = imgs.to(device)\n",
        "          f_imgs = G(train_sample)\n",
        "\n",
        "          \"\"\" Medium: Use WGAN Loss. \"\"\"\n",
        "          # Label\n",
        "          # r_label = torch.ones((batch_size)).to(device)\n",
        "          # f_label = torch.zeros((batch_size)).to(device)\n",
        "          # Model forwarding\n",
        "          # r_logit = D(r_imgs.detach())\n",
        "          # f_logit = D(f_imgs.detach())\n",
        "\n",
        "          # Compute the loss for the discriminator.\n",
        "          # r_loss = criterion(r_logit, r_label)\n",
        "          # f_loss = criterion(f_logit, f_label)\n",
        "          # loss_D = (r_loss + f_loss) / 2\n",
        "\n",
        "          # WGAN Loss\n",
        "          loss_D = -1*torch.mean(D(r_imgs)) + torch.mean(D(f_imgs))\n",
        "          # loss_D = -1*torch.mean(D(r_imgs)) + torch.mean(D(f_imgs)) + \\\n",
        "          # WGAN-GP Loss\n",
        "          # oss_D = -1*torch.mean(D(r_imgs)) + torch.mean(D(f_imgs)) +\n",
        "          #      torch.mean(GP_term(f_imgs,r_imgs,D,10))\n",
        "\n",
        "\n",
        "          # Model backwarding\n",
        "          D.zero_grad()\n",
        "          loss_D.backward()\n",
        "\n",
        "          # Update the discriminator.\n",
        "          opt_D.step()\n",
        "\n",
        "          \"\"\" Medium: Clip weights of discriminator. \"\"\"\n",
        "          for p in D.parameters():\n",
        "             p.data.clamp_(-1*wgan_clip_value, wgan_clip_value)\n",
        "\n",
        "          # ============================================\n",
        "          #  Train G\n",
        "          # ============================================\n",
        "          if steps % generator_step == 0:\n",
        "              D.zero_grad()\n",
        "              for p in D.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "              # Generate some fake images.\n",
        "              train_sample = torch.randn(batch_size, generator_in_dimension)\n",
        "              train_sample = train_sample.to(device)\n",
        "              f_imgs = G(train_sample)\n",
        "\n",
        "              # Model forwarding\n",
        "              # f_logit = D(f_imgs)\n",
        "\n",
        "\n",
        "              \"\"\" Medium: Use WGAN Loss\"\"\"\n",
        "              # Compute the loss for the generator.\n",
        "              # loss_G = criterion(f_logit, r_label)\n",
        "              # WGAN Loss\n",
        "\n",
        "              loss_G = -torch.mean(D(f_imgs))\n",
        "\n",
        "              # Model backwarding\n",
        "              G.zero_grad()\n",
        "              loss_G.backward()\n",
        "              # for name, paras in D.named_parameters():\n",
        "              #   print(paras.grad)\n",
        "              # Update the generator.\n",
        "              opt_G.step()\n",
        "\n",
        "              for p in D.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "          steps += 1\n",
        "\n",
        "          # Set the info of the progress bar\n",
        "          #   Note that the value of the GAN loss is not directly related to\n",
        "          #   the quality of the generated images.\n",
        "          progress_bar.set_infos({\n",
        "              'Loss_D': round(loss_D.item(), 4),\n",
        "              'Loss_G': round(loss_G.item(), 4),\n",
        "              'Epoch': e+1,\n",
        "              'Step': steps,\n",
        "          })\n",
        "\n",
        "      G.eval()\n",
        "      f_imgs_sample = (G(sample).data + 1) / 2.0\n",
        "      filename = workspace_dir + log_doc_path + f'Wgan_GP_Epoch_{epoch+1:03d}.jpg'\n",
        "      torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n",
        "      print(f' | Save some samples to {filename}.')\n",
        "\n",
        "      # Show generated images in the jupyter notebook.\n",
        "      grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n",
        "      plt.figure(figsize=(10,10))\n",
        "      plt.imshow(grid_img.permute(1, 2, 0))\n",
        "      plt.show()\n",
        "      G.train()\n",
        "\n",
        "      if (e+1) % 5 == 0 or e == 0:\n",
        "          # Save the checkpoints.\n",
        "          torch.save(G.state_dict(), workspace_dir + ckpt_file_path + 'G_GP.pth')\n",
        "          torch.save(D.state_dict(), workspace_dir + ckpt_file_path + 'D_GP.pth')"
      ],
      "metadata": {
        "id": "75F27YzEpShx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val(**setting)"
      ],
      "metadata": {
        "id": "RrvrE0K4lYnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def inference(documents,model,**kwargs):\n",
        "  # documents\n",
        "    paths = list(documents.values())\n",
        "    workspace_dir, log_doc_path, ckpt_file_path, model_temp_save_path, out_doc_path = paths\n",
        "\n",
        "  # Model\n",
        "    sample_number, generator_in_dimension = list(model.values())\n",
        "    G = Generator(in_dim = generator_in_dimension)\n",
        "    G.load_state_dict(torch.load(workspace_dir + ckpt_file_path + 'G_GP.pth'))\n",
        "    G.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    G.to(device)\n",
        "    # Generate 1000 images and make a grid to save them.\n",
        "    sample_number = 1000\n",
        "    sample = torch.randn(sample_number, generator_in_dimension).to(device)\n",
        "    imgs_sample = (G(sample).data + 1) / 2.0\n",
        "\n",
        "    # log and save image\n",
        "    log_dir = workspace_dir+log_doc_path\n",
        "    filename = os.path.join(log_dir, 'result.jpg')\n",
        "    torchvision.utils.save_image(imgs_sample, filename, nrow=10)\n",
        "\n",
        "    # Show 32 of the images.\n",
        "    grid_img = torchvision.utils.make_grid(imgs_sample[:32].cpu(), nrow=10)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(grid_img.permute(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "    # Save the generated images.\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    for i in range(1000):\n",
        "        torchvision.utils.save_image(imgs_sample[i], f'output/{i+1}.jpg')\n"
      ],
      "metadata": {
        "id": "Jg0neoFboZL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference(**setting)"
      ],
      "metadata": {
        "id": "z6B1VNQ2whh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compress the images.\n",
        "%cd output\n",
        "!tar -zcf ../images.tgz *.jpg\n",
        "%cd .."
      ],
      "metadata": {
        "id": "jIiKQFxjtV67"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}