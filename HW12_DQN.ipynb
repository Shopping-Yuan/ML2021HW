{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXcUpFj2FJmyGhHcgyO4mm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5631fa9f94a448758b34263826d7ae42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67130c845a6b4b04bf71a0314521c4e1",
              "IPY_MODEL_8d4e8ca991544572a6a5fd51a4ca93ef",
              "IPY_MODEL_2b76eb22950c49b7a7fdbe55a4425136"
            ],
            "layout": "IPY_MODEL_c0c01072a7d94472ab9f8f9701ebc0ba"
          }
        },
        "67130c845a6b4b04bf71a0314521c4e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebebec1e912b48eb820ea67249bb0409",
            "placeholder": "​",
            "style": "IPY_MODEL_b9951ccfbfb14505a5db597258dab31d",
            "value": "Total: -196.2, Final: -100.0:  40%"
          }
        },
        "8d4e8ca991544572a6a5fd51a4ca93ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04106722ecbc4c8b81e211a28529e14a",
            "max": 800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08c2f9278be84ceca10b3553c154ab41",
            "value": 324
          }
        },
        "2b76eb22950c49b7a7fdbe55a4425136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bd7ed5a65414f55a9de08a6339875ae",
            "placeholder": "​",
            "style": "IPY_MODEL_eb1cc01000464476971f0043106e78e1",
            "value": " 324/800 [03:47&lt;08:31,  1.07s/it]"
          }
        },
        "c0c01072a7d94472ab9f8f9701ebc0ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebebec1e912b48eb820ea67249bb0409": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9951ccfbfb14505a5db597258dab31d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04106722ecbc4c8b81e211a28529e14a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08c2f9278be84ceca10b3553c154ab41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bd7ed5a65414f55a9de08a6339875ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb1cc01000464476971f0043106e78e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_vscode_branch/HW12_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "「環境」給予的 reward 大致是這樣計算：\n",
        "- 小艇墜毀得到 -100 分\n",
        "- 小艇在黃旗幟之間成功著地則得 100~140 分\n",
        "- 噴射主引擎（向下噴火）每次 -0.3 分\n",
        "- 小艇最終完全靜止則再得 100 分\n",
        "- 小艇每隻腳碰觸地面 +10 分\n",
        "- 噴射左或右引擎（順時針或逆時針旋轉）每次 -0.03 分"
      ],
      "metadata": {
        "id": "t8L-I5fzx0HK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setting"
      ],
      "metadata": {
        "id": "edOt-bsF5971"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###Install Package\n",
        "\n"
      ],
      "metadata": {
        "id": "-LWA0Saa_RG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update\n",
        "!apt install python3-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install swig\n",
        "!pip install gym[box2d]\n",
        "!pip install tqdm\n",
        "!pip install box2d-kengz"
      ],
      "metadata": {
        "id": "17g9RvkL_P_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e90e93d2-ad3c-49c6-acd5-828cc82e3d13"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Connecting to security.ubuntu.com (91.189.91.8\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Connecting to security.ubuntu.com (91.189.91.8\u001b[0m\r                                                                                                    \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [2 InRelease 1,581 B/1,581 B 10\u001b[0m\u001b[33m\r                                                                                                    \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                    \rHit:3 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                    \rHit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                    \rGet:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [5 InRelease 14.2 kB/110 kB 13%] [Waiting for headers]\u001b[0m\r                                                                               \rHit:6 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [5 InRelease 14.2 kB/110 kB 13%] [Waiting for headers]\u001b[0m\r                                                                               \rHit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [5 InRelease 14.2 kB/110 kB 13%] [Waiting for headers]\u001b[0m\r                                                                               \rHit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [805 kB]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,124 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,969 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,690 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,081 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.7 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,357 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,173 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [61.3 kB]\n",
            "Fetched 11.5 MB in 3s (4,030 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  freeglut3 libglu1-mesa\n",
            "Suggested packages:\n",
            "  libgle3 python3-numpy\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 libglu1-mesa python3-opengl\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 824 kB of archives.\n",
            "After this operation, 8,092 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-opengl all 3.1.5+dfsg-1 [605 kB]\n",
            "Fetched 824 kB in 1s (998 kB/s)\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "(Reading database ... 121752 files and directories currently installed.)\n",
            "Preparing to unpack .../freeglut3_2.8.1-6_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-6) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package python3-opengl.\n",
            "Preparing to unpack .../python3-opengl_3.1.5+dfsg-1_all.deb ...\n",
            "Unpacking python3-opengl (3.1.5+dfsg-1) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-6) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up python3-opengl (3.1.5+dfsg-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 7,813 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.10 [28.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.10 [863 kB]\n",
            "Fetched 7,813 kB in 1s (6,818 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 124836 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.10_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.10_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (4.2.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376134 sha256=6bff1ce01d56f0eb1ab4adb608f0c7511ec00bfca2d71d5424f3b494a0d9bc5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed box2d-py-2.3.5 pygame-2.1.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Collecting box2d-kengz\n",
            "  Downloading Box2D-kengz-2.3.3.tar.gz (425 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.4/425.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-kengz\n",
            "  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-kengz: filename=Box2D_kengz-2.3.3-cp310-cp310-linux_x86_64.whl size=2394324 sha256=af5dd71efcfb7b053ff29f7674e7863e2581c8d6a1a0451662e5a9ee91df6a4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/a3/5f/6396406aa0163da86c2a8d28304a120b55cfa98363654d853b\n",
            "Successfully built box2d-kengz\n",
            "Installing collected packages: box2d-kengz\n",
            "Successfully installed box2d-kengz-2.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Set Seed"
      ],
      "metadata": {
        "id": "7f4Fa25H_1CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "seed = 543\n",
        "def fix(env, seed):\n",
        "  env.seed(seed)\n",
        "  env.action_space.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "6NGiR0Cy_y5c"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pretrain\n",
        "######Using policy gradient to pretrain model to get a better Q."
      ],
      "metadata": {
        "id": "Jyx8R8w5uxDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class PolicyGradientNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(8, 16)\n",
        "        self.fc2 = nn.Linear(16, 32)\n",
        "        self.fc3 = nn.Linear(32, 16)\n",
        "        self.fc4 = nn.Linear(16, 8)\n",
        "        self.fc5 = nn.Linear(8, 4)\n",
        "\n",
        "    def forward(self, state):\n",
        "        hid = torch.tanh(self.fc1(state))\n",
        "        hid = torch.tanh(self.fc2(hid))\n",
        "        hid = torch.tanh(self.fc3(hid))\n",
        "        hid = torch.tanh(self.fc4(hid))\n",
        "        return self.fc5(hid)"
      ],
      "metadata": {
        "id": "TW6428fiF3Bi"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "class PolicyGradientAgent():\n",
        "\n",
        "    def __init__(self, network):\n",
        "        self.network = network\n",
        "        self.optimizer = optim.SGD(self.network.parameters(), lr=0.001)\n",
        "\n",
        "    def learn(self, log_probs, rewards):\n",
        "\n",
        "        loss = (-log_probs * rewards).sum()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def sample(self, state):\n",
        "        action_prob = F.softmax(self.network(torch.FloatTensor(state)), dim=-1)\n",
        "        action_dist = Categorical(action_prob)\n",
        "        action = action_dist.sample()\n",
        "        log_prob = action_dist.log_prob(action)\n",
        "        return action.item(), log_prob"
      ],
      "metadata": {
        "id": "9j5D_rw-F8uh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "def policy_gradient_training(env,agent):\n",
        "    agent.network.train()\n",
        "    EPISODE_PER_BATCH = 5\n",
        "    NUM_BATCH = 800\n",
        "    avg_total_rewards, avg_final_rewards = [], []\n",
        "\n",
        "    prg_bar = tqdm(range(NUM_BATCH))\n",
        "    for batch in prg_bar:\n",
        "\n",
        "        log_probs, rewards = [], []\n",
        "        total_rewards, final_rewards = [], []\n",
        "\n",
        "        for episode in range(EPISODE_PER_BATCH):\n",
        "\n",
        "            state = env.reset()\n",
        "            total_reward, total_step = 0, 0\n",
        "            seq_rewards = []\n",
        "            while True:\n",
        "\n",
        "                action, log_prob = agent.sample(state) # at(int) , log(at|st)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "                total_step += 1\n",
        "\n",
        "                seq_rewards.append(0)\n",
        "                len_of_seq_reward = len(seq_rewards)\n",
        "                seq_rewards = [seq_rewards[index] +(0.99)**(len_of_seq_reward-(index+1))\\\n",
        "                        *reward for index in range(len_of_seq_reward)]\n",
        "                if done:\n",
        "                    rewards+=seq_rewards\n",
        "                    final_rewards.append(reward)\n",
        "                    total_rewards.append(total_reward)\n",
        "                    break\n",
        "\n",
        "        avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "        avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
        "        avg_total_rewards.append(avg_total_reward)\n",
        "        avg_final_rewards.append(avg_final_reward)\n",
        "        prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
        "\n",
        "        rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)\n",
        "        agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
        "    print(f\"Average total reward of last {EPISODE_PER_BATCH*10} episode of pretraining \"+\n",
        "       + f\"is {sum(avg_total_rewards[-10:])/10.}\")\n",
        "    print(f\"Average final reward of last {EPISODE_PER_BATCH*10} episode of pretraining \"+\n",
        "       + f\"is {sum(avg_final_rewards[-10:])/10.}\")"
      ],
      "metadata": {
        "id": "BQDHq0A5GFhZ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "def main_pre_train(env,device):\n",
        "  network = PolicyGradientNetwork()\n",
        "  agent = PolicyGradientAgent(network)\n",
        "  policy_gradient_training(env,agent)\n",
        "  torch.save(agent.network.state_dict(), \"/content/pretrain_model.pth\")\n"
      ],
      "metadata": {
        "id": "vfzlzym9GlB7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "VIcr6Yzn9jyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Q_Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(8, 16)\n",
        "        self.fc2 = nn.Linear(16, 32)\n",
        "        self.fc3 = nn.Linear(32, 16)\n",
        "        self.fc4 = nn.Linear(16, 8)\n",
        "        self.fc5 = nn.Linear(8, 4)\n",
        "\n",
        "    def forward(self, state):\n",
        "        hid = torch.tanh(self.fc1(state))\n",
        "        hid = torch.tanh(self.fc2(hid))\n",
        "        hid = torch.tanh(self.fc3(hid))\n",
        "        hid = torch.tanh(self.fc4(hid))\n",
        "        return self.fc5(hid)"
      ],
      "metadata": {
        "id": "6DNEbp4XDWbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac635d1-bd25-40a0-c602-38ed678cff9e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import deque\n",
        "class Buffer():\n",
        "  def __init__(self, capacity):\n",
        "      self.memory = deque([], maxlen=capacity)\n",
        "  def to_memory(self, new_data):\n",
        "      self.memory.append(new_data)\n",
        "  def __len__(self):\n",
        "      return len(self.memory)\n",
        "  def sample_batch(self, batch_size):\n",
        "      return random.sample(self.memory, batch_size)"
      ],
      "metadata": {
        "id": "99JlDMppYPhx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import gym\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from collections import namedtuple\n",
        "\n",
        "class Q_Learning_Process():\n",
        "    def __init__(self, env, device, policy_net, target_net, buffer):\n",
        "        self.env = env\n",
        "        self.device = device\n",
        "        self.policy_net = policy_net.to(self.device)\n",
        "        self.target_net = target_net.to(self.device)\n",
        "        self.buffer = buffer\n",
        "\n",
        "    def _exploration(self,steps_done,state,EPS_START,EPS_END,EPS_DECAY):\n",
        "        sample = random.random()\n",
        "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "            math.exp(-1. * steps_done / EPS_DECAY)\n",
        "        if sample > eps_threshold:\n",
        "            with torch.no_grad():\n",
        "              input = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "              return self.policy_net(input).max(1).indices.detach().item()\n",
        "\n",
        "        else:\n",
        "          return self.env.action_space.sample()\n",
        "\n",
        "    def update_buffer(self,state,done,steps_done,EPS_START,EPS_END,EPS_DECAY):\n",
        "        transition = namedtuple('transition_calss',('state', 'action', 'next_state', 'reward'))\n",
        "        action = self._exploration(steps_done, state,EPS_START,EPS_END,EPS_DECAY)\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "        if not done:\n",
        "          data = transition(state,action,next_state,reward)\n",
        "          self.buffer.to_memory(data)\n",
        "        steps_done += 1\n",
        "        return reward,next_state,done,steps_done\n",
        "\n",
        "    def optimize_model(self, optimizer, LR, BATCH_SIZE, GAMMA):\n",
        "        self.policy_net.train()\n",
        "        batch = self.buffer.sample_batch(BATCH_SIZE)\n",
        "        state = torch.FloatTensor([d.state for d in batch]).to(self.device)\n",
        "        action = torch.LongTensor([d.action for d in batch]).view(-1,1).to(self.device)\n",
        "        next_state = torch.FloatTensor([d.next_state for d in batch]).to(self.device)\n",
        "        reward = torch.FloatTensor([d.reward for d in batch]).to(self.device)\n",
        "        out = self.policy_net(state)\n",
        "\n",
        "        Q = out.gather(1,action)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          Q_next_max = self.target_net(next_state).max(1).values\n",
        "\n",
        "        criterion = nn.SmoothL1Loss()\n",
        "        loss = criterion(Q,(reward + GAMMA*Q_next_max).unsqueeze(1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
        "        optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def eval(self,EPISODE_PER_BATCH):\n",
        "        total_rewards = []\n",
        "        final_rewards = []\n",
        "        for e in range(EPISODE_PER_BATCH):\n",
        "          rewards = 0\n",
        "          done = False\n",
        "          state = self.env.reset()\n",
        "          while not done:\n",
        "            input = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            action = \\\n",
        "            self.target_net(input).max(1).indices.detach().item()\n",
        "\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            rewards += reward\n",
        "\n",
        "            if done:\n",
        "              total_rewards.append(rewards)\n",
        "              final_rewards.append(reward)\n",
        "            state = next_state\n",
        "\n",
        "        return sum(total_rewards)/len(total_rewards),\\\n",
        "            sum(final_rewards)/len(total_rewards)\n",
        "    def soft_update(self,TAU):\n",
        "      agent_target_net_state_dict = self.target_net.state_dict()\n",
        "      agent_policy_net_state_dict = self.policy_net.state_dict()\n",
        "      for key in agent_policy_net_state_dict:\n",
        "          agent_target_net_state_dict[key] = \\\n",
        "          agent_policy_net_state_dict[key]*TAU + agent_target_net_state_dict[key]*(1-TAU)\n",
        "      self.target_net.load_state_dict(agent_target_net_state_dict)\n",
        "\n",
        "    def save(self, optimizer, PATH):\n",
        "        Model_Dict = {\n",
        "            \"policy_net\" : self.policy_net.state_dict(),\n",
        "            \"target_net\" : self.target_net.state_dict(),\n",
        "            \"optimizer\" : optimizer.state_dict(),\n",
        "        }\n",
        "        torch.save(Model_Dict, PATH)\n",
        "\n",
        "    def load(self, optimizer, PATH):\n",
        "        checkpoint = torch.load(PATH)\n",
        "        self.policy_net.load_state_dict(checkpoint[\"policy_net\"])\n",
        "        self.target_net.load_state_dict(checkpoint[\"target_net\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "1Z3CLDzsDiwM"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LR_Scheduler():\n",
        "    def __init__(self,LR,optimizer):\n",
        "        self.LR = LR\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self._rate = 0\n",
        "    def step(self):\n",
        "        self._step += 1\n",
        "        if self._step<=200:\n",
        "          self._rate = self.LR\n",
        "        else:\n",
        "          self._rate = self.LR * \\\n",
        "          (self._step-200) ** (-0.5)\n",
        "\n",
        "        self.optimizer.param_groups[0][\"lr\"] = self._rate\n",
        "        self.optimizer.step()\n",
        "    def zero_grad(self):\n",
        "        return self.optimizer.zero_grad()\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.optimizer.state_dict()\n",
        "\n",
        "    def load_state_dict(self,state_dict):\n",
        "        return self.optimizer.load_state_dict(state_dict)\n",
        "\n",
        "    def set_step(self,step):\n",
        "        self._step = step"
      ],
      "metadata": {
        "id": "j_8r8I601wi8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data(is_ipython,loss,train_total_rewards,train_final_rewards,eval_total_rewards,eval_final_rewards):\n",
        "  plt.clf()\n",
        "\n",
        "  plt.figure(1)\n",
        "\n",
        "  plt.title(\"Train\")\n",
        "  index = list(range(len(train_total_rewards)))\n",
        "  plt.plot(index, train_total_rewards, c='tab:cyan', label='total rewards')\n",
        "  plt.plot(index, train_final_rewards, c='tab:red', label='final rewards')\n",
        "  plt.xlabel('Batch')\n",
        "  plt.ylabel('Reward')\n",
        "  plt.ylim(-300,200)\n",
        "  if is_ipython:\n",
        "    display.display(plt.gcf())\n",
        "\n",
        "  plt.figure(2)\n",
        "\n",
        "  plt.title(\"Eval\")\n",
        "  index = list(range(len(eval_total_rewards)))\n",
        "  plt.plot(index, eval_total_rewards, c='tab:cyan', label='total rewards')\n",
        "  plt.plot(index, eval_final_rewards, c='tab:red', label='final rewards')\n",
        "  plt.xlabel('Batch')\n",
        "  plt.ylabel('Reward')\n",
        "  if is_ipython:\n",
        "    display.display(plt.gcf())\n",
        "\n",
        "  plt.figure(3)\n",
        "\n",
        "  plt.title(\"Loss\")\n",
        "  index = list(range(len(loss)))\n",
        "  plt.plot(index, loss, c='tab:cyan', label='total rewards')\n",
        "  plt.xlabel('Batch')\n",
        "\n",
        "  if is_ipython:\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)"
      ],
      "metadata": {
        "id": "IGaQrtR2bItR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def train_eval(is_ipython,\n",
        "        env,\n",
        "        device,\n",
        "        pretrain,\n",
        "        LOAD,\n",
        "        MODEL_PATH,\n",
        "        TRAIN_REWARD_PATH,\n",
        "        EVAL_REWARD_PATH,\n",
        "        LOSS_PATH,\n",
        "        SAVE_EPISODE,\n",
        "        BUFFER_SIZE,\n",
        "        LR,\n",
        "        NUM_EPISODE,\n",
        "        GAMMA,\n",
        "        TAU,\n",
        "        EPS_START,\n",
        "        EPS_END,\n",
        "        EPS_DECAY,\n",
        "        BATCH_SIZE):\n",
        "  start = time.time()\n",
        "\n",
        "  buffer = Buffer(BUFFER_SIZE)\n",
        "  agent = Q_Learning_Process(env,device,Q_Network(),Q_Network(),buffer)\n",
        "  if pretrain :\n",
        "    if LOAD:\n",
        "      pretrain = False\n",
        "      print(f\"The model is load from {MODEL_PATH}, it doesn't require pretraining\")\n",
        "    main_pre_train(env,device)\n",
        "\n",
        "  agent.policy_net.load_state_dict(torch.load(\"/content/pretrain_model.pth\"))\n",
        "  agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
        "\n",
        "  train_total_rewards = []\n",
        "  train_final_rewards = []\n",
        "  eval_total_rewards = []\n",
        "  eval_final_rewards = []\n",
        "  avg_loss = []\n",
        "\n",
        "  with open(TRAIN_REWARD_PATH, 'w') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow(['total_reward', 'final_reward'])\n",
        "  with open(EVAL_REWARD_PATH, 'w') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow(['total_reward', 'final_reward'])\n",
        "\n",
        "  steps_done = 0\n",
        "\n",
        "  optimizer = LR_Scheduler(LR,optim.AdamW(agent.policy_net.parameters(), lr=LR, amsgrad=True))\n",
        "  if LOAD :\n",
        "    optimizer = agent.load(optimizer,MODEL_PATH)\n",
        "    print(f\"Load model from {MODEL_PATH}\")\n",
        "\n",
        "  prg_bar = tqdm(range(NUM_EPISODE))\n",
        "\n",
        "  for episode_index in prg_bar:\n",
        "    rewards = 0\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    loss = []\n",
        "    while not done :\n",
        "      reward,next_state,done,steps_done = \\\n",
        "      agent.update_buffer(state,done,steps_done,EPS_START,EPS_END,EPS_DECAY)\n",
        "      state = next_state\n",
        "      rewards += reward\n",
        "\n",
        "      if agent.buffer.__len__()>= BATCH_SIZE:\n",
        "        now_loss = agent.optimize_model(optimizer,LR,BATCH_SIZE,GAMMA).detach().item()\n",
        "        loss.append(now_loss)\n",
        "\n",
        "        agent.soft_update(TAU)\n",
        "\n",
        "    if len(loss)>0:\n",
        "      avg_loss.append(sum(loss) / len(loss) )\n",
        "    train_total_rewards.append(rewards)\n",
        "    train_final_rewards.append(reward)\n",
        "\n",
        "\n",
        "    if (episode_index+1) % SAVE_EPISODE ==0:\n",
        "      # save model\n",
        "      agent.save(optimizer,MODEL_PATH)\n",
        "      print(f\"Save model from {episode_index+1}th batch to {MODEL_PATH}\")\n",
        "\n",
        "      # save train reward\n",
        "      total_rewards = list(zip(train_total_rewards,train_final_rewards))\n",
        "      total_rewards = total_rewards[episode_index+1-SAVE_EPISODE:episode_index+1]\n",
        "      with open(TRAIN_REWARD_PATH, 'a') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(total_rewards)\n",
        "      print(f\"Save policy rewards from {(episode_index+2)-SAVE_EPISODE}th\"+\n",
        "         f\"to {episode_index+1}th batchs to {TRAIN_REWARD_PATH}\")\n",
        "\n",
        "      # do eval and save eval reward\n",
        "      eval_total_reward, eval_final_reward = agent.eval(5)\n",
        "      eval_total_rewards.append(eval_total_reward)\n",
        "      eval_final_rewards.append(eval_final_reward)\n",
        "\n",
        "      with open(EVAL_REWARD_PATH, 'a') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([eval_total_reward,eval_final_reward])\n",
        "\n",
        "      print(f\"Save target rewards of {episode_index+1}th batch to {TRAIN_REWARD_PATH}\")\n",
        "\n",
        "      # save loss\n",
        "      with open(LOSS_PATH, 'a') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows([[x] for x in avg_loss[episode_index+1-SAVE_EPISODE:episode_index+1]])\n",
        "\n",
        "      print(f\"Save loss of {(episode_index+2)-SAVE_EPISODE}th \"+\n",
        "         f\"to {episode_index+1}th batchs to {LOSS_PATH}\")\n",
        "      # plot data\n",
        "      plot_data(is_ipython,avg_loss,train_total_rewards,train_final_rewards,eval_total_rewards,eval_final_rewards)\n",
        "\n",
        "  end = time.time()\n",
        "  print(f\"total_spend_time = {end-start} seconds\")"
      ],
      "metadata": {
        "id": "PuOkaJnQGwcH"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(env,device,BUFFER_SIZE,LR,**kwargs):\n",
        "  policy_net = Q_Network()\n",
        "  target_net = Q_Network()\n",
        "  buffer = Buffer(BUFFER_SIZE)\n",
        "  agent = Q_Learning_Process(env,device,policy_net,target_net,buffer)\n",
        "  agent.policy_net.load_state_dict(torch.load(\"/content/DQN_model.pth\")[\"policy_net\"])\n",
        "  target_net.load_state_dict(policy_net.state_dict())\n",
        "  optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "\n",
        "  agent.policy_net.eval()\n",
        "  agent.target_net.eval()\n",
        "  NUM_OF_TEST = 5\n",
        "  action_list = []\n",
        "\n",
        "  total_rewards = []\n",
        "  final_rewards = []\n",
        "  for i in range(NUM_OF_TEST):\n",
        "    actions = []\n",
        "    state = env.reset()\n",
        "\n",
        "    img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "    total_reward = 0\n",
        "    env_as_nparray = env.render(mode='rgb_array')\n",
        "    img = plt.imshow(env_as_nparray)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      input = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "      input = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "      action = policy_net(input).max(1).indices.detach().item()\n",
        "      img.set_data(env.render(mode='rgb_array'))\n",
        "      display.display(plt.gcf())\n",
        "      display.clear_output(wait=True)\n",
        "\n",
        "      next_state, reward, done, info = env.step(action)\n",
        "      total_reward += reward\n",
        "      actions.append(action)\n",
        "      if done:\n",
        "        final_rewards.append(reward)\n",
        "\n",
        "    total_rewards.append(total_reward)\n",
        "    action_list.append(actions)\n",
        "  print(f\"Average total reward of {NUM_OF_TEST} test is {sum(total_rewards)/len(total_rewards)}\")\n",
        "  print(f\"Average final reward if {NUM_OF_TEST} test is {sum(final_rewards)/len(final_rewards)}\")"
      ],
      "metadata": {
        "id": "kBAxEdMnbIn6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_load ={\n",
        "\"LOAD\" : False,\n",
        "\"SAVE_EPISODE\" : 10,\n",
        "\"MODEL_PATH\" : \"/content/DQN_model.pth\",\n",
        "\"TRAIN_REWARD_PATH\" : \"/content/DQN_train_rewards.csv\",\n",
        "\"EVAL_REWARD_PATH\" : \"/content/DQN_eval_rewards.csv\",\n",
        "\"LOSS_PATH\" : \"/content/DQN_loss.csv\"\n",
        "}\n",
        "h_paras = {\n",
        "\"BUFFER_SIZE\" : 10000,\n",
        "\"BATCH_SIZE\" : 128,\n",
        "\"NUM_EPISODE\" : 600,\n",
        "\"GAMMA\" : 0.99,\n",
        "\"EPS_START\" : 0.9,\n",
        "\"EPS_END\" : 0.05,\n",
        "\"EPS_DECAY\" : 500,\n",
        "\"TAU\" : 0.005,\n",
        "\"LR\" : 1e-3,\n",
        "}"
      ],
      "metadata": {
        "id": "HHxS3JfZ55X4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "%matplotlib inline\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "def main(pretrain):\n",
        "  env = gym.make('LunarLander-v2')\n",
        "  fix(env, seed)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  train_eval(is_ipython,env,device,pretrain,**save_load,**h_paras)\n",
        "  inference(env,device,**h_paras)\n",
        "\n",
        "main(pretrain=True)"
      ],
      "metadata": {
        "id": "IpLRyZoooe0z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529,
          "referenced_widgets": [
            "5631fa9f94a448758b34263826d7ae42",
            "67130c845a6b4b04bf71a0314521c4e1",
            "8d4e8ca991544572a6a5fd51a4ca93ef",
            "2b76eb22950c49b7a7fdbe55a4425136",
            "c0c01072a7d94472ab9f8f9701ebc0ba",
            "ebebec1e912b48eb820ea67249bb0409",
            "b9951ccfbfb14505a5db597258dab31d",
            "04106722ecbc4c8b81e211a28529e14a",
            "08c2f9278be84ceca10b3553c154ab41",
            "5bd7ed5a65414f55a9de08a6339875ae",
            "eb1cc01000464476971f0043106e78e1"
          ]
        },
        "outputId": "d12b359f-2d97-4a99-cc36-41a856552127"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/800 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5631fa9f94a448758b34263826d7ae42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-1cd159b3e4ea>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mh_paras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-1cd159b3e4ea>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(pretrain)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mtrain_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_ipython\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msave_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mh_paras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mh_paras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-23c4b873d88a>\u001b[0m in \u001b[0;36mtrain_eval\u001b[0;34m(is_ipython, env, device, pretrain, LOAD, MODEL_PATH, TRAIN_REWARD_PATH, EVAL_REWARD_PATH, LOSS_PATH, SAVE_EPISODE, BUFFER_SIZE, LR, NUM_EPISODE, GAMMA, TAU, EPS_START, EPS_END, EPS_DECAY, BATCH_SIZE)\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mpretrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The model is load from {MODEL_PATH}, it doesn't require pretraining\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmain_pre_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/pretrain_model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-afc91ff18051>\u001b[0m in \u001b[0;36mmain_pre_train\u001b[0;34m(env, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyGradientNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyGradientAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mpolicy_gradient_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/pretrain_model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-e08fdea7ac79>\u001b[0m in \u001b[0;36mpolicy_gradient_training\u001b[0;34m(env, agent)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average total reward of last {EPISODE_PER_BATCH*10} episode of pretraining is {avg_total_rewards[-1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average final reward of last {EPISODE_PER_BATCH*10} episode of pretraining is {avg_final_rewards[-1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-6af49b1824cb>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, log_probs, rewards)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}