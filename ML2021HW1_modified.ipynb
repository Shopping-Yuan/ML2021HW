{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrSjQuG1E8slhxfnKmUpzV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML2021HW/blob/Shopping_branch/ML2021HW1_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pytorch Setting:"
      ],
      "metadata": {
        "id": "qaaQpqayiLbl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8mZwtss3iFBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e6625a3-5d4b-4529-dce9-5ee0442297c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "#import pytorch\n",
        "import torch\n",
        "\n",
        "# torch.backends.cudnn: set CNN algorithmtorch.backends.cudnn\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# get the current available device ('cpu' or 'cuda')\n",
        "def get_device():\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = get_device()\n",
        "print(torch.cuda.is_available())\n",
        "device = get_device()\n",
        "\n",
        "#set random variable\n",
        "import numpy as np\n",
        "myseed = 1\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Set"
      ],
      "metadata": {
        "id": "EvBtNdcmjKTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write data_set_function\n",
        "#normalize data if needed\n",
        "def normalize(df):\n",
        "    return ((df - df.mean(axis = 0)) / (df.std(axis = 0)))\n",
        "# generate index_list for spliting train & validation set\n",
        "def got_index(df,mode):\n",
        "    index_list = []\n",
        "    first_day = 0\n",
        "    for i in range(df.shape[1]):\n",
        "      #count total data in that catetory\n",
        "      days = df.iloc[:,i].sum()\n",
        "\n",
        "      last_day = int(first_day + days-1)\n",
        "      split_day = int(first_day + days*0.9)\n",
        "\n",
        "      if mode == \"train\":\n",
        "        index_list += list(range(first_day,split_day+1))\n",
        "      elif mode == \"val\":\n",
        "        index_list += list(range(split_day+1,last_day+1))\n",
        "\n",
        "      first_day = last_day+1\n",
        "    return index_list\n",
        "\n",
        "#split train & validation set by index\n",
        "def covid19_train_val_f(mode,d_l = \"both\"):\n",
        "    train__val_df = pd.read_csv(data_info[mode][\"path\"])\n",
        "    print('Size of training data: {}'.format(train__val_df.shape))\n",
        "\n",
        "    train__val_df.iloc[:,41:] = normalize(train__val_df.iloc[:,41:])\n",
        "\n",
        "    index_list = got_index(train__val_df.iloc[:,1:41],mode)\n",
        "    train__val_df = train__val_df.iloc[ index_list,1:]\n",
        "    label = torch.FloatTensor(train__val_df.iloc[:, -1].to_numpy())\n",
        "    label = torch.unsqueeze(label, 1)\n",
        "    data = torch.FloatTensor(train__val_df.iloc[:, 0:-1].to_numpy())\n",
        "    return({\"data\":data , \"label\":label})\n",
        "\n",
        "def covid19_test_f(mode,d_l = \"data\"):\n",
        "    test_df = pd.read_csv(data_info[mode][\"path\"])\n",
        "    print('Size of testing data: {}'.format(test_df.shape))\n",
        "    test_df.iloc[:,41:] = normalize(test_df.iloc[:,41:])\n",
        "    data = torch.FloatTensor(test_df.to_numpy()[:,1:])\n",
        "    return(data)\n",
        "#create a dict of functions and path w.r.t. different mode\n",
        "data_info = {\n",
        "    \"train\":{\"function\":covid19_train_val_f,\"path\":\"./covid.train.csv\",\"data_or_label\":\"both\"},\n",
        "    \"val\":{\"function\":covid19_train_val_f,\"path\":\"./covid.train.csv\",\"data_or_label\":\"both\"},\n",
        "    \"test\":{\"function\":covid19_test_f,\"path\":\"./covid.test.csv\",\"data_or_label\":\"data\"}\n",
        "}"
      ],
      "metadata": {
        "id": "gWBDrmbiU5Ce"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite class Dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Dataset_preparation(Dataset):\n",
        "  def __init__(self,mode,data_info):\n",
        "      self.mode = mode\n",
        "      self.data_or_label = data_info[mode][\"data_or_label\"]\n",
        "\n",
        "      if self.data_or_label == \"data\":\n",
        "        self.data = data_info[self.mode][\"function\"](self.mode,\"data\")\n",
        "      elif self.data_or_label == \"both\":\n",
        "        self.data = data_info[self.mode][\"function\"](self.mode,\"both\")[\"data\"]\n",
        "        self.label = data_info[self.mode][\"function\"](self.mode,\"both\")[\"label\"]\n",
        "      else :\n",
        "        self.label = data_info[self.mode][\"function\"](self.mode,\"label\")[\"label\"]\n",
        "\n",
        "      self.dim = self.data.shape[1]\n",
        "      print('Finished reading the {mode} set of Dataset ({len} samples found, each dim = {dim})'\n",
        "              .format(mode = self.mode, len =len(self.data), dim=self.dim))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      if self.data_or_label == \"data\":\n",
        "        return self.data[index]\n",
        "      elif self.data_or_label == \"both\":\n",
        "        return self.data[index], self.label[index]\n",
        "      else :\n",
        "        return self.label[index]\n",
        "  def __len__(self):\n",
        "      # Returns the size of the dataset\n",
        "      return len(self.data)"
      ],
      "metadata": {
        "id": "O_5yInY7LWzF"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_test = Dataset_preparation(\"test\",data_info)\n",
        "print(covid19_train_val_f(\"val\",d_l = \"both\")[\"label\"].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ijlw1EJjqWH",
        "outputId": "db14dbac-2256-475a-ff0e-3a0cdce7842c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of training data: (2700, 95)\n",
            "torch.Size([240, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#decide how to load data\n",
        "def prep_dataloader(mode,d_info,batch_size,n_jobs=0):\n",
        "    dataset = Dataset_preparation(mode,d_info)\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size,\n",
        "        shuffle=(mode == 'train'), drop_last=False,\n",
        "        num_workers=n_jobs, pin_memory=False)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "pPAPaENVnMqE"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.activation import Sigmoid\n",
        "import torch.nn as nn\n",
        "class Multi100Layer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Multi100Layer, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = 100.0 * x\n",
        "        return x\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        # Define your neural network here\n",
        "        # TODO: How to modify this model to achieve better performance?\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "#            nn.Sigmoid(),\n",
        "#            Multi100Layer()\n",
        "            )\n",
        "\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        return x\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "\n",
        "        return self.criterion(pred, target)"
      ],
      "metadata": {
        "id": "8D3Drze8Fi2s"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([5.0])\n",
        "model = NeuralNet(1)\n",
        "model_d = model.to(device)\n",
        "print(model_d(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fh9DHrQ9JK-",
        "outputId": "28413c4b-80e8-4910-d9c5-e2c77b31cdde"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([83.4867], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h_paras = {\n",
        "    # maximum number of epochs\n",
        "    'n_epochs': 3000,\n",
        "    # mini-batch size for dataloader\n",
        "    'batch_size': 270,\n",
        "    # optimization algorithm (optimizer in torch.optim)\n",
        "    'optimizer': 'SGD',\n",
        "    # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "    'optim_hparas': {\n",
        "        # learning rate of SGD\n",
        "        'lr': 0.001,\n",
        "        # momentum for SGD\n",
        "        'momentum': 0.9\n",
        "    },\n",
        "    # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'early_stop': 200,\n",
        "    # if loss small than this , early-stopping counter will reset(may update in validation process)\n",
        "    'early_stop_loss_init': 10.0,\n",
        "    # your model will be saved here\n",
        "    'save_path': './model.pth'\n",
        "}"
      ],
      "metadata": {
        "id": "qNHqYt34ZP5y"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_set,model_d,optimizer,device):\n",
        "    # set model to training mode\n",
        "    model_d.train()\n",
        "    # iterate through the dataloader\n",
        "    for data , label in train_set:\n",
        "      # move data to device (cpu/cuda)\n",
        "      data_d , label_d = data.to(device), label.to(device)\n",
        "      # forward pass (compute output tensor)\n",
        "      pred = (model_d(data))\n",
        "      # compute loss\n",
        "      mse_loss = model_d.cal_loss(pred , label_d)\n",
        "      # compute gradient (backpropagation)\n",
        "      mse_loss.backward()\n",
        "      # update model with optimizer\n",
        "      optimizer.step()\n",
        "      # set optimizer gradient to zero\n",
        "      optimizer.zero_grad()\n",
        "      return mse_loss.detach().cpu().item()"
      ],
      "metadata": {
        "id": "6jcKe6kSdIPE"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val(val_set,model_d,device):\n",
        "    # set model to evalutation mode\n",
        "    model_d.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    # iterate through the dataloader\n",
        "    for data , label in val_set:\n",
        "    # move data to device (cpu/cuda)\n",
        "      data_d, label_d = data.to(device), label.to(device)\n",
        "      # disable gradient calculation\n",
        "      with torch.no_grad():\n",
        "        # forward pass (compute output)\n",
        "        pred = model_d(data_d)\n",
        "        # compute loss\n",
        "        mse_loss = model_d.cal_loss(pred, label_d)\n",
        "      # accumulate loss\n",
        "      batch_size = len(data_d)\n",
        "      total_loss += mse_loss.detach().cpu().item() * batch_size\n",
        "    # compute averaged loss\n",
        "    totol_size = len(val_set.dataset)\n",
        "    avg_loss =  total_loss/totol_size\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "zFMoWlxkc89K"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_set, model_d, device):\n",
        "    # set model to evalutation mode\n",
        "    model_d.eval()\n",
        "    preds = []\n",
        "    # iterate through the dataloader\n",
        "    for data in test_set:\n",
        "      # move data to device (cpu/cuda)\n",
        "      data_d = data.to(device)\n",
        "      # disable gradient calculation\n",
        "      with torch.no_grad():\n",
        "        # forward pass (compute output)\n",
        "        pred = model_d(data_d)\n",
        "        # collect prediction\n",
        "        preds.append(pred.detach().cpu())\n",
        "    # concatenate all predictions and convert to a numpy array\n",
        "    preds = torch.cat(preds, dim=0).numpy()\n",
        "    return preds"
      ],
      "metadata": {
        "id": "DgYuELvCitWh"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_process(training_set, validation_set, model, h_paras, device):\n",
        "    #1: move model to device (cpu/cuda)\n",
        "    model_d = model.to(device)\n",
        "    #2: set optimizer = torch.optim.SGD(model_d.parameters(),lr=0.001,momentum=0.9)\n",
        "    optimizer = getattr(torch.optim, h_paras['optimizer'])(\n",
        "        model.parameters(), **h_paras['optim_hparas'])\n",
        "\n",
        "    #3: set epoch = h_paras['n_epochs'] = 3000\n",
        "    n_epochs = h_paras['n_epochs']\n",
        "\n",
        "    #4: record training loss\n",
        "    loss_record = {'train': [], \"val\": []}\n",
        "\n",
        "    #5: setting paras\n",
        "    # epoch para\n",
        "    epoch = 0\n",
        "    # early-stoping paras\n",
        "    early_stop_cnt = 0\n",
        "    min_loss = h_paras[\"early_stop_loss_init\"]\n",
        "\n",
        "    #start training\n",
        "    while epoch < n_epochs:\n",
        "\n",
        "      # Part1 : training process\n",
        "      # updata model weight and compute model loss\n",
        "      train_loss = train(training_set,model_d,optimizer,device)\n",
        "      # save loss to loss_record['train']\n",
        "      loss_record['train'].append(train_loss)\n",
        "      # renew epoch para\n",
        "      epoch += 1\n",
        "\n",
        "      # Part2 validation process\n",
        "      # After each epoch, test your model\n",
        "      # on the validation (development) set.\n",
        "      val_loss = val(validation_set, model_d, device)\n",
        "      # save loss to loss_record[\"val\"]\n",
        "      loss_record[\"val\"].append(val_loss)\n",
        "\n",
        "      # Part3 early stopping\n",
        "      # Compute early-stopping counter\n",
        "      if val_loss < min_loss:\n",
        "        min_loss = val_loss\n",
        "        print('Saving model (epoch = {:4d}, loss = {:.4f})'\\\n",
        "        .format(epoch , val_loss))\n",
        "      # Save model to specified path if your model improved\n",
        "        torch.save(model_d.state_dict(), h_paras['save_path'])\n",
        "        early_stop_cnt = 0\n",
        "      else:\n",
        "        early_stop_cnt += 1\n",
        "      # Check early stop criteria\n",
        "      if early_stop_cnt > h_paras['early_stop']:\n",
        "          # Stop training if your model stops improving\n",
        "          # for \"h_paras['early_stop']\" epochs.\n",
        "          break\n",
        "\n",
        "    print('Finished training after {} epochs'.format(epoch))\n",
        "    return min_loss, loss_record"
      ],
      "metadata": {
        "id": "iuUnrvH7L9CG"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = prep_dataloader('train',data_info,h_paras['batch_size'])\n",
        "val_set = prep_dataloader('val',data_info,h_paras['batch_size'])\n",
        "test_set = prep_dataloader('test',data_info,h_paras['batch_size'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KsKJk5fk32z",
        "outputId": "2fb7503d-f07d-4212-cc9b-d57adca4bada"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of training data: (2700, 95)\n",
            "Size of training data: (2700, 95)\n",
            "Finished reading the train set of Dataset (2460 samples found, each dim = 93)\n",
            "Size of training data: (2700, 95)\n",
            "Size of training data: (2700, 95)\n",
            "Finished reading the val set of Dataset (240 samples found, each dim = 93)\n",
            "Size of testing data: (893, 94)\n",
            "Finished reading the test set of Dataset (893 samples found, each dim = 93)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = NeuralNet(train_set.dataset.dim)\n",
        "# model_d = model.to(device)\n",
        "# optimizer = torch.optim.SGD(model_d.parameters(),lr = 0.001,momentum=0.9)\n",
        "# print(optimizer)"
      ],
      "metadata": {
        "id": "UUCsUXhiuGtq"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct model\n",
        "model = NeuralNet(train_set.dataset.dim)\n",
        "model_loss, model_loss_record = train_val_process(train_set, val_set, model, h_paras, device)\n",
        "#print(model_loss_record[\"train\"])"
      ],
      "metadata": {
        "id": "5alEFp4OMMuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e76bb70a-0d23-423e-c0f9-e24a8a81a444"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model (epoch =    1, loss = 1.0951)\n",
            "Saving model (epoch =    2, loss = 1.0814)\n",
            "Saving model (epoch =    3, loss = 1.0611)\n",
            "Saving model (epoch =    4, loss = 1.0354)\n",
            "Saving model (epoch =    5, loss = 1.0044)\n",
            "Saving model (epoch =    6, loss = 0.9692)\n",
            "Saving model (epoch =    7, loss = 0.9318)\n",
            "Saving model (epoch =    8, loss = 0.8926)\n",
            "Saving model (epoch =    9, loss = 0.8525)\n",
            "Saving model (epoch =   10, loss = 0.8123)\n",
            "Saving model (epoch =   11, loss = 0.7730)\n",
            "Saving model (epoch =   12, loss = 0.7341)\n",
            "Saving model (epoch =   13, loss = 0.6961)\n",
            "Saving model (epoch =   14, loss = 0.6592)\n",
            "Saving model (epoch =   15, loss = 0.6236)\n",
            "Saving model (epoch =   16, loss = 0.5893)\n",
            "Saving model (epoch =   17, loss = 0.5566)\n",
            "Saving model (epoch =   18, loss = 0.5259)\n",
            "Saving model (epoch =   19, loss = 0.4972)\n",
            "Saving model (epoch =   20, loss = 0.4702)\n",
            "Saving model (epoch =   21, loss = 0.4447)\n",
            "Saving model (epoch =   22, loss = 0.4208)\n",
            "Saving model (epoch =   23, loss = 0.3982)\n",
            "Saving model (epoch =   24, loss = 0.3771)\n",
            "Saving model (epoch =   25, loss = 0.3575)\n",
            "Saving model (epoch =   26, loss = 0.3393)\n",
            "Saving model (epoch =   27, loss = 0.3221)\n",
            "Saving model (epoch =   28, loss = 0.3063)\n",
            "Saving model (epoch =   29, loss = 0.2921)\n",
            "Saving model (epoch =   30, loss = 0.2790)\n",
            "Saving model (epoch =   31, loss = 0.2670)\n",
            "Saving model (epoch =   32, loss = 0.2563)\n",
            "Saving model (epoch =   33, loss = 0.2467)\n",
            "Saving model (epoch =   34, loss = 0.2383)\n",
            "Saving model (epoch =   35, loss = 0.2308)\n",
            "Saving model (epoch =   36, loss = 0.2242)\n",
            "Saving model (epoch =   37, loss = 0.2183)\n",
            "Saving model (epoch =   38, loss = 0.2130)\n",
            "Saving model (epoch =   39, loss = 0.2082)\n",
            "Saving model (epoch =   40, loss = 0.2039)\n",
            "Saving model (epoch =   41, loss = 0.1998)\n",
            "Saving model (epoch =   42, loss = 0.1963)\n",
            "Saving model (epoch =   43, loss = 0.1932)\n",
            "Saving model (epoch =   44, loss = 0.1905)\n",
            "Saving model (epoch =   45, loss = 0.1880)\n",
            "Saving model (epoch =   46, loss = 0.1859)\n",
            "Saving model (epoch =   47, loss = 0.1838)\n",
            "Saving model (epoch =   48, loss = 0.1819)\n",
            "Saving model (epoch =   49, loss = 0.1802)\n",
            "Saving model (epoch =   50, loss = 0.1787)\n",
            "Saving model (epoch =   51, loss = 0.1772)\n",
            "Saving model (epoch =   52, loss = 0.1758)\n",
            "Saving model (epoch =   53, loss = 0.1744)\n",
            "Saving model (epoch =   54, loss = 0.1730)\n",
            "Saving model (epoch =   55, loss = 0.1716)\n",
            "Saving model (epoch =   56, loss = 0.1703)\n",
            "Saving model (epoch =   57, loss = 0.1690)\n",
            "Saving model (epoch =   58, loss = 0.1676)\n",
            "Saving model (epoch =   59, loss = 0.1663)\n",
            "Saving model (epoch =   60, loss = 0.1650)\n",
            "Saving model (epoch =   61, loss = 0.1637)\n",
            "Saving model (epoch =   62, loss = 0.1623)\n",
            "Saving model (epoch =   63, loss = 0.1609)\n",
            "Saving model (epoch =   64, loss = 0.1597)\n",
            "Saving model (epoch =   65, loss = 0.1585)\n",
            "Saving model (epoch =   66, loss = 0.1573)\n",
            "Saving model (epoch =   67, loss = 0.1562)\n",
            "Saving model (epoch =   68, loss = 0.1550)\n",
            "Saving model (epoch =   69, loss = 0.1540)\n",
            "Saving model (epoch =   70, loss = 0.1529)\n",
            "Saving model (epoch =   71, loss = 0.1519)\n",
            "Saving model (epoch =   72, loss = 0.1509)\n",
            "Saving model (epoch =   73, loss = 0.1500)\n",
            "Saving model (epoch =   74, loss = 0.1490)\n",
            "Saving model (epoch =   75, loss = 0.1482)\n",
            "Saving model (epoch =   76, loss = 0.1474)\n",
            "Saving model (epoch =   77, loss = 0.1466)\n",
            "Saving model (epoch =   78, loss = 0.1458)\n",
            "Saving model (epoch =   79, loss = 0.1451)\n",
            "Saving model (epoch =   80, loss = 0.1446)\n",
            "Saving model (epoch =   81, loss = 0.1441)\n",
            "Saving model (epoch =   82, loss = 0.1436)\n",
            "Saving model (epoch =   83, loss = 0.1431)\n",
            "Saving model (epoch =   84, loss = 0.1426)\n",
            "Saving model (epoch =   85, loss = 0.1422)\n",
            "Saving model (epoch =   86, loss = 0.1418)\n",
            "Saving model (epoch =   87, loss = 0.1415)\n",
            "Saving model (epoch =   88, loss = 0.1412)\n",
            "Saving model (epoch =   89, loss = 0.1408)\n",
            "Saving model (epoch =   90, loss = 0.1405)\n",
            "Saving model (epoch =   91, loss = 0.1400)\n",
            "Saving model (epoch =   92, loss = 0.1396)\n",
            "Saving model (epoch =   93, loss = 0.1392)\n",
            "Saving model (epoch =   94, loss = 0.1389)\n",
            "Saving model (epoch =   95, loss = 0.1386)\n",
            "Saving model (epoch =   96, loss = 0.1383)\n",
            "Saving model (epoch =   97, loss = 0.1380)\n",
            "Saving model (epoch =   98, loss = 0.1377)\n",
            "Saving model (epoch =   99, loss = 0.1374)\n",
            "Saving model (epoch =  100, loss = 0.1370)\n",
            "Saving model (epoch =  101, loss = 0.1367)\n",
            "Saving model (epoch =  102, loss = 0.1364)\n",
            "Saving model (epoch =  103, loss = 0.1362)\n",
            "Saving model (epoch =  104, loss = 0.1359)\n",
            "Saving model (epoch =  105, loss = 0.1357)\n",
            "Saving model (epoch =  106, loss = 0.1355)\n",
            "Saving model (epoch =  107, loss = 0.1352)\n",
            "Saving model (epoch =  108, loss = 0.1350)\n",
            "Saving model (epoch =  109, loss = 0.1347)\n",
            "Saving model (epoch =  110, loss = 0.1343)\n",
            "Saving model (epoch =  111, loss = 0.1340)\n",
            "Saving model (epoch =  112, loss = 0.1338)\n",
            "Saving model (epoch =  113, loss = 0.1335)\n",
            "Saving model (epoch =  114, loss = 0.1333)\n",
            "Saving model (epoch =  115, loss = 0.1330)\n",
            "Saving model (epoch =  116, loss = 0.1327)\n",
            "Saving model (epoch =  117, loss = 0.1326)\n",
            "Saving model (epoch =  118, loss = 0.1324)\n",
            "Saving model (epoch =  119, loss = 0.1322)\n",
            "Saving model (epoch =  120, loss = 0.1320)\n",
            "Saving model (epoch =  121, loss = 0.1318)\n",
            "Saving model (epoch =  122, loss = 0.1316)\n",
            "Saving model (epoch =  123, loss = 0.1312)\n",
            "Saving model (epoch =  124, loss = 0.1308)\n",
            "Saving model (epoch =  125, loss = 0.1304)\n",
            "Saving model (epoch =  126, loss = 0.1301)\n",
            "Saving model (epoch =  127, loss = 0.1297)\n",
            "Saving model (epoch =  128, loss = 0.1294)\n",
            "Saving model (epoch =  129, loss = 0.1290)\n",
            "Saving model (epoch =  130, loss = 0.1287)\n",
            "Saving model (epoch =  131, loss = 0.1285)\n",
            "Saving model (epoch =  132, loss = 0.1282)\n",
            "Saving model (epoch =  133, loss = 0.1279)\n",
            "Saving model (epoch =  134, loss = 0.1275)\n",
            "Saving model (epoch =  135, loss = 0.1272)\n",
            "Saving model (epoch =  136, loss = 0.1269)\n",
            "Saving model (epoch =  137, loss = 0.1266)\n",
            "Saving model (epoch =  138, loss = 0.1263)\n",
            "Saving model (epoch =  139, loss = 0.1260)\n",
            "Saving model (epoch =  140, loss = 0.1257)\n",
            "Saving model (epoch =  141, loss = 0.1254)\n",
            "Saving model (epoch =  142, loss = 0.1251)\n",
            "Saving model (epoch =  143, loss = 0.1248)\n",
            "Saving model (epoch =  144, loss = 0.1245)\n",
            "Saving model (epoch =  145, loss = 0.1243)\n",
            "Saving model (epoch =  146, loss = 0.1240)\n",
            "Saving model (epoch =  147, loss = 0.1237)\n",
            "Saving model (epoch =  148, loss = 0.1233)\n",
            "Saving model (epoch =  149, loss = 0.1230)\n",
            "Saving model (epoch =  150, loss = 0.1226)\n",
            "Saving model (epoch =  151, loss = 0.1222)\n",
            "Saving model (epoch =  152, loss = 0.1217)\n",
            "Saving model (epoch =  153, loss = 0.1213)\n",
            "Saving model (epoch =  154, loss = 0.1209)\n",
            "Saving model (epoch =  155, loss = 0.1205)\n",
            "Saving model (epoch =  156, loss = 0.1201)\n",
            "Saving model (epoch =  157, loss = 0.1196)\n",
            "Saving model (epoch =  158, loss = 0.1192)\n",
            "Saving model (epoch =  159, loss = 0.1187)\n",
            "Saving model (epoch =  160, loss = 0.1183)\n",
            "Saving model (epoch =  161, loss = 0.1179)\n",
            "Saving model (epoch =  162, loss = 0.1175)\n",
            "Saving model (epoch =  163, loss = 0.1171)\n",
            "Saving model (epoch =  164, loss = 0.1167)\n",
            "Saving model (epoch =  165, loss = 0.1164)\n",
            "Saving model (epoch =  166, loss = 0.1161)\n",
            "Saving model (epoch =  167, loss = 0.1157)\n",
            "Saving model (epoch =  168, loss = 0.1153)\n",
            "Saving model (epoch =  169, loss = 0.1149)\n",
            "Saving model (epoch =  170, loss = 0.1145)\n",
            "Saving model (epoch =  171, loss = 0.1142)\n",
            "Saving model (epoch =  172, loss = 0.1139)\n",
            "Saving model (epoch =  173, loss = 0.1136)\n",
            "Saving model (epoch =  174, loss = 0.1133)\n",
            "Saving model (epoch =  175, loss = 0.1131)\n",
            "Saving model (epoch =  176, loss = 0.1129)\n",
            "Saving model (epoch =  177, loss = 0.1128)\n",
            "Saving model (epoch =  178, loss = 0.1127)\n",
            "Saving model (epoch =  179, loss = 0.1127)\n",
            "Saving model (epoch =  180, loss = 0.1126)\n",
            "Saving model (epoch =  181, loss = 0.1126)\n",
            "Saving model (epoch =  182, loss = 0.1126)\n",
            "Saving model (epoch =  183, loss = 0.1126)\n",
            "Saving model (epoch =  185, loss = 0.1125)\n",
            "Saving model (epoch =  186, loss = 0.1123)\n",
            "Saving model (epoch =  187, loss = 0.1121)\n",
            "Saving model (epoch =  188, loss = 0.1119)\n",
            "Saving model (epoch =  189, loss = 0.1115)\n",
            "Saving model (epoch =  190, loss = 0.1112)\n",
            "Saving model (epoch =  191, loss = 0.1108)\n",
            "Saving model (epoch =  192, loss = 0.1104)\n",
            "Saving model (epoch =  193, loss = 0.1099)\n",
            "Saving model (epoch =  194, loss = 0.1096)\n",
            "Saving model (epoch =  195, loss = 0.1092)\n",
            "Saving model (epoch =  196, loss = 0.1090)\n",
            "Saving model (epoch =  197, loss = 0.1087)\n",
            "Saving model (epoch =  198, loss = 0.1084)\n",
            "Saving model (epoch =  199, loss = 0.1081)\n",
            "Saving model (epoch =  200, loss = 0.1078)\n",
            "Saving model (epoch =  201, loss = 0.1074)\n",
            "Saving model (epoch =  202, loss = 0.1071)\n",
            "Saving model (epoch =  203, loss = 0.1067)\n",
            "Saving model (epoch =  204, loss = 0.1063)\n",
            "Saving model (epoch =  205, loss = 0.1058)\n",
            "Saving model (epoch =  206, loss = 0.1053)\n",
            "Saving model (epoch =  207, loss = 0.1049)\n",
            "Saving model (epoch =  208, loss = 0.1046)\n",
            "Saving model (epoch =  209, loss = 0.1042)\n",
            "Saving model (epoch =  210, loss = 0.1040)\n",
            "Saving model (epoch =  211, loss = 0.1037)\n",
            "Saving model (epoch =  212, loss = 0.1035)\n",
            "Saving model (epoch =  213, loss = 0.1033)\n",
            "Saving model (epoch =  214, loss = 0.1031)\n",
            "Saving model (epoch =  215, loss = 0.1029)\n",
            "Saving model (epoch =  216, loss = 0.1028)\n",
            "Saving model (epoch =  217, loss = 0.1027)\n",
            "Saving model (epoch =  218, loss = 0.1026)\n",
            "Saving model (epoch =  219, loss = 0.1025)\n",
            "Saving model (epoch =  220, loss = 0.1025)\n",
            "Saving model (epoch =  221, loss = 0.1024)\n",
            "Saving model (epoch =  222, loss = 0.1023)\n",
            "Saving model (epoch =  223, loss = 0.1022)\n",
            "Saving model (epoch =  224, loss = 0.1020)\n",
            "Saving model (epoch =  225, loss = 0.1019)\n",
            "Saving model (epoch =  226, loss = 0.1017)\n",
            "Saving model (epoch =  227, loss = 0.1014)\n",
            "Saving model (epoch =  228, loss = 0.1012)\n",
            "Saving model (epoch =  229, loss = 0.1010)\n",
            "Saving model (epoch =  230, loss = 0.1008)\n",
            "Saving model (epoch =  231, loss = 0.1007)\n",
            "Saving model (epoch =  232, loss = 0.1007)\n",
            "Saving model (epoch =  233, loss = 0.1005)\n",
            "Saving model (epoch =  234, loss = 0.1004)\n",
            "Saving model (epoch =  235, loss = 0.1003)\n",
            "Saving model (epoch =  236, loss = 0.1001)\n",
            "Saving model (epoch =  237, loss = 0.1000)\n",
            "Saving model (epoch =  238, loss = 0.0998)\n",
            "Saving model (epoch =  239, loss = 0.0996)\n",
            "Saving model (epoch =  240, loss = 0.0994)\n",
            "Saving model (epoch =  241, loss = 0.0993)\n",
            "Saving model (epoch =  242, loss = 0.0991)\n",
            "Saving model (epoch =  243, loss = 0.0990)\n",
            "Saving model (epoch =  244, loss = 0.0988)\n",
            "Saving model (epoch =  245, loss = 0.0986)\n",
            "Saving model (epoch =  246, loss = 0.0983)\n",
            "Saving model (epoch =  247, loss = 0.0982)\n",
            "Saving model (epoch =  248, loss = 0.0979)\n",
            "Saving model (epoch =  249, loss = 0.0977)\n",
            "Saving model (epoch =  250, loss = 0.0976)\n",
            "Saving model (epoch =  251, loss = 0.0973)\n",
            "Saving model (epoch =  252, loss = 0.0971)\n",
            "Saving model (epoch =  253, loss = 0.0968)\n",
            "Saving model (epoch =  254, loss = 0.0965)\n",
            "Saving model (epoch =  255, loss = 0.0962)\n",
            "Saving model (epoch =  256, loss = 0.0960)\n",
            "Saving model (epoch =  257, loss = 0.0957)\n",
            "Saving model (epoch =  258, loss = 0.0955)\n",
            "Saving model (epoch =  259, loss = 0.0954)\n",
            "Saving model (epoch =  260, loss = 0.0951)\n",
            "Saving model (epoch =  261, loss = 0.0949)\n",
            "Saving model (epoch =  262, loss = 0.0945)\n",
            "Saving model (epoch =  263, loss = 0.0942)\n",
            "Saving model (epoch =  264, loss = 0.0940)\n",
            "Saving model (epoch =  265, loss = 0.0938)\n",
            "Saving model (epoch =  266, loss = 0.0935)\n",
            "Saving model (epoch =  267, loss = 0.0933)\n",
            "Saving model (epoch =  268, loss = 0.0929)\n",
            "Saving model (epoch =  269, loss = 0.0926)\n",
            "Saving model (epoch =  270, loss = 0.0923)\n",
            "Saving model (epoch =  271, loss = 0.0921)\n",
            "Saving model (epoch =  272, loss = 0.0918)\n",
            "Saving model (epoch =  273, loss = 0.0915)\n",
            "Saving model (epoch =  274, loss = 0.0913)\n",
            "Saving model (epoch =  275, loss = 0.0910)\n",
            "Saving model (epoch =  276, loss = 0.0908)\n",
            "Saving model (epoch =  277, loss = 0.0905)\n",
            "Saving model (epoch =  278, loss = 0.0903)\n",
            "Saving model (epoch =  279, loss = 0.0900)\n",
            "Saving model (epoch =  280, loss = 0.0898)\n",
            "Saving model (epoch =  281, loss = 0.0896)\n",
            "Saving model (epoch =  282, loss = 0.0894)\n",
            "Saving model (epoch =  283, loss = 0.0893)\n",
            "Saving model (epoch =  284, loss = 0.0891)\n",
            "Saving model (epoch =  285, loss = 0.0889)\n",
            "Saving model (epoch =  286, loss = 0.0888)\n",
            "Saving model (epoch =  287, loss = 0.0886)\n",
            "Saving model (epoch =  288, loss = 0.0885)\n",
            "Saving model (epoch =  289, loss = 0.0885)\n",
            "Saving model (epoch =  290, loss = 0.0885)\n",
            "Saving model (epoch =  291, loss = 0.0884)\n",
            "Saving model (epoch =  294, loss = 0.0884)\n",
            "Saving model (epoch =  295, loss = 0.0884)\n",
            "Saving model (epoch =  296, loss = 0.0883)\n",
            "Saving model (epoch =  297, loss = 0.0883)\n",
            "Saving model (epoch =  298, loss = 0.0882)\n",
            "Saving model (epoch =  299, loss = 0.0882)\n",
            "Saving model (epoch =  301, loss = 0.0882)\n",
            "Saving model (epoch =  302, loss = 0.0881)\n",
            "Saving model (epoch =  303, loss = 0.0880)\n",
            "Saving model (epoch =  304, loss = 0.0878)\n",
            "Saving model (epoch =  305, loss = 0.0875)\n",
            "Saving model (epoch =  306, loss = 0.0872)\n",
            "Saving model (epoch =  307, loss = 0.0869)\n",
            "Saving model (epoch =  308, loss = 0.0865)\n",
            "Saving model (epoch =  309, loss = 0.0863)\n",
            "Saving model (epoch =  310, loss = 0.0860)\n",
            "Saving model (epoch =  311, loss = 0.0857)\n",
            "Saving model (epoch =  312, loss = 0.0854)\n",
            "Saving model (epoch =  313, loss = 0.0851)\n",
            "Saving model (epoch =  314, loss = 0.0848)\n",
            "Saving model (epoch =  315, loss = 0.0846)\n",
            "Saving model (epoch =  316, loss = 0.0843)\n",
            "Saving model (epoch =  317, loss = 0.0840)\n",
            "Saving model (epoch =  318, loss = 0.0837)\n",
            "Saving model (epoch =  319, loss = 0.0834)\n",
            "Saving model (epoch =  320, loss = 0.0831)\n",
            "Saving model (epoch =  321, loss = 0.0828)\n",
            "Saving model (epoch =  322, loss = 0.0826)\n",
            "Saving model (epoch =  323, loss = 0.0824)\n",
            "Saving model (epoch =  324, loss = 0.0822)\n",
            "Saving model (epoch =  325, loss = 0.0821)\n",
            "Saving model (epoch =  326, loss = 0.0820)\n",
            "Saving model (epoch =  327, loss = 0.0819)\n",
            "Saving model (epoch =  328, loss = 0.0819)\n",
            "Saving model (epoch =  329, loss = 0.0818)\n",
            "Saving model (epoch =  330, loss = 0.0817)\n",
            "Saving model (epoch =  331, loss = 0.0816)\n",
            "Saving model (epoch =  332, loss = 0.0816)\n",
            "Saving model (epoch =  333, loss = 0.0815)\n",
            "Saving model (epoch =  334, loss = 0.0815)\n",
            "Saving model (epoch =  335, loss = 0.0814)\n",
            "Saving model (epoch =  336, loss = 0.0814)\n",
            "Saving model (epoch =  337, loss = 0.0813)\n",
            "Saving model (epoch =  338, loss = 0.0812)\n",
            "Saving model (epoch =  339, loss = 0.0811)\n",
            "Saving model (epoch =  340, loss = 0.0810)\n",
            "Saving model (epoch =  341, loss = 0.0809)\n",
            "Saving model (epoch =  342, loss = 0.0808)\n",
            "Saving model (epoch =  343, loss = 0.0807)\n",
            "Saving model (epoch =  344, loss = 0.0805)\n",
            "Saving model (epoch =  345, loss = 0.0803)\n",
            "Saving model (epoch =  346, loss = 0.0801)\n",
            "Saving model (epoch =  347, loss = 0.0800)\n",
            "Saving model (epoch =  348, loss = 0.0798)\n",
            "Saving model (epoch =  349, loss = 0.0796)\n",
            "Saving model (epoch =  350, loss = 0.0793)\n",
            "Saving model (epoch =  351, loss = 0.0791)\n",
            "Saving model (epoch =  352, loss = 0.0789)\n",
            "Saving model (epoch =  353, loss = 0.0787)\n",
            "Saving model (epoch =  354, loss = 0.0786)\n",
            "Saving model (epoch =  355, loss = 0.0784)\n",
            "Saving model (epoch =  356, loss = 0.0781)\n",
            "Saving model (epoch =  357, loss = 0.0780)\n",
            "Saving model (epoch =  358, loss = 0.0777)\n",
            "Saving model (epoch =  359, loss = 0.0775)\n",
            "Saving model (epoch =  360, loss = 0.0773)\n",
            "Saving model (epoch =  361, loss = 0.0772)\n",
            "Saving model (epoch =  362, loss = 0.0771)\n",
            "Saving model (epoch =  363, loss = 0.0770)\n",
            "Saving model (epoch =  364, loss = 0.0770)\n",
            "Saving model (epoch =  365, loss = 0.0769)\n",
            "Saving model (epoch =  366, loss = 0.0768)\n",
            "Saving model (epoch =  367, loss = 0.0767)\n",
            "Saving model (epoch =  368, loss = 0.0766)\n",
            "Saving model (epoch =  369, loss = 0.0765)\n",
            "Saving model (epoch =  370, loss = 0.0764)\n",
            "Saving model (epoch =  371, loss = 0.0762)\n",
            "Saving model (epoch =  372, loss = 0.0760)\n",
            "Saving model (epoch =  373, loss = 0.0758)\n",
            "Saving model (epoch =  374, loss = 0.0756)\n",
            "Saving model (epoch =  375, loss = 0.0755)\n",
            "Saving model (epoch =  376, loss = 0.0753)\n",
            "Saving model (epoch =  377, loss = 0.0752)\n",
            "Saving model (epoch =  378, loss = 0.0750)\n",
            "Saving model (epoch =  379, loss = 0.0749)\n",
            "Saving model (epoch =  380, loss = 0.0748)\n",
            "Saving model (epoch =  381, loss = 0.0746)\n",
            "Saving model (epoch =  382, loss = 0.0744)\n",
            "Saving model (epoch =  383, loss = 0.0741)\n",
            "Saving model (epoch =  384, loss = 0.0739)\n",
            "Saving model (epoch =  385, loss = 0.0737)\n",
            "Saving model (epoch =  386, loss = 0.0735)\n",
            "Saving model (epoch =  387, loss = 0.0733)\n",
            "Saving model (epoch =  388, loss = 0.0732)\n",
            "Saving model (epoch =  389, loss = 0.0730)\n",
            "Saving model (epoch =  390, loss = 0.0729)\n",
            "Saving model (epoch =  391, loss = 0.0728)\n",
            "Saving model (epoch =  392, loss = 0.0728)\n",
            "Saving model (epoch =  393, loss = 0.0727)\n",
            "Saving model (epoch =  394, loss = 0.0725)\n",
            "Saving model (epoch =  395, loss = 0.0724)\n",
            "Saving model (epoch =  396, loss = 0.0721)\n",
            "Saving model (epoch =  397, loss = 0.0720)\n",
            "Saving model (epoch =  398, loss = 0.0718)\n",
            "Saving model (epoch =  399, loss = 0.0715)\n",
            "Saving model (epoch =  400, loss = 0.0713)\n",
            "Saving model (epoch =  401, loss = 0.0711)\n",
            "Saving model (epoch =  402, loss = 0.0709)\n",
            "Saving model (epoch =  403, loss = 0.0707)\n",
            "Saving model (epoch =  404, loss = 0.0705)\n",
            "Saving model (epoch =  405, loss = 0.0702)\n",
            "Saving model (epoch =  406, loss = 0.0701)\n",
            "Saving model (epoch =  407, loss = 0.0700)\n",
            "Saving model (epoch =  408, loss = 0.0698)\n",
            "Saving model (epoch =  409, loss = 0.0696)\n",
            "Saving model (epoch =  410, loss = 0.0695)\n",
            "Saving model (epoch =  411, loss = 0.0694)\n",
            "Saving model (epoch =  412, loss = 0.0694)\n",
            "Saving model (epoch =  413, loss = 0.0693)\n",
            "Saving model (epoch =  414, loss = 0.0692)\n",
            "Saving model (epoch =  415, loss = 0.0692)\n",
            "Saving model (epoch =  416, loss = 0.0691)\n",
            "Saving model (epoch =  417, loss = 0.0690)\n",
            "Saving model (epoch =  418, loss = 0.0689)\n",
            "Saving model (epoch =  419, loss = 0.0688)\n",
            "Saving model (epoch =  420, loss = 0.0686)\n",
            "Saving model (epoch =  421, loss = 0.0685)\n",
            "Saving model (epoch =  422, loss = 0.0683)\n",
            "Saving model (epoch =  423, loss = 0.0681)\n",
            "Saving model (epoch =  424, loss = 0.0679)\n",
            "Saving model (epoch =  425, loss = 0.0678)\n",
            "Saving model (epoch =  426, loss = 0.0676)\n",
            "Saving model (epoch =  427, loss = 0.0675)\n",
            "Saving model (epoch =  428, loss = 0.0673)\n",
            "Saving model (epoch =  429, loss = 0.0672)\n",
            "Saving model (epoch =  430, loss = 0.0670)\n",
            "Saving model (epoch =  431, loss = 0.0668)\n",
            "Saving model (epoch =  432, loss = 0.0666)\n",
            "Saving model (epoch =  433, loss = 0.0665)\n",
            "Saving model (epoch =  434, loss = 0.0663)\n",
            "Saving model (epoch =  435, loss = 0.0662)\n",
            "Saving model (epoch =  436, loss = 0.0661)\n",
            "Saving model (epoch =  437, loss = 0.0659)\n",
            "Saving model (epoch =  438, loss = 0.0658)\n",
            "Saving model (epoch =  439, loss = 0.0657)\n",
            "Saving model (epoch =  440, loss = 0.0655)\n",
            "Saving model (epoch =  441, loss = 0.0654)\n",
            "Saving model (epoch =  442, loss = 0.0653)\n",
            "Saving model (epoch =  443, loss = 0.0652)\n",
            "Saving model (epoch =  444, loss = 0.0650)\n",
            "Saving model (epoch =  445, loss = 0.0649)\n",
            "Saving model (epoch =  446, loss = 0.0648)\n",
            "Saving model (epoch =  447, loss = 0.0647)\n",
            "Saving model (epoch =  448, loss = 0.0647)\n",
            "Saving model (epoch =  449, loss = 0.0646)\n",
            "Saving model (epoch =  450, loss = 0.0646)\n",
            "Saving model (epoch =  451, loss = 0.0645)\n",
            "Saving model (epoch =  452, loss = 0.0645)\n",
            "Saving model (epoch =  453, loss = 0.0644)\n",
            "Saving model (epoch =  454, loss = 0.0644)\n",
            "Saving model (epoch =  455, loss = 0.0643)\n",
            "Saving model (epoch =  456, loss = 0.0643)\n",
            "Saving model (epoch =  457, loss = 0.0642)\n",
            "Saving model (epoch =  458, loss = 0.0641)\n",
            "Saving model (epoch =  459, loss = 0.0641)\n",
            "Saving model (epoch =  462, loss = 0.0641)\n",
            "Saving model (epoch =  463, loss = 0.0641)\n",
            "Saving model (epoch =  464, loss = 0.0640)\n",
            "Saving model (epoch =  465, loss = 0.0640)\n",
            "Saving model (epoch =  466, loss = 0.0640)\n",
            "Saving model (epoch =  467, loss = 0.0640)\n",
            "Saving model (epoch =  468, loss = 0.0639)\n",
            "Saving model (epoch =  469, loss = 0.0637)\n",
            "Saving model (epoch =  470, loss = 0.0636)\n",
            "Saving model (epoch =  471, loss = 0.0635)\n",
            "Saving model (epoch =  472, loss = 0.0633)\n",
            "Saving model (epoch =  473, loss = 0.0632)\n",
            "Saving model (epoch =  474, loss = 0.0630)\n",
            "Saving model (epoch =  475, loss = 0.0627)\n",
            "Saving model (epoch =  476, loss = 0.0625)\n",
            "Saving model (epoch =  477, loss = 0.0622)\n",
            "Saving model (epoch =  478, loss = 0.0620)\n",
            "Saving model (epoch =  479, loss = 0.0617)\n",
            "Saving model (epoch =  480, loss = 0.0615)\n",
            "Saving model (epoch =  481, loss = 0.0613)\n",
            "Saving model (epoch =  482, loss = 0.0611)\n",
            "Saving model (epoch =  483, loss = 0.0609)\n",
            "Saving model (epoch =  484, loss = 0.0606)\n",
            "Saving model (epoch =  485, loss = 0.0605)\n",
            "Saving model (epoch =  486, loss = 0.0604)\n",
            "Saving model (epoch =  487, loss = 0.0603)\n",
            "Saving model (epoch =  488, loss = 0.0603)\n",
            "Saving model (epoch =  489, loss = 0.0603)\n",
            "Saving model (epoch =  490, loss = 0.0602)\n",
            "Saving model (epoch =  491, loss = 0.0601)\n",
            "Saving model (epoch =  492, loss = 0.0601)\n",
            "Saving model (epoch =  494, loss = 0.0601)\n",
            "Saving model (epoch =  495, loss = 0.0600)\n",
            "Saving model (epoch =  496, loss = 0.0600)\n",
            "Saving model (epoch =  504, loss = 0.0599)\n",
            "Saving model (epoch =  505, loss = 0.0598)\n",
            "Saving model (epoch =  506, loss = 0.0596)\n",
            "Saving model (epoch =  507, loss = 0.0595)\n",
            "Saving model (epoch =  508, loss = 0.0594)\n",
            "Saving model (epoch =  509, loss = 0.0593)\n",
            "Saving model (epoch =  510, loss = 0.0593)\n",
            "Saving model (epoch =  511, loss = 0.0591)\n",
            "Saving model (epoch =  512, loss = 0.0590)\n",
            "Saving model (epoch =  513, loss = 0.0588)\n",
            "Saving model (epoch =  514, loss = 0.0586)\n",
            "Saving model (epoch =  515, loss = 0.0583)\n",
            "Saving model (epoch =  516, loss = 0.0581)\n",
            "Saving model (epoch =  517, loss = 0.0580)\n",
            "Saving model (epoch =  518, loss = 0.0579)\n",
            "Saving model (epoch =  519, loss = 0.0578)\n",
            "Saving model (epoch =  520, loss = 0.0577)\n",
            "Saving model (epoch =  521, loss = 0.0576)\n",
            "Saving model (epoch =  522, loss = 0.0575)\n",
            "Saving model (epoch =  523, loss = 0.0574)\n",
            "Saving model (epoch =  524, loss = 0.0572)\n",
            "Saving model (epoch =  525, loss = 0.0571)\n",
            "Saving model (epoch =  526, loss = 0.0569)\n",
            "Saving model (epoch =  527, loss = 0.0567)\n",
            "Saving model (epoch =  528, loss = 0.0565)\n",
            "Saving model (epoch =  529, loss = 0.0564)\n",
            "Saving model (epoch =  530, loss = 0.0562)\n",
            "Saving model (epoch =  531, loss = 0.0561)\n",
            "Saving model (epoch =  532, loss = 0.0560)\n",
            "Saving model (epoch =  533, loss = 0.0559)\n",
            "Saving model (epoch =  534, loss = 0.0559)\n",
            "Saving model (epoch =  535, loss = 0.0559)\n",
            "Saving model (epoch =  536, loss = 0.0558)\n",
            "Saving model (epoch =  537, loss = 0.0557)\n",
            "Saving model (epoch =  538, loss = 0.0557)\n",
            "Saving model (epoch =  539, loss = 0.0556)\n",
            "Saving model (epoch =  540, loss = 0.0556)\n",
            "Saving model (epoch =  541, loss = 0.0555)\n",
            "Saving model (epoch =  542, loss = 0.0555)\n",
            "Saving model (epoch =  545, loss = 0.0555)\n",
            "Saving model (epoch =  546, loss = 0.0555)\n",
            "Saving model (epoch =  547, loss = 0.0555)\n",
            "Saving model (epoch =  550, loss = 0.0554)\n",
            "Saving model (epoch =  551, loss = 0.0554)\n",
            "Saving model (epoch =  552, loss = 0.0553)\n",
            "Saving model (epoch =  553, loss = 0.0552)\n",
            "Saving model (epoch =  554, loss = 0.0552)\n",
            "Saving model (epoch =  555, loss = 0.0551)\n",
            "Saving model (epoch =  556, loss = 0.0551)\n",
            "Saving model (epoch =  557, loss = 0.0550)\n",
            "Saving model (epoch =  558, loss = 0.0550)\n",
            "Saving model (epoch =  559, loss = 0.0549)\n",
            "Saving model (epoch =  560, loss = 0.0548)\n",
            "Saving model (epoch =  561, loss = 0.0548)\n",
            "Saving model (epoch =  562, loss = 0.0546)\n",
            "Saving model (epoch =  563, loss = 0.0545)\n",
            "Saving model (epoch =  564, loss = 0.0544)\n",
            "Saving model (epoch =  565, loss = 0.0544)\n",
            "Saving model (epoch =  566, loss = 0.0543)\n",
            "Saving model (epoch =  567, loss = 0.0541)\n",
            "Saving model (epoch =  568, loss = 0.0540)\n",
            "Saving model (epoch =  569, loss = 0.0539)\n",
            "Saving model (epoch =  570, loss = 0.0537)\n",
            "Saving model (epoch =  571, loss = 0.0535)\n",
            "Saving model (epoch =  572, loss = 0.0533)\n",
            "Saving model (epoch =  573, loss = 0.0531)\n",
            "Saving model (epoch =  574, loss = 0.0529)\n",
            "Saving model (epoch =  575, loss = 0.0527)\n",
            "Saving model (epoch =  576, loss = 0.0525)\n",
            "Saving model (epoch =  577, loss = 0.0523)\n",
            "Saving model (epoch =  578, loss = 0.0521)\n",
            "Saving model (epoch =  579, loss = 0.0519)\n",
            "Saving model (epoch =  580, loss = 0.0518)\n",
            "Saving model (epoch =  581, loss = 0.0517)\n",
            "Saving model (epoch =  582, loss = 0.0516)\n",
            "Saving model (epoch =  583, loss = 0.0514)\n",
            "Saving model (epoch =  584, loss = 0.0512)\n",
            "Saving model (epoch =  585, loss = 0.0512)\n",
            "Saving model (epoch =  586, loss = 0.0510)\n",
            "Saving model (epoch =  587, loss = 0.0509)\n",
            "Saving model (epoch =  588, loss = 0.0508)\n",
            "Saving model (epoch =  589, loss = 0.0508)\n",
            "Saving model (epoch =  590, loss = 0.0507)\n",
            "Saving model (epoch =  591, loss = 0.0507)\n",
            "Saving model (epoch =  592, loss = 0.0506)\n",
            "Saving model (epoch =  593, loss = 0.0506)\n",
            "Saving model (epoch =  618, loss = 0.0506)\n",
            "Saving model (epoch =  619, loss = 0.0505)\n",
            "Saving model (epoch =  620, loss = 0.0505)\n",
            "Saving model (epoch =  621, loss = 0.0504)\n",
            "Saving model (epoch =  622, loss = 0.0503)\n",
            "Saving model (epoch =  623, loss = 0.0502)\n",
            "Saving model (epoch =  624, loss = 0.0501)\n",
            "Saving model (epoch =  625, loss = 0.0500)\n",
            "Saving model (epoch =  626, loss = 0.0499)\n",
            "Saving model (epoch =  627, loss = 0.0498)\n",
            "Saving model (epoch =  628, loss = 0.0498)\n",
            "Saving model (epoch =  629, loss = 0.0497)\n",
            "Saving model (epoch =  630, loss = 0.0495)\n",
            "Saving model (epoch =  631, loss = 0.0493)\n",
            "Saving model (epoch =  632, loss = 0.0491)\n",
            "Saving model (epoch =  633, loss = 0.0490)\n",
            "Saving model (epoch =  634, loss = 0.0488)\n",
            "Saving model (epoch =  635, loss = 0.0487)\n",
            "Saving model (epoch =  636, loss = 0.0485)\n",
            "Saving model (epoch =  637, loss = 0.0484)\n",
            "Saving model (epoch =  638, loss = 0.0482)\n",
            "Saving model (epoch =  639, loss = 0.0480)\n",
            "Saving model (epoch =  640, loss = 0.0479)\n",
            "Saving model (epoch =  641, loss = 0.0478)\n",
            "Saving model (epoch =  642, loss = 0.0477)\n",
            "Saving model (epoch =  643, loss = 0.0476)\n",
            "Saving model (epoch =  644, loss = 0.0475)\n",
            "Saving model (epoch =  645, loss = 0.0475)\n",
            "Saving model (epoch =  646, loss = 0.0474)\n",
            "Saving model (epoch =  647, loss = 0.0474)\n",
            "Saving model (epoch =  648, loss = 0.0474)\n",
            "Saving model (epoch =  649, loss = 0.0473)\n",
            "Saving model (epoch =  650, loss = 0.0472)\n",
            "Saving model (epoch =  651, loss = 0.0472)\n",
            "Saving model (epoch =  652, loss = 0.0471)\n",
            "Saving model (epoch =  653, loss = 0.0471)\n",
            "Saving model (epoch =  654, loss = 0.0470)\n",
            "Saving model (epoch =  655, loss = 0.0469)\n",
            "Saving model (epoch =  656, loss = 0.0468)\n",
            "Saving model (epoch =  657, loss = 0.0468)\n",
            "Saving model (epoch =  682, loss = 0.0468)\n",
            "Saving model (epoch =  683, loss = 0.0466)\n",
            "Saving model (epoch =  684, loss = 0.0465)\n",
            "Saving model (epoch =  685, loss = 0.0465)\n",
            "Saving model (epoch =  686, loss = 0.0464)\n",
            "Saving model (epoch =  687, loss = 0.0463)\n",
            "Saving model (epoch =  688, loss = 0.0463)\n",
            "Saving model (epoch =  689, loss = 0.0462)\n",
            "Saving model (epoch =  690, loss = 0.0461)\n",
            "Saving model (epoch =  691, loss = 0.0460)\n",
            "Saving model (epoch =  692, loss = 0.0459)\n",
            "Saving model (epoch =  693, loss = 0.0458)\n",
            "Saving model (epoch =  694, loss = 0.0458)\n",
            "Saving model (epoch =  695, loss = 0.0457)\n",
            "Saving model (epoch =  696, loss = 0.0457)\n",
            "Saving model (epoch =  697, loss = 0.0456)\n",
            "Saving model (epoch =  698, loss = 0.0456)\n",
            "Saving model (epoch =  699, loss = 0.0456)\n",
            "Saving model (epoch =  700, loss = 0.0456)\n",
            "Saving model (epoch =  702, loss = 0.0456)\n",
            "Saving model (epoch =  703, loss = 0.0456)\n",
            "Saving model (epoch =  707, loss = 0.0455)\n",
            "Saving model (epoch =  708, loss = 0.0455)\n",
            "Saving model (epoch =  709, loss = 0.0455)\n",
            "Saving model (epoch =  714, loss = 0.0455)\n",
            "Saving model (epoch =  717, loss = 0.0454)\n",
            "Saving model (epoch =  718, loss = 0.0454)\n",
            "Saving model (epoch =  721, loss = 0.0454)\n",
            "Saving model (epoch =  722, loss = 0.0454)\n",
            "Saving model (epoch =  723, loss = 0.0453)\n",
            "Saving model (epoch =  724, loss = 0.0453)\n",
            "Saving model (epoch =  725, loss = 0.0453)\n",
            "Saving model (epoch =  726, loss = 0.0452)\n",
            "Saving model (epoch =  727, loss = 0.0451)\n",
            "Saving model (epoch =  728, loss = 0.0450)\n",
            "Saving model (epoch =  729, loss = 0.0449)\n",
            "Saving model (epoch =  730, loss = 0.0448)\n",
            "Saving model (epoch =  731, loss = 0.0447)\n",
            "Saving model (epoch =  732, loss = 0.0445)\n",
            "Saving model (epoch =  733, loss = 0.0444)\n",
            "Saving model (epoch =  734, loss = 0.0442)\n",
            "Saving model (epoch =  735, loss = 0.0441)\n",
            "Saving model (epoch =  736, loss = 0.0440)\n",
            "Saving model (epoch =  737, loss = 0.0439)\n",
            "Saving model (epoch =  738, loss = 0.0438)\n",
            "Saving model (epoch =  739, loss = 0.0437)\n",
            "Saving model (epoch =  740, loss = 0.0436)\n",
            "Saving model (epoch =  741, loss = 0.0434)\n",
            "Saving model (epoch =  742, loss = 0.0433)\n",
            "Saving model (epoch =  743, loss = 0.0432)\n",
            "Saving model (epoch =  744, loss = 0.0432)\n",
            "Saving model (epoch =  745, loss = 0.0432)\n",
            "Saving model (epoch =  764, loss = 0.0432)\n",
            "Saving model (epoch =  765, loss = 0.0431)\n",
            "Saving model (epoch =  766, loss = 0.0431)\n",
            "Saving model (epoch =  767, loss = 0.0430)\n",
            "Saving model (epoch =  768, loss = 0.0430)\n",
            "Saving model (epoch =  769, loss = 0.0429)\n",
            "Saving model (epoch =  770, loss = 0.0428)\n",
            "Saving model (epoch =  771, loss = 0.0427)\n",
            "Saving model (epoch =  772, loss = 0.0426)\n",
            "Saving model (epoch =  773, loss = 0.0426)\n",
            "Saving model (epoch =  774, loss = 0.0425)\n",
            "Saving model (epoch =  775, loss = 0.0424)\n",
            "Saving model (epoch =  776, loss = 0.0423)\n",
            "Saving model (epoch =  777, loss = 0.0423)\n",
            "Saving model (epoch =  778, loss = 0.0422)\n",
            "Saving model (epoch =  779, loss = 0.0422)\n",
            "Saving model (epoch =  780, loss = 0.0422)\n",
            "Saving model (epoch =  781, loss = 0.0421)\n",
            "Saving model (epoch =  783, loss = 0.0421)\n",
            "Saving model (epoch =  785, loss = 0.0421)\n",
            "Saving model (epoch =  786, loss = 0.0421)\n",
            "Saving model (epoch =  787, loss = 0.0421)\n",
            "Saving model (epoch =  788, loss = 0.0420)\n",
            "Saving model (epoch =  789, loss = 0.0420)\n",
            "Saving model (epoch =  790, loss = 0.0420)\n",
            "Saving model (epoch =  791, loss = 0.0419)\n",
            "Saving model (epoch =  792, loss = 0.0419)\n",
            "Saving model (epoch =  793, loss = 0.0419)\n",
            "Saving model (epoch =  794, loss = 0.0418)\n",
            "Saving model (epoch =  795, loss = 0.0418)\n",
            "Saving model (epoch =  796, loss = 0.0418)\n",
            "Saving model (epoch =  797, loss = 0.0418)\n",
            "Saving model (epoch =  807, loss = 0.0418)\n",
            "Saving model (epoch =  808, loss = 0.0417)\n",
            "Saving model (epoch =  809, loss = 0.0416)\n",
            "Saving model (epoch =  810, loss = 0.0415)\n",
            "Saving model (epoch =  811, loss = 0.0414)\n",
            "Saving model (epoch =  812, loss = 0.0413)\n",
            "Saving model (epoch =  813, loss = 0.0412)\n",
            "Saving model (epoch =  814, loss = 0.0412)\n",
            "Saving model (epoch =  815, loss = 0.0412)\n",
            "Saving model (epoch =  816, loss = 0.0411)\n",
            "Saving model (epoch =  817, loss = 0.0411)\n",
            "Saving model (epoch =  818, loss = 0.0410)\n",
            "Saving model (epoch =  819, loss = 0.0410)\n",
            "Saving model (epoch =  820, loss = 0.0409)\n",
            "Saving model (epoch =  821, loss = 0.0409)\n",
            "Saving model (epoch =  822, loss = 0.0408)\n",
            "Saving model (epoch =  823, loss = 0.0408)\n",
            "Saving model (epoch =  824, loss = 0.0408)\n",
            "Saving model (epoch =  837, loss = 0.0408)\n",
            "Saving model (epoch =  838, loss = 0.0407)\n",
            "Saving model (epoch =  839, loss = 0.0406)\n",
            "Saving model (epoch =  840, loss = 0.0405)\n",
            "Saving model (epoch =  841, loss = 0.0404)\n",
            "Saving model (epoch =  842, loss = 0.0403)\n",
            "Saving model (epoch =  843, loss = 0.0402)\n",
            "Saving model (epoch =  844, loss = 0.0402)\n",
            "Saving model (epoch =  845, loss = 0.0401)\n",
            "Saving model (epoch =  846, loss = 0.0400)\n",
            "Saving model (epoch =  847, loss = 0.0399)\n",
            "Saving model (epoch =  848, loss = 0.0398)\n",
            "Saving model (epoch =  849, loss = 0.0397)\n",
            "Saving model (epoch =  850, loss = 0.0396)\n",
            "Saving model (epoch =  851, loss = 0.0395)\n",
            "Saving model (epoch =  852, loss = 0.0394)\n",
            "Saving model (epoch =  853, loss = 0.0394)\n",
            "Saving model (epoch =  854, loss = 0.0393)\n",
            "Saving model (epoch =  855, loss = 0.0392)\n",
            "Saving model (epoch =  856, loss = 0.0391)\n",
            "Saving model (epoch =  857, loss = 0.0391)\n",
            "Saving model (epoch =  858, loss = 0.0390)\n",
            "Saving model (epoch =  859, loss = 0.0390)\n",
            "Saving model (epoch =  878, loss = 0.0390)\n",
            "Saving model (epoch =  879, loss = 0.0389)\n",
            "Saving model (epoch =  880, loss = 0.0389)\n",
            "Saving model (epoch =  881, loss = 0.0389)\n",
            "Saving model (epoch =  882, loss = 0.0388)\n",
            "Saving model (epoch =  883, loss = 0.0388)\n",
            "Saving model (epoch =  884, loss = 0.0388)\n",
            "Saving model (epoch =  885, loss = 0.0388)\n",
            "Saving model (epoch =  886, loss = 0.0388)\n",
            "Saving model (epoch =  887, loss = 0.0387)\n",
            "Saving model (epoch =  888, loss = 0.0387)\n",
            "Saving model (epoch =  889, loss = 0.0387)\n",
            "Saving model (epoch =  890, loss = 0.0386)\n",
            "Saving model (epoch =  891, loss = 0.0386)\n",
            "Saving model (epoch =  892, loss = 0.0386)\n",
            "Saving model (epoch =  895, loss = 0.0386)\n",
            "Saving model (epoch =  896, loss = 0.0386)\n",
            "Saving model (epoch =  934, loss = 0.0385)\n",
            "Saving model (epoch =  935, loss = 0.0385)\n",
            "Saving model (epoch =  936, loss = 0.0384)\n",
            "Saving model (epoch =  937, loss = 0.0383)\n",
            "Saving model (epoch =  938, loss = 0.0383)\n",
            "Saving model (epoch =  939, loss = 0.0382)\n",
            "Saving model (epoch =  940, loss = 0.0382)\n",
            "Saving model (epoch =  941, loss = 0.0382)\n",
            "Saving model (epoch =  942, loss = 0.0382)\n",
            "Saving model (epoch =  943, loss = 0.0381)\n",
            "Saving model (epoch =  944, loss = 0.0381)\n",
            "Saving model (epoch =  945, loss = 0.0380)\n",
            "Saving model (epoch =  946, loss = 0.0380)\n",
            "Saving model (epoch =  947, loss = 0.0379)\n",
            "Saving model (epoch =  948, loss = 0.0379)\n",
            "Saving model (epoch =  949, loss = 0.0378)\n",
            "Saving model (epoch =  950, loss = 0.0378)\n",
            "Saving model (epoch =  951, loss = 0.0378)\n",
            "Saving model (epoch =  977, loss = 0.0378)\n",
            "Saving model (epoch =  978, loss = 0.0377)\n",
            "Saving model (epoch =  979, loss = 0.0376)\n",
            "Saving model (epoch =  980, loss = 0.0376)\n",
            "Saving model (epoch =  981, loss = 0.0375)\n",
            "Saving model (epoch =  982, loss = 0.0375)\n",
            "Saving model (epoch =  983, loss = 0.0374)\n",
            "Saving model (epoch =  984, loss = 0.0374)\n",
            "Saving model (epoch =  985, loss = 0.0373)\n",
            "Saving model (epoch =  986, loss = 0.0373)\n",
            "Saving model (epoch =  987, loss = 0.0372)\n",
            "Saving model (epoch =  988, loss = 0.0372)\n",
            "Saving model (epoch =  989, loss = 0.0371)\n",
            "Saving model (epoch =  990, loss = 0.0371)\n",
            "Saving model (epoch =  991, loss = 0.0371)\n",
            "Saving model (epoch =  992, loss = 0.0370)\n",
            "Saving model (epoch = 1030, loss = 0.0370)\n",
            "Saving model (epoch = 1031, loss = 0.0369)\n",
            "Saving model (epoch = 1032, loss = 0.0368)\n",
            "Saving model (epoch = 1033, loss = 0.0367)\n",
            "Saving model (epoch = 1034, loss = 0.0366)\n",
            "Saving model (epoch = 1035, loss = 0.0365)\n",
            "Saving model (epoch = 1036, loss = 0.0365)\n",
            "Saving model (epoch = 1037, loss = 0.0364)\n",
            "Saving model (epoch = 1038, loss = 0.0364)\n",
            "Saving model (epoch = 1039, loss = 0.0364)\n",
            "Saving model (epoch = 1040, loss = 0.0364)\n",
            "Saving model (epoch = 1041, loss = 0.0364)\n",
            "Saving model (epoch = 1047, loss = 0.0363)\n",
            "Saving model (epoch = 1048, loss = 0.0363)\n",
            "Saving model (epoch = 1049, loss = 0.0363)\n",
            "Saving model (epoch = 1050, loss = 0.0363)\n",
            "Saving model (epoch = 1051, loss = 0.0363)\n",
            "Saving model (epoch = 1054, loss = 0.0362)\n",
            "Saving model (epoch = 1055, loss = 0.0362)\n",
            "Saving model (epoch = 1070, loss = 0.0362)\n",
            "Saving model (epoch = 1071, loss = 0.0362)\n",
            "Saving model (epoch = 1072, loss = 0.0362)\n",
            "Saving model (epoch = 1073, loss = 0.0362)\n",
            "Saving model (epoch = 1074, loss = 0.0362)\n",
            "Saving model (epoch = 1075, loss = 0.0362)\n",
            "Saving model (epoch = 1077, loss = 0.0361)\n",
            "Saving model (epoch = 1078, loss = 0.0361)\n",
            "Saving model (epoch = 1079, loss = 0.0361)\n",
            "Saving model (epoch = 1080, loss = 0.0361)\n",
            "Saving model (epoch = 1081, loss = 0.0360)\n",
            "Saving model (epoch = 1082, loss = 0.0360)\n",
            "Saving model (epoch = 1083, loss = 0.0359)\n",
            "Saving model (epoch = 1084, loss = 0.0359)\n",
            "Saving model (epoch = 1085, loss = 0.0358)\n",
            "Saving model (epoch = 1086, loss = 0.0358)\n",
            "Saving model (epoch = 1087, loss = 0.0358)\n",
            "Saving model (epoch = 1088, loss = 0.0357)\n",
            "Saving model (epoch = 1089, loss = 0.0357)\n",
            "Saving model (epoch = 1090, loss = 0.0356)\n",
            "Saving model (epoch = 1091, loss = 0.0356)\n",
            "Saving model (epoch = 1092, loss = 0.0356)\n",
            "Saving model (epoch = 1093, loss = 0.0356)\n",
            "Saving model (epoch = 1094, loss = 0.0356)\n",
            "Saving model (epoch = 1095, loss = 0.0356)\n",
            "Saving model (epoch = 1096, loss = 0.0355)\n",
            "Saving model (epoch = 1097, loss = 0.0355)\n",
            "Saving model (epoch = 1098, loss = 0.0354)\n",
            "Saving model (epoch = 1099, loss = 0.0354)\n",
            "Saving model (epoch = 1100, loss = 0.0354)\n",
            "Saving model (epoch = 1101, loss = 0.0354)\n",
            "Saving model (epoch = 1102, loss = 0.0353)\n",
            "Saving model (epoch = 1103, loss = 0.0353)\n",
            "Saving model (epoch = 1104, loss = 0.0353)\n",
            "Saving model (epoch = 1105, loss = 0.0353)\n",
            "Saving model (epoch = 1106, loss = 0.0353)\n",
            "Saving model (epoch = 1107, loss = 0.0352)\n",
            "Saving model (epoch = 1108, loss = 0.0352)\n",
            "Saving model (epoch = 1109, loss = 0.0352)\n",
            "Saving model (epoch = 1110, loss = 0.0352)\n",
            "Saving model (epoch = 1129, loss = 0.0352)\n",
            "Saving model (epoch = 1130, loss = 0.0352)\n",
            "Saving model (epoch = 1131, loss = 0.0352)\n",
            "Saving model (epoch = 1132, loss = 0.0351)\n",
            "Saving model (epoch = 1133, loss = 0.0351)\n",
            "Saving model (epoch = 1134, loss = 0.0351)\n",
            "Saving model (epoch = 1135, loss = 0.0350)\n",
            "Saving model (epoch = 1136, loss = 0.0350)\n",
            "Saving model (epoch = 1137, loss = 0.0350)\n",
            "Saving model (epoch = 1138, loss = 0.0349)\n",
            "Saving model (epoch = 1139, loss = 0.0349)\n",
            "Saving model (epoch = 1140, loss = 0.0349)\n",
            "Saving model (epoch = 1141, loss = 0.0348)\n",
            "Saving model (epoch = 1142, loss = 0.0348)\n",
            "Saving model (epoch = 1143, loss = 0.0348)\n",
            "Saving model (epoch = 1145, loss = 0.0348)\n",
            "Saving model (epoch = 1146, loss = 0.0348)\n",
            "Saving model (epoch = 1147, loss = 0.0347)\n",
            "Saving model (epoch = 1148, loss = 0.0347)\n",
            "Saving model (epoch = 1149, loss = 0.0347)\n",
            "Saving model (epoch = 1150, loss = 0.0347)\n",
            "Saving model (epoch = 1180, loss = 0.0347)\n",
            "Saving model (epoch = 1181, loss = 0.0346)\n",
            "Saving model (epoch = 1182, loss = 0.0346)\n",
            "Saving model (epoch = 1183, loss = 0.0346)\n",
            "Saving model (epoch = 1184, loss = 0.0345)\n",
            "Saving model (epoch = 1185, loss = 0.0345)\n",
            "Saving model (epoch = 1186, loss = 0.0345)\n",
            "Saving model (epoch = 1187, loss = 0.0344)\n",
            "Saving model (epoch = 1188, loss = 0.0344)\n",
            "Saving model (epoch = 1189, loss = 0.0343)\n",
            "Saving model (epoch = 1190, loss = 0.0343)\n",
            "Saving model (epoch = 1191, loss = 0.0343)\n",
            "Saving model (epoch = 1192, loss = 0.0342)\n",
            "Saving model (epoch = 1193, loss = 0.0342)\n",
            "Saving model (epoch = 1194, loss = 0.0341)\n",
            "Saving model (epoch = 1195, loss = 0.0340)\n",
            "Saving model (epoch = 1196, loss = 0.0340)\n",
            "Saving model (epoch = 1197, loss = 0.0339)\n",
            "Saving model (epoch = 1198, loss = 0.0339)\n",
            "Saving model (epoch = 1199, loss = 0.0339)\n",
            "Saving model (epoch = 1200, loss = 0.0339)\n",
            "Saving model (epoch = 1255, loss = 0.0339)\n",
            "Saving model (epoch = 1256, loss = 0.0338)\n",
            "Saving model (epoch = 1257, loss = 0.0337)\n",
            "Saving model (epoch = 1258, loss = 0.0337)\n",
            "Saving model (epoch = 1259, loss = 0.0336)\n",
            "Saving model (epoch = 1260, loss = 0.0336)\n",
            "Saving model (epoch = 1261, loss = 0.0336)\n",
            "Saving model (epoch = 1262, loss = 0.0336)\n",
            "Saving model (epoch = 1266, loss = 0.0336)\n",
            "Saving model (epoch = 1267, loss = 0.0335)\n",
            "Saving model (epoch = 1268, loss = 0.0335)\n",
            "Saving model (epoch = 1279, loss = 0.0335)\n",
            "Saving model (epoch = 1280, loss = 0.0335)\n",
            "Saving model (epoch = 1281, loss = 0.0335)\n",
            "Saving model (epoch = 1282, loss = 0.0334)\n",
            "Saving model (epoch = 1283, loss = 0.0334)\n",
            "Saving model (epoch = 1284, loss = 0.0333)\n",
            "Saving model (epoch = 1285, loss = 0.0332)\n",
            "Saving model (epoch = 1286, loss = 0.0332)\n",
            "Saving model (epoch = 1287, loss = 0.0331)\n",
            "Saving model (epoch = 1288, loss = 0.0331)\n",
            "Saving model (epoch = 1289, loss = 0.0330)\n",
            "Saving model (epoch = 1290, loss = 0.0330)\n",
            "Saving model (epoch = 1291, loss = 0.0329)\n",
            "Saving model (epoch = 1292, loss = 0.0329)\n",
            "Saving model (epoch = 1293, loss = 0.0328)\n",
            "Saving model (epoch = 1294, loss = 0.0328)\n",
            "Saving model (epoch = 1295, loss = 0.0327)\n",
            "Saving model (epoch = 1296, loss = 0.0327)\n",
            "Saving model (epoch = 1297, loss = 0.0327)\n",
            "Saving model (epoch = 1298, loss = 0.0326)\n",
            "Saving model (epoch = 1299, loss = 0.0326)\n",
            "Saving model (epoch = 1300, loss = 0.0326)\n",
            "Saving model (epoch = 1301, loss = 0.0326)\n",
            "Saving model (epoch = 1302, loss = 0.0326)\n",
            "Saving model (epoch = 1303, loss = 0.0326)\n",
            "Saving model (epoch = 1304, loss = 0.0326)\n",
            "Saving model (epoch = 1310, loss = 0.0326)\n",
            "Saving model (epoch = 1311, loss = 0.0325)\n",
            "Saving model (epoch = 1312, loss = 0.0325)\n",
            "Saving model (epoch = 1313, loss = 0.0325)\n",
            "Saving model (epoch = 1314, loss = 0.0325)\n",
            "Saving model (epoch = 1315, loss = 0.0325)\n",
            "Saving model (epoch = 1318, loss = 0.0325)\n",
            "Saving model (epoch = 1319, loss = 0.0325)\n",
            "Saving model (epoch = 1365, loss = 0.0324)\n",
            "Saving model (epoch = 1366, loss = 0.0324)\n",
            "Saving model (epoch = 1367, loss = 0.0324)\n",
            "Saving model (epoch = 1368, loss = 0.0323)\n",
            "Saving model (epoch = 1369, loss = 0.0323)\n",
            "Saving model (epoch = 1370, loss = 0.0323)\n",
            "Saving model (epoch = 1371, loss = 0.0323)\n",
            "Saving model (epoch = 1372, loss = 0.0323)\n",
            "Saving model (epoch = 1376, loss = 0.0323)\n",
            "Saving model (epoch = 1377, loss = 0.0323)\n",
            "Saving model (epoch = 1379, loss = 0.0323)\n",
            "Saving model (epoch = 1422, loss = 0.0322)\n",
            "Saving model (epoch = 1423, loss = 0.0322)\n",
            "Saving model (epoch = 1424, loss = 0.0322)\n",
            "Saving model (epoch = 1425, loss = 0.0321)\n",
            "Saving model (epoch = 1426, loss = 0.0321)\n",
            "Saving model (epoch = 1427, loss = 0.0321)\n",
            "Saving model (epoch = 1439, loss = 0.0321)\n",
            "Saving model (epoch = 1440, loss = 0.0320)\n",
            "Saving model (epoch = 1441, loss = 0.0320)\n",
            "Saving model (epoch = 1442, loss = 0.0319)\n",
            "Saving model (epoch = 1443, loss = 0.0319)\n",
            "Saving model (epoch = 1444, loss = 0.0319)\n",
            "Saving model (epoch = 1445, loss = 0.0318)\n",
            "Saving model (epoch = 1446, loss = 0.0318)\n",
            "Saving model (epoch = 1447, loss = 0.0317)\n",
            "Saving model (epoch = 1448, loss = 0.0317)\n",
            "Saving model (epoch = 1449, loss = 0.0316)\n",
            "Saving model (epoch = 1450, loss = 0.0316)\n",
            "Saving model (epoch = 1451, loss = 0.0315)\n",
            "Saving model (epoch = 1452, loss = 0.0315)\n",
            "Saving model (epoch = 1453, loss = 0.0314)\n",
            "Saving model (epoch = 1454, loss = 0.0314)\n",
            "Saving model (epoch = 1569, loss = 0.0314)\n",
            "Saving model (epoch = 1570, loss = 0.0314)\n",
            "Saving model (epoch = 1571, loss = 0.0313)\n",
            "Saving model (epoch = 1572, loss = 0.0313)\n",
            "Saving model (epoch = 1573, loss = 0.0312)\n",
            "Saving model (epoch = 1574, loss = 0.0312)\n",
            "Saving model (epoch = 1575, loss = 0.0312)\n",
            "Saving model (epoch = 1576, loss = 0.0311)\n",
            "Saving model (epoch = 1577, loss = 0.0311)\n",
            "Saving model (epoch = 1578, loss = 0.0311)\n",
            "Saving model (epoch = 1579, loss = 0.0311)\n",
            "Saving model (epoch = 1580, loss = 0.0311)\n",
            "Saving model (epoch = 1581, loss = 0.0311)\n",
            "Saving model (epoch = 1582, loss = 0.0310)\n",
            "Saving model (epoch = 1583, loss = 0.0310)\n",
            "Saving model (epoch = 1584, loss = 0.0310)\n",
            "Saving model (epoch = 1585, loss = 0.0310)\n",
            "Saving model (epoch = 1640, loss = 0.0310)\n",
            "Saving model (epoch = 1641, loss = 0.0309)\n",
            "Saving model (epoch = 1642, loss = 0.0309)\n",
            "Saving model (epoch = 1643, loss = 0.0308)\n",
            "Saving model (epoch = 1644, loss = 0.0308)\n",
            "Saving model (epoch = 1645, loss = 0.0308)\n",
            "Saving model (epoch = 1646, loss = 0.0307)\n",
            "Saving model (epoch = 1684, loss = 0.0307)\n",
            "Saving model (epoch = 1685, loss = 0.0307)\n",
            "Saving model (epoch = 1687, loss = 0.0307)\n",
            "Saving model (epoch = 1688, loss = 0.0307)\n",
            "Saving model (epoch = 1689, loss = 0.0307)\n",
            "Saving model (epoch = 1690, loss = 0.0307)\n",
            "Saving model (epoch = 1691, loss = 0.0307)\n",
            "Saving model (epoch = 1692, loss = 0.0306)\n",
            "Saving model (epoch = 1693, loss = 0.0306)\n",
            "Saving model (epoch = 1694, loss = 0.0306)\n",
            "Saving model (epoch = 1695, loss = 0.0306)\n",
            "Saving model (epoch = 1696, loss = 0.0306)\n",
            "Saving model (epoch = 1697, loss = 0.0306)\n",
            "Saving model (epoch = 1698, loss = 0.0306)\n",
            "Saving model (epoch = 1699, loss = 0.0306)\n",
            "Saving model (epoch = 1700, loss = 0.0306)\n",
            "Saving model (epoch = 1701, loss = 0.0305)\n",
            "Saving model (epoch = 1702, loss = 0.0305)\n",
            "Saving model (epoch = 1703, loss = 0.0305)\n",
            "Saving model (epoch = 1704, loss = 0.0304)\n",
            "Saving model (epoch = 1705, loss = 0.0304)\n",
            "Saving model (epoch = 1706, loss = 0.0304)\n",
            "Saving model (epoch = 1707, loss = 0.0304)\n",
            "Saving model (epoch = 1708, loss = 0.0304)\n",
            "Saving model (epoch = 1709, loss = 0.0303)\n",
            "Saving model (epoch = 1712, loss = 0.0303)\n",
            "Saving model (epoch = 1713, loss = 0.0303)\n",
            "Saving model (epoch = 1714, loss = 0.0303)\n",
            "Saving model (epoch = 1715, loss = 0.0303)\n",
            "Saving model (epoch = 1716, loss = 0.0303)\n",
            "Saving model (epoch = 1719, loss = 0.0303)\n",
            "Saving model (epoch = 1720, loss = 0.0303)\n",
            "Saving model (epoch = 1721, loss = 0.0303)\n",
            "Saving model (epoch = 1722, loss = 0.0303)\n",
            "Saving model (epoch = 1723, loss = 0.0303)\n",
            "Saving model (epoch = 1789, loss = 0.0302)\n",
            "Saving model (epoch = 1790, loss = 0.0302)\n",
            "Saving model (epoch = 1791, loss = 0.0302)\n",
            "Saving model (epoch = 1792, loss = 0.0302)\n",
            "Saving model (epoch = 1797, loss = 0.0302)\n",
            "Saving model (epoch = 1820, loss = 0.0302)\n",
            "Saving model (epoch = 1821, loss = 0.0301)\n",
            "Saving model (epoch = 1822, loss = 0.0301)\n",
            "Saving model (epoch = 1823, loss = 0.0300)\n",
            "Saving model (epoch = 1824, loss = 0.0300)\n",
            "Saving model (epoch = 1825, loss = 0.0299)\n",
            "Saving model (epoch = 1826, loss = 0.0299)\n",
            "Saving model (epoch = 1827, loss = 0.0299)\n",
            "Saving model (epoch = 1828, loss = 0.0299)\n",
            "Saving model (epoch = 1880, loss = 0.0299)\n",
            "Saving model (epoch = 1881, loss = 0.0299)\n",
            "Saving model (epoch = 1882, loss = 0.0299)\n",
            "Saving model (epoch = 1892, loss = 0.0298)\n",
            "Saving model (epoch = 1933, loss = 0.0298)\n",
            "Saving model (epoch = 1934, loss = 0.0298)\n",
            "Saving model (epoch = 1935, loss = 0.0298)\n",
            "Saving model (epoch = 1944, loss = 0.0298)\n",
            "Saving model (epoch = 1945, loss = 0.0298)\n",
            "Saving model (epoch = 1946, loss = 0.0298)\n",
            "Saving model (epoch = 1947, loss = 0.0298)\n",
            "Saving model (epoch = 1948, loss = 0.0298)\n",
            "Saving model (epoch = 1949, loss = 0.0298)\n",
            "Saving model (epoch = 1950, loss = 0.0298)\n",
            "Saving model (epoch = 1951, loss = 0.0298)\n",
            "Saving model (epoch = 1954, loss = 0.0297)\n",
            "Saving model (epoch = 1955, loss = 0.0297)\n",
            "Saving model (epoch = 1956, loss = 0.0297)\n",
            "Saving model (epoch = 1957, loss = 0.0297)\n",
            "Saving model (epoch = 1958, loss = 0.0296)\n",
            "Saving model (epoch = 1959, loss = 0.0296)\n",
            "Saving model (epoch = 1960, loss = 0.0296)\n",
            "Saving model (epoch = 1961, loss = 0.0296)\n",
            "Saving model (epoch = 1962, loss = 0.0296)\n",
            "Saving model (epoch = 1963, loss = 0.0295)\n",
            "Saving model (epoch = 1964, loss = 0.0295)\n",
            "Saving model (epoch = 1965, loss = 0.0295)\n",
            "Saving model (epoch = 1966, loss = 0.0294)\n",
            "Saving model (epoch = 1967, loss = 0.0294)\n",
            "Saving model (epoch = 1968, loss = 0.0294)\n",
            "Saving model (epoch = 1969, loss = 0.0294)\n",
            "Saving model (epoch = 1970, loss = 0.0293)\n",
            "Saving model (epoch = 1971, loss = 0.0293)\n",
            "Saving model (epoch = 1972, loss = 0.0293)\n",
            "Saving model (epoch = 1973, loss = 0.0293)\n",
            "Saving model (epoch = 1974, loss = 0.0293)\n",
            "Saving model (epoch = 1975, loss = 0.0293)\n",
            "Saving model (epoch = 1976, loss = 0.0293)\n",
            "Saving model (epoch = 1977, loss = 0.0292)\n",
            "Saving model (epoch = 1978, loss = 0.0292)\n",
            "Saving model (epoch = 1979, loss = 0.0292)\n",
            "Saving model (epoch = 1980, loss = 0.0292)\n",
            "Saving model (epoch = 1981, loss = 0.0292)\n",
            "Saving model (epoch = 1982, loss = 0.0292)\n",
            "Saving model (epoch = 1983, loss = 0.0292)\n",
            "Saving model (epoch = 1984, loss = 0.0292)\n",
            "Saving model (epoch = 1998, loss = 0.0292)\n",
            "Saving model (epoch = 1999, loss = 0.0292)\n",
            "Saving model (epoch = 2001, loss = 0.0292)\n",
            "Saving model (epoch = 2003, loss = 0.0291)\n",
            "Saving model (epoch = 2022, loss = 0.0291)\n",
            "Saving model (epoch = 2023, loss = 0.0291)\n",
            "Saving model (epoch = 2024, loss = 0.0291)\n",
            "Saving model (epoch = 2025, loss = 0.0291)\n",
            "Saving model (epoch = 2026, loss = 0.0290)\n",
            "Saving model (epoch = 2027, loss = 0.0290)\n",
            "Saving model (epoch = 2028, loss = 0.0290)\n",
            "Saving model (epoch = 2029, loss = 0.0290)\n",
            "Saving model (epoch = 2030, loss = 0.0290)\n",
            "Saving model (epoch = 2031, loss = 0.0290)\n",
            "Saving model (epoch = 2032, loss = 0.0290)\n",
            "Saving model (epoch = 2033, loss = 0.0289)\n",
            "Saving model (epoch = 2034, loss = 0.0289)\n",
            "Saving model (epoch = 2035, loss = 0.0289)\n",
            "Saving model (epoch = 2036, loss = 0.0289)\n",
            "Saving model (epoch = 2079, loss = 0.0289)\n",
            "Saving model (epoch = 2080, loss = 0.0289)\n",
            "Saving model (epoch = 2081, loss = 0.0288)\n",
            "Saving model (epoch = 2082, loss = 0.0288)\n",
            "Saving model (epoch = 2083, loss = 0.0288)\n",
            "Saving model (epoch = 2084, loss = 0.0288)\n",
            "Saving model (epoch = 2085, loss = 0.0287)\n",
            "Saving model (epoch = 2086, loss = 0.0287)\n",
            "Saving model (epoch = 2087, loss = 0.0287)\n",
            "Saving model (epoch = 2232, loss = 0.0287)\n",
            "Saving model (epoch = 2233, loss = 0.0287)\n",
            "Saving model (epoch = 2234, loss = 0.0287)\n",
            "Saving model (epoch = 2235, loss = 0.0287)\n",
            "Saving model (epoch = 2236, loss = 0.0287)\n",
            "Saving model (epoch = 2237, loss = 0.0286)\n",
            "Saving model (epoch = 2238, loss = 0.0286)\n",
            "Saving model (epoch = 2239, loss = 0.0286)\n",
            "Saving model (epoch = 2240, loss = 0.0286)\n",
            "Saving model (epoch = 2241, loss = 0.0285)\n",
            "Saving model (epoch = 2242, loss = 0.0285)\n",
            "Saving model (epoch = 2243, loss = 0.0285)\n",
            "Saving model (epoch = 2244, loss = 0.0284)\n",
            "Saving model (epoch = 2245, loss = 0.0284)\n",
            "Saving model (epoch = 2246, loss = 0.0284)\n",
            "Saving model (epoch = 2247, loss = 0.0284)\n",
            "Saving model (epoch = 2248, loss = 0.0283)\n",
            "Saving model (epoch = 2249, loss = 0.0283)\n",
            "Saving model (epoch = 2250, loss = 0.0283)\n",
            "Saving model (epoch = 2251, loss = 0.0283)\n",
            "Saving model (epoch = 2252, loss = 0.0283)\n",
            "Saving model (epoch = 2253, loss = 0.0283)\n",
            "Saving model (epoch = 2254, loss = 0.0282)\n",
            "Saving model (epoch = 2255, loss = 0.0282)\n",
            "Saving model (epoch = 2256, loss = 0.0282)\n",
            "Saving model (epoch = 2257, loss = 0.0282)\n",
            "Saving model (epoch = 2261, loss = 0.0282)\n",
            "Saving model (epoch = 2262, loss = 0.0282)\n",
            "Saving model (epoch = 2263, loss = 0.0282)\n",
            "Saving model (epoch = 2264, loss = 0.0282)\n",
            "Saving model (epoch = 2265, loss = 0.0282)\n",
            "Saving model (epoch = 2266, loss = 0.0282)\n",
            "Saving model (epoch = 2267, loss = 0.0282)\n",
            "Saving model (epoch = 2268, loss = 0.0281)\n",
            "Saving model (epoch = 2269, loss = 0.0281)\n",
            "Saving model (epoch = 2270, loss = 0.0281)\n",
            "Saving model (epoch = 2271, loss = 0.0281)\n",
            "Saving model (epoch = 2272, loss = 0.0281)\n",
            "Saving model (epoch = 2273, loss = 0.0281)\n",
            "Saving model (epoch = 2443, loss = 0.0280)\n",
            "Saving model (epoch = 2444, loss = 0.0280)\n",
            "Saving model (epoch = 2445, loss = 0.0280)\n",
            "Saving model (epoch = 2446, loss = 0.0279)\n",
            "Saving model (epoch = 2447, loss = 0.0279)\n",
            "Saving model (epoch = 2448, loss = 0.0279)\n",
            "Saving model (epoch = 2513, loss = 0.0279)\n",
            "Saving model (epoch = 2514, loss = 0.0278)\n",
            "Saving model (epoch = 2515, loss = 0.0278)\n",
            "Saving model (epoch = 2516, loss = 0.0278)\n",
            "Saving model (epoch = 2517, loss = 0.0277)\n",
            "Saving model (epoch = 2518, loss = 0.0277)\n",
            "Saving model (epoch = 2519, loss = 0.0277)\n",
            "Saving model (epoch = 2520, loss = 0.0277)\n",
            "Saving model (epoch = 2521, loss = 0.0277)\n",
            "Saving model (epoch = 2522, loss = 0.0277)\n",
            "Saving model (epoch = 2523, loss = 0.0277)\n",
            "Saving model (epoch = 2524, loss = 0.0277)\n",
            "Saving model (epoch = 2525, loss = 0.0277)\n",
            "Saving model (epoch = 2526, loss = 0.0276)\n",
            "Saving model (epoch = 2527, loss = 0.0276)\n",
            "Saving model (epoch = 2528, loss = 0.0276)\n",
            "Saving model (epoch = 2529, loss = 0.0276)\n",
            "Saving model (epoch = 2530, loss = 0.0276)\n",
            "Saving model (epoch = 2531, loss = 0.0275)\n",
            "Saving model (epoch = 2532, loss = 0.0275)\n",
            "Saving model (epoch = 2533, loss = 0.0275)\n",
            "Saving model (epoch = 2535, loss = 0.0275)\n",
            "Saving model (epoch = 2633, loss = 0.0275)\n",
            "Saving model (epoch = 2634, loss = 0.0275)\n",
            "Saving model (epoch = 2635, loss = 0.0275)\n",
            "Saving model (epoch = 2636, loss = 0.0275)\n",
            "Saving model (epoch = 2637, loss = 0.0275)\n",
            "Saving model (epoch = 2638, loss = 0.0275)\n",
            "Saving model (epoch = 2639, loss = 0.0275)\n",
            "Saving model (epoch = 2640, loss = 0.0274)\n",
            "Saving model (epoch = 2641, loss = 0.0274)\n",
            "Saving model (epoch = 2642, loss = 0.0274)\n",
            "Saving model (epoch = 2692, loss = 0.0274)\n",
            "Saving model (epoch = 2693, loss = 0.0274)\n",
            "Saving model (epoch = 2694, loss = 0.0273)\n",
            "Saving model (epoch = 2695, loss = 0.0273)\n",
            "Saving model (epoch = 2696, loss = 0.0273)\n",
            "Saving model (epoch = 2698, loss = 0.0273)\n",
            "Saving model (epoch = 2818, loss = 0.0273)\n",
            "Saving model (epoch = 2820, loss = 0.0273)\n",
            "Saving model (epoch = 2839, loss = 0.0273)\n",
            "Saving model (epoch = 2840, loss = 0.0273)\n",
            "Saving model (epoch = 2841, loss = 0.0272)\n",
            "Saving model (epoch = 2842, loss = 0.0272)\n",
            "Saving model (epoch = 2852, loss = 0.0272)\n",
            "Saving model (epoch = 2853, loss = 0.0272)\n",
            "Saving model (epoch = 2854, loss = 0.0272)\n",
            "Saving model (epoch = 2855, loss = 0.0272)\n",
            "Saving model (epoch = 2892, loss = 0.0272)\n",
            "Saving model (epoch = 2893, loss = 0.0272)\n",
            "Saving model (epoch = 2894, loss = 0.0272)\n",
            "Saving model (epoch = 2895, loss = 0.0272)\n",
            "Saving model (epoch = 2896, loss = 0.0271)\n",
            "Saving model (epoch = 2897, loss = 0.0271)\n",
            "Saving model (epoch = 2898, loss = 0.0271)\n",
            "Saving model (epoch = 2899, loss = 0.0271)\n",
            "Saving model (epoch = 2900, loss = 0.0271)\n",
            "Saving model (epoch = 2901, loss = 0.0271)\n",
            "Saving model (epoch = 2913, loss = 0.0271)\n",
            "Saving model (epoch = 2914, loss = 0.0270)\n",
            "Saving model (epoch = 2915, loss = 0.0270)\n",
            "Saving model (epoch = 2916, loss = 0.0270)\n",
            "Saving model (epoch = 2917, loss = 0.0269)\n",
            "Saving model (epoch = 2918, loss = 0.0269)\n",
            "Saving model (epoch = 2919, loss = 0.0269)\n",
            "Saving model (epoch = 2920, loss = 0.0269)\n",
            "Saving model (epoch = 2921, loss = 0.0269)\n",
            "Saving model (epoch = 2922, loss = 0.0269)\n",
            "Saving model (epoch = 2923, loss = 0.0269)\n",
            "Saving model (epoch = 2926, loss = 0.0269)\n",
            "Saving model (epoch = 2927, loss = 0.0269)\n",
            "Saving model (epoch = 2929, loss = 0.0268)\n",
            "Saving model (epoch = 2930, loss = 0.0268)\n",
            "Saving model (epoch = 2932, loss = 0.0268)\n",
            "Saving model (epoch = 2933, loss = 0.0268)\n",
            "Saving model (epoch = 2934, loss = 0.0268)\n",
            "Saving model (epoch = 2937, loss = 0.0268)\n",
            "Saving model (epoch = 2938, loss = 0.0268)\n",
            "Saving model (epoch = 2939, loss = 0.0268)\n",
            "Finished training after 3000 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pred(preds, file):\n",
        "    import pandas as pd\n",
        "    print('Saving results to {}'.format(file))\n",
        "    # with open(file, 'w') as fp:\n",
        "    #     writer = csv.writer(fp)\n",
        "    #     writer.writerow(['id', 'tested_positive'])\n",
        "    #     for i, p in enumerate(preds):\n",
        "    #         writer.writerow([i, p])\n",
        "    df = pd.DataFrame(preds,columns=[\"tested_positive\"])\n",
        "    df.to_csv(file)\n",
        "\n",
        "preds = test(test_set, model, device)  # predict COVID-19 cases with your model\n",
        "save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
      ],
      "metadata": {
        "id": "JcymugDXMZ4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb8c04c-ea31-4b4f-f37c-dc3b10ca4dcd"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results to pred.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNet(train_set.dataset.dim).to(device)  # Construct model and move to device\n",
        "model_loading = torch.load(h_paras['save_path'], map_location=device)  # Load your best model\n",
        "model.load_state_dict(model_loading)"
      ],
      "metadata": {
        "id": "RGl3nIhF6aBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11c5ed2-458e-41ca-c90a-1a15e252b366"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    }
  ]
}