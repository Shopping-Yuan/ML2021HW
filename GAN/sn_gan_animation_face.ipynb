{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/ML_Project/blob/Shopping_vscode_branch/HW6/HW06_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hw6\n",
        "###Part 0 : setting & install package\n",
        "###Part 1 : data cleaning\n",
        "###Part 2 : build model\n",
        "###Part 3 : training and validation\n",
        "###Part 4 : inference"
      ],
      "metadata": {
        "id": "iR-_Z01Xkjiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###setting"
      ],
      "metadata": {
        "id": "NF6NBgMgl_Y7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDuJitOPW69-"
      },
      "outputs": [],
      "source": [
        "setting = {\n",
        "# information of the path of dataset\n",
        "\"documents\" : {\n",
        "    \"workspace_dir\" : \"/content/drive/MyDrive/HW6_data\",\n",
        "    \"log_doc_path\" : \"/log\",\n",
        "    \"ckpt_file_path\" : \"/model\",\n",
        "    \"model_temp_save_path\" : \"/temporary_model\",\n",
        "    \"out_doc_path\" : \"/sn_gan_out\",\n",
        "},\n",
        "\"dataset\" : {\n",
        "},\n",
        "\"dataloader\" : {\n",
        "    \"batch_size\" : 64,\n",
        "    \"num_workers\" : 2,\n",
        "    \"shuffle\" : True,\n",
        "},\n",
        "# model setting\n",
        "\"model\" : {\n",
        "    \"sample_number\" : 100,\n",
        "    \"generator_in_dimension\" : 100\n",
        "},\n",
        "# setting in training and validation process ,\n",
        "# including optimization setting.\n",
        "\"training_hparas\" : {\n",
        "    \"total_epoch\" : 50, # 50\n",
        "    \"generator_step\" : 5, # 5\n",
        "    \"temp_save_epoch\" : 1,\n",
        "    \"wgan_clip_value\" : 0.01,\n",
        "    },\n",
        "\"generator_optimizer\" : {\n",
        "    \"lr\" : 1e-4,\n",
        "    \"betas\" : (0.5, 0.999)\n",
        "    },\n",
        "\"discriminator_optimizer\" : {\n",
        "    \"lr\" : 1e-4,\n",
        "    \"betas\" : (0.5, 0.999)\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###install package"
      ],
      "metadata": {
        "id": "LRia-UVamFIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qqdm\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYFCGoI7XgrK",
        "outputId": "374f1a2f-e3d9-4dcd-ff70-226a9c5834e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qqdm\n",
            "  Downloading qqdm-0.0.7.tar.gz (5.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting addict (from qqdm)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting jupyter (from qqdm)\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm) (6.5.5)\n",
            "Collecting qtconsole (from jupyter->qqdm)\n",
            "  Downloading qtconsole-5.5.1-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm) (7.7.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm) (6.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->qqdm) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->qqdm) (3.0.10)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->qqdm) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->qqdm) (2.16.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (3.1.3)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (5.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (0.9.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (5.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (24.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm) (1.2.1)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm) (0.18.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm) (1.0.0)\n",
            "Collecting qtpy>=2.4.0 (from qtconsole->jupyter->qqdm)\n",
            "  Downloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->jupyter->qqdm)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->jupyter->qqdm) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter->qqdm) (4.2.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter->qqdm) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter->qqdm) (0.2.4)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->qqdm) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->qqdm) (4.19.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->qqdm) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter->qqdm) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter->qqdm) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter->qqdm) (2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter->qqdm) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter->qqdm) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter->qqdm) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->qqdm) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->qqdm) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->qqdm) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->qqdm) (0.18.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->qqdm) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->qqdm) (1.7.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->qqdm) (1.16.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->qqdm) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->qqdm) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->qqdm) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->qqdm) (2.21)\n",
            "Building wheels for collected packages: qqdm\n",
            "  Building wheel for qqdm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qqdm: filename=qqdm-0.0.7-py3-none-any.whl size=6466 sha256=50c750d148c1f23c38a52244a2d12f5a36e88ca24a99a8ebe9a200dcaa809a20\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1a/56/5dccdea123a172661eb65c8c29fde4567dbda2b72b5fc5893a\n",
            "Successfully built qqdm\n",
            "Installing collected packages: addict, qtpy, jedi, qtconsole, jupyter, qqdm\n",
            "Successfully installed addict-2.4.0 jedi-0.19.1 jupyter-1.0.0 qqdm-0.0.7 qtconsole-5.5.1 qtpy-2.4.1\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 : download dataset from drive to google colab\n",
        "# original dataset is in \"https://mega.nz/#!vEcTCISJ!3Rw0eHTZWPpdHBTbQEqBDikDEdFPr7fI8WxaXK9yZ9U\"\n",
        "\n",
        "workspace_dir = \"/content/drive/MyDrive/HW6_data\"\n",
        "rawdata_file_path = workspace_dir + \"/crypko_data.zip\"\n",
        "unzip_path = \"/content/unzip_image/\"\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive_path = \"/content/drive\"\n",
        "drive.mount(drive_path)\n",
        "\n",
        "# step 2 : unzip dataset\n",
        "import zipfile\n",
        "with zipfile.ZipFile(rawdata_file_path, 'r') as zip_f:\n",
        "    zip_f.extractall(unzip_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9wHCD-ZYDl4",
        "outputId": "d77bce63-c82d-455f-da4c-c765aee7d8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def seeds(seed):\n",
        "    # Python built-in random module\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Torch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "8V1sAzE2YQXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import os\n",
        "import glob\n",
        "import torchvision\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "class CrypkoDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.imgs = glob.glob(os.path.join(unzip_path,\"faces\",\"*\"))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = torchvision.io.read_image(self.imgs[index])\n",
        "    return self.crypko_transform(img)\n",
        "    # return img\n",
        "\n",
        "  def crypko_transform(self,img):\n",
        "    compose = [\n",
        "          v2.ToPILImage(),\n",
        "          v2.Resize((64, 64)),\n",
        "          v2.ToTensor(),\n",
        "          v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "          ]\n",
        "    transform =  v2.Compose(compose)\n",
        "    return transform(img)\n",
        "\n",
        "def get_dataloader(dataset,batch_size,shuffle,num_workers):\n",
        "  return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "nfWGElInr_bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = CrypkoDataset()\n",
        "# images = [dataset[i] for i in range(100)]\n",
        "# # print(images[0][0][0][0],type(images[0][0][0][0].item()))\n",
        "# grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
        "# plt.figure(figsize=(10,10))\n",
        "# out = grid_img.permute(1, 2, 0)\n",
        "# plt.imshow(out)\n",
        "# plt.show()\n",
        "\n",
        "# from qqdm.notebook import qqdm\n",
        "# loader = DataLoader(dataset, batch_size=12, shuffle=True, num_workers=0)\n",
        "# step = 0\n",
        "# for data in qqdm(loader):\n",
        "#   if step<2:\n",
        "#     images = [data[i] for i in range(10)]\n",
        "#     grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
        "#     plt.figure(figsize=(10,10))\n",
        "#     out = grid_img.permute(1, 2, 0)\n",
        "#     plt.imshow(out)\n",
        "#     plt.show()\n",
        "#   step += 1"
      ],
      "metadata": {
        "id": "yvdMuE6dkh9A"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "def weights_init(layer):\n",
        "    classname = layer.__class__.__name__\n",
        "    # if hasattr(layer,\"weight\"):\n",
        "    #   print(classname,layer.weight.shape)\n",
        "    if classname.find('Conv') != -1:\n",
        "        if hasattr(layer,\"weight\"):\n",
        "          layer.weight.data.normal_(0.0, 0.02)\n",
        "        elif hasattr(layer,\"w_without_sn\"):\n",
        "          layer.w_without_sn.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        if hasattr(layer,\"weight\"):\n",
        "          layer.weight.data.normal_(1.0, 0.02)\n",
        "        elif hasattr(layer,\"w_without_sn\"):\n",
        "          layer.w_without_sn.data.normal_(1.0, 0.02)\n",
        "        layer.bias.data.fill_(0)\n",
        "class Spectral_Normalization(nn.Module):\n",
        "    def __init__(self,layer,eps = 1e-12,power_iteration = 1):\n",
        "        super().__init__()\n",
        "        self.layer = layer\n",
        "        self.classname = self.layer.__class__.__name__\n",
        "        self.eps = eps\n",
        "        self.power_iteration = power_iteration\n",
        "        self.set_sn_weight()\n",
        "    \"\"\"\n",
        "    in forward process : spectral normalization weight is used\n",
        "    but in backward process : calculate the gradient of original weight\n",
        "    (i.e. weight without spectral normalization)\n",
        "\n",
        "    So the data of the original weight must be keep as a new \"parameter\",\n",
        "    then the \"parameter\" \"weight\"(whose data is the original weight) must be delete,\n",
        "    and spectral normalization weight must be add as \"attribute\"(not \"parameter\") named \"weight\".\n",
        "\n",
        "    Since spectral normalization weight is named \"weight\" but not a \"parameter\",\n",
        "    it will be used to compute the output in forward process.(named \"weight\")\n",
        "    But its gradient will not calculated in backward process.(not a \"parameter\")\n",
        "\n",
        "    On the contrary, since the original weight is save as a new \"parameter\",\n",
        "    the gradient of original weight is calculated in backward process,\n",
        "    so the parameter (with respected to original weight) will be updated.\n",
        "\n",
        "    In next forward process, with the updated parameter (with respected to original weight)\n",
        "    the new spectral normalization weight can be computed.\n",
        "    Reset the attribute \"weight\"(with respected to new spectral normalization weight),\n",
        "    the new output of can be computed.\n",
        "    \"\"\"\n",
        "    def set_sn_weight(self):\n",
        "        # save weight to another parameter\n",
        "        weight_without_sn = getattr(self.layer, \"weight\").data\n",
        "        self.layer.register_parameter(self.classname+\"w_without_sn\", nn.Parameter(weight_without_sn))\n",
        "        # delete the original weight\n",
        "        del self.layer._parameters[\"weight\"]\n",
        "\n",
        "        # to computed spectral normalization weight,\n",
        "        # u and v (SVD matrixs of weight) must be setting\n",
        "        # step 1 : reshape weight to 2D-matrix to produce u,v\n",
        "        weight_2D = weight_without_sn.view(weight_without_sn.shape[0],-1)\n",
        "\n",
        "        # step 2 : using weight_2D to initialize u and v\n",
        "        u = torch.empty(weight_2D.shape[0]).normal_(0, 1)\n",
        "        v = torch.empty(weight_2D.shape[1]).normal_(0, 1)\n",
        "        u.data = u.data/(u.data.norm() + self.eps)\n",
        "        v.data = v.data/(v.data.norm() + self.eps)\n",
        "\n",
        "        # step 3 : u,v are saved as buffer for updating spectral normalization weight\n",
        "        self.layer.register_buffer(self.classname+\"_u\", u)\n",
        "        self.layer.register_buffer(self.classname+\"_v\", v)\n",
        "\n",
        "    # update_sn_weight is the function call in forward process\n",
        "    # spectral normalization weight is computed/updated here\n",
        "    def update_sn_weight(self):\n",
        "        # step 4 : get u,v ,get and reshape original weight\n",
        "        u = getattr(self.layer, self.classname + \"_u\")\n",
        "        v = getattr(self.layer, self.classname + \"_v\")\n",
        "\n",
        "        weight_without_sn = getattr(self.layer, self.classname+\"w_without_sn\")\n",
        "        w_shape = weight_without_sn.shape\n",
        "        weight_2D = weight_without_sn.view(w_shape[0],-1)\n",
        "\n",
        "        # step 5 : using power iteration to update u,v\n",
        "\n",
        "        for times in range(self.power_iteration):\n",
        "          v = torch.mv(torch.transpose(weight_2D, 0, 1),u)\n",
        "          u = torch.mv(weight_2D,v)\n",
        "          u.data = u.data/(u.data.norm() + self.eps)\n",
        "          v.data = v.data/(v.data.norm() + self.eps)\n",
        "\n",
        "        # step 6 : compute spectral normalization weight using u,v\n",
        "        w_max_sv = u@(torch.mv(weight_2D,v)).data\n",
        "        w_sn = weight_without_sn / w_max_sv\n",
        "\n",
        "        # add new weight(spectral normalization weight) as attribute back to the layer\n",
        "        setattr(self.layer, \"weight\", w_sn)\n",
        "\n",
        "    def forward(self,x):\n",
        "      self.update_sn_weight()\n",
        "      return(self.layer.forward(x))\n",
        "\n",
        "\n",
        "class Print(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "    def forward(self,x):\n",
        "      print(x.shape)\n",
        "      return x\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_dim, dim=64):\n",
        "      super().__init__()\n",
        "      self.indim = in_dim\n",
        "      self.dim = dim\n",
        "      def dconv_bn_relu(in_dim, out_dim):\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_dim, out_dim, 5, 2,\n",
        "                          padding=2, output_padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_dim),\n",
        "                nn.ReLU())\n",
        "      self.l1 = nn.Sequential(\n",
        "            nn.Linear(self.indim, self.dim * 8 * 4 * 4, bias=False),\n",
        "            nn.BatchNorm1d(self.dim * 8 * 4 * 4),\n",
        "            nn.ReLU())\n",
        "      self.l2_5 = nn.Sequential(\n",
        "            dconv_bn_relu(self.dim * 8, self.dim * 4),\n",
        "            dconv_bn_relu(self.dim * 4, self.dim * 2),\n",
        "            dconv_bn_relu(self.dim * 2, self.dim),\n",
        "            nn.ConvTranspose2d(self.dim, 3, 5, 2, padding=2, output_padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "      self.apply(weights_init)\n",
        "    def forward(self,x):\n",
        "      y = self.l1(x)\n",
        "      batch_size = y.size(0)\n",
        "      dconv_dim = self.dim*8\n",
        "      y = y.view(batch_size,dconv_dim,4,4)\n",
        "      y = self.l2_5(y)\n",
        "      return y\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Input shape: (N, 3, 64, 64)\n",
        "    Output shape: (N, )\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        def conv_bn_lrelu(in_dim, out_dim):\n",
        "            return nn.Sequential(\n",
        "                Spectral_Normalization(nn.Conv2d(in_dim, out_dim, 5, 2, 2)),\n",
        "                Spectral_Normalization(nn.BatchNorm2d(out_dim)),\n",
        "                nn.LeakyReLU(0.2))\n",
        "\n",
        "        \"\"\" Medium: Remove the last sigmoid layer for WGAN. \"\"\"\n",
        "        self.ls = nn.Sequential(\n",
        "            Spectral_Normalization(nn.Conv2d(in_dim, dim, 5, 2, 2)),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            conv_bn_lrelu(dim, dim * 2),\n",
        "            conv_bn_lrelu(dim * 2, dim * 4),\n",
        "            conv_bn_lrelu(dim * 4, dim * 8),\n",
        "            Spectral_Normalization(nn.Conv2d(dim * 8, 1, 4)),\n",
        "            # nn.Sigmoid(),\n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.ls(x)\n",
        "        y = y.view(-1)\n",
        "        return y"
      ],
      "metadata": {
        "id": "x-GS6dFHo4LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchinfo import summary\n",
        "model_G = Generator(100)\n",
        "# print(summary(model_G,(32,100)))\n",
        "# input = torch.rand(32,100)\n",
        "# print(model_G(input).shape)\n",
        "model_D = Discriminator(3)\n",
        "print(model_D.state_dict().keys())\n",
        "\n",
        "# print(summary(model_D,(32,3,64,64)))\n",
        "# input = torch.rand(32,3,64,64)\n",
        "# print(model_D(input).shape)"
      ],
      "metadata": {
        "id": "yZh-Rnqu637V",
        "outputId": "c6d72021-6465-4426-a6b5-ab05b63b315d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['ls.0.layer.bias', 'ls.0.layer.Conv2dw_without_sn', 'ls.0.layer.Conv2d_u', 'ls.0.layer.Conv2d_v', 'ls.2.0.layer.bias', 'ls.2.0.layer.Conv2dw_without_sn', 'ls.2.0.layer.Conv2d_u', 'ls.2.0.layer.Conv2d_v', 'ls.2.1.layer.bias', 'ls.2.1.layer.BatchNorm2dw_without_sn', 'ls.2.1.layer.running_mean', 'ls.2.1.layer.running_var', 'ls.2.1.layer.num_batches_tracked', 'ls.2.1.layer.BatchNorm2d_u', 'ls.2.1.layer.BatchNorm2d_v', 'ls.3.0.layer.bias', 'ls.3.0.layer.Conv2dw_without_sn', 'ls.3.0.layer.Conv2d_u', 'ls.3.0.layer.Conv2d_v', 'ls.3.1.layer.bias', 'ls.3.1.layer.BatchNorm2dw_without_sn', 'ls.3.1.layer.running_mean', 'ls.3.1.layer.running_var', 'ls.3.1.layer.num_batches_tracked', 'ls.3.1.layer.BatchNorm2d_u', 'ls.3.1.layer.BatchNorm2d_v', 'ls.4.0.layer.bias', 'ls.4.0.layer.Conv2dw_without_sn', 'ls.4.0.layer.Conv2d_u', 'ls.4.0.layer.Conv2d_v', 'ls.4.1.layer.bias', 'ls.4.1.layer.BatchNorm2dw_without_sn', 'ls.4.1.layer.running_mean', 'ls.4.1.layer.running_var', 'ls.4.1.layer.num_batches_tracked', 'ls.4.1.layer.BatchNorm2d_u', 'ls.4.1.layer.BatchNorm2d_v', 'ls.5.layer.bias', 'ls.5.layer.Conv2dw_without_sn', 'ls.5.layer.Conv2d_u', 'ls.5.layer.Conv2d_v'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(list(setting[\"discriminator_optimizer\"].values())[0])"
      ],
      "metadata": {
        "id": "o9d16rSM27Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GP_term(G_out,ref,D,l=10):\n",
        "  epison = torch.rand((1)).item()\n",
        "  G_out_detach = G_out.detach()\n",
        "  ref_detach = ref.detach()\n",
        "  random_point = epison*ref_detach + (1-epison)*G_out_detach\n",
        "  random_point.requires_grad = True\n",
        "  D.zero_grad()\n",
        "  for p in D.parameters():\n",
        "    p.requires_grad = False\n",
        "  diff = D(random_point)\n",
        "  diff.backward(torch.ones(diff.size(0)))\n",
        "  grad = random_point.grad\n",
        "  for p in D.parameters():\n",
        "    p.requires_grad = True\n",
        "  grad_flat = grad.view(32,-1)\n",
        "  return(l*torch.square(torch.torch.norm(grad_flat,dim = -1)-1))\n",
        "\n",
        "# from torchinfo import summary\n",
        "# dataset = CrypkoDataset()\n",
        "# ref = torch.stack([dataset[i] for i in range(32)])\n",
        "# print(ref.shape)\n",
        "# model_G = Generator(100)\n",
        "# G_input = torch.rand(32,100)\n",
        "# G_out = model_G(input)\n",
        "# print(model_G(input).shape)\n",
        "# model_D = Discriminator(3)\n",
        "# GP_term(G_out,ref,model_D,l=10).shape"
      ],
      "metadata": {
        "id": "BmzH7jpcjBED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from qqdm.notebook import qqdm\n",
        "import torch.nn.functional as F\n",
        "def train_val(documents,dataset,dataloader,model,training_hparas,\n",
        "        generator_optimizer,discriminator_optimizer):\n",
        "\n",
        "  #doc paths\n",
        "  paths = list(documents.values())\n",
        "  workspace_dir, log_doc_path, ckpt_file_path, model_temp_save_path, out_doc_path = paths\n",
        "  print(workspace_dir)\n",
        "  for path in paths[1:]:\n",
        "    full_path = workspace_dir+path\n",
        "    os.makedirs(full_path,exist_ok=True)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  # Dataset\n",
        "  dataset = CrypkoDataset()\n",
        "  # DataLoader\n",
        "  # data_loader = get_dataloader(dataset, **dataloader)\n",
        "  data_loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "  # test sample\n",
        "  sample_number, generator_in_dimension = list(model.values())\n",
        "  sample = torch.randn(sample_number, generator_in_dimension)\n",
        "  sample = sample.to(device)\n",
        "\n",
        "  # Model\n",
        "  G = Generator(in_dim = generator_in_dimension).to(device)\n",
        "  D = Discriminator(3).to(device)\n",
        "  G.train()\n",
        "  D.train()\n",
        "\n",
        "  # Loss\n",
        "  criterion = nn.BCELoss()\n",
        "  # set seeds\n",
        "  seeds(2021)\n",
        "  \"\"\" Medium: Use RMSprop for WGAN. \"\"\"\n",
        "  # Optimizer\n",
        "  # opt_D = torch.optim.Adam(D.parameters(), **discriminator_optimizer)\n",
        "  # opt_G = torch.optim.Adam(G.parameters(), **generator_optimizer)\n",
        "  opt_D = torch.optim.RMSprop(D.parameters(), lr = list(discriminator_optimizer.values())[0])\n",
        "  opt_G = torch.optim.RMSprop(G.parameters(), lr = list(generator_optimizer.values())[0])\n",
        "\n",
        "  # training_hparas\n",
        "  total_epoch, generator_step, temp_save_epoch, wgan_clip_value = \\\n",
        "  list(training_hparas.values())\n",
        "\n",
        "\n",
        "  steps = 0\n",
        "  for e, epoch in enumerate(range(total_epoch)):\n",
        "      progress_bar = qqdm(data_loader)\n",
        "      for i, data in enumerate(progress_bar):\n",
        "          imgs = data\n",
        "          imgs = imgs.to(device)\n",
        "          batch_size = imgs.size(0)\n",
        "          # if steps % 100 == 0:\n",
        "          #   images = [data[j] for j in range(int(batch_size/5))]\n",
        "          #   grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
        "          #   plt.figure(figsize=(10,10))\n",
        "          #   out = grid_img.permute(1, 2, 0)\n",
        "          #   plt.imshow(out)\n",
        "          #   plt.show()\n",
        "\n",
        "          # ============================================\n",
        "          #  Train D\n",
        "          # ============================================\n",
        "          train_sample = torch.randn(batch_size, generator_in_dimension)\n",
        "          train_sample = train_sample.to(device)\n",
        "          r_imgs = imgs.to(device)\n",
        "          f_imgs = G(train_sample)\n",
        "\n",
        "          \"\"\" Medium: Use WGAN Loss. \"\"\"\n",
        "          # Label\n",
        "          # r_label = torch.ones((batch_size)).to(device)\n",
        "          # f_label = torch.zeros((batch_size)).to(device)\n",
        "          # Model forwarding\n",
        "          # r_logit = D(r_imgs.detach())\n",
        "          # f_logit = D(f_imgs.detach())\n",
        "\n",
        "          # Compute the loss for the discriminator.\n",
        "          # r_loss = criterion(r_logit, r_label)\n",
        "          # f_loss = criterion(f_logit, f_label)\n",
        "          # loss_D = (r_loss + f_loss) / 2\n",
        "\n",
        "          # WGAN Loss\n",
        "          # loss_D = -1*torch.mean(D(r_imgs)) + torch.mean(D(f_imgs))\n",
        "          # WGAN-GP Loss\n",
        "          # loss_D = -1*torch.mean(D(r_imgs)) + torch.mean(D(f_imgs)) +\n",
        "          #      torch.mean(GP_term(f_imgs,r_imgs,D,10))\n",
        "          # SN-GAN Loss\n",
        "          # D_r = D(r_imgs)\n",
        "          # D_r[-1+D_r<0] = 0\n",
        "          # D_f = D(f_imgs)\n",
        "          # D_f[(-1-D_f)<0] = 0\n",
        "          # loss_D = torch.mean(-1+D_r)+torch.mean(-1-D_f)\n",
        "          loss_D = (torch.mean(F.relu(1-D(r_imgs)))+torch.mean(F.relu(1+D(f_imgs))))\n",
        "\n",
        "\n",
        "          # Model backwarding\n",
        "          D.zero_grad()\n",
        "          loss_D.backward()\n",
        "\n",
        "          # Update the discriminator.\n",
        "          opt_D.step()\n",
        "\n",
        "          \"\"\" Medium: Clip weights of discriminator. \"\"\"\n",
        "          for p in D.parameters():\n",
        "             p.data.clamp_(-1*wgan_clip_value, wgan_clip_value)\n",
        "\n",
        "          # ============================================\n",
        "          #  Train G\n",
        "          # ============================================\n",
        "          if steps % generator_step == 0:\n",
        "              D.zero_grad()\n",
        "              for p in D.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "              # Generate some fake images.\n",
        "              train_sample = torch.randn(batch_size, generator_in_dimension)\n",
        "              train_sample = train_sample.to(device)\n",
        "              f_imgs = G(train_sample)\n",
        "\n",
        "              # Model forwarding\n",
        "              # f_logit = D(f_imgs)\n",
        "\n",
        "\n",
        "              \"\"\" Medium: Use WGAN Loss\"\"\"\n",
        "              # Compute the loss for the generator.\n",
        "              # loss_G = criterion(f_logit, r_label)\n",
        "              # WGAN Loss\n",
        "\n",
        "              loss_G = -torch.mean(D(f_imgs))\n",
        "\n",
        "              # Model backwarding\n",
        "              G.zero_grad()\n",
        "              loss_G.backward()\n",
        "              # for name, paras in D.named_parameters():\n",
        "              #   print(paras.grad)\n",
        "              # Update the generator.\n",
        "              opt_G.step()\n",
        "\n",
        "              for p in D.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "          steps += 1\n",
        "\n",
        "          # Set the info of the progress bar\n",
        "          #   Note that the value of the GAN loss is not directly related to\n",
        "          #   the quality of the generated images.\n",
        "          progress_bar.set_infos({\n",
        "              'Loss_D': round(loss_D.item(), 4),\n",
        "              'Loss_G': round(loss_G.item(), 4),\n",
        "              'Epoch': e+1,\n",
        "              'Step': steps,\n",
        "          })\n",
        "\n",
        "      G.eval()\n",
        "      f_imgs_sample = (G(sample).data + 1) / 2.0\n",
        "      filename = workspace_dir + log_doc_path + f'Wgan_GP_Epoch_{epoch+1:03d}.jpg'\n",
        "      torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n",
        "      print(f' | Save some samples to {filename}.')\n",
        "\n",
        "      # Show generated images in the jupyter notebook.\n",
        "      grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n",
        "      plt.figure(figsize=(10,10))\n",
        "      plt.imshow(grid_img.permute(1, 2, 0))\n",
        "      plt.show()\n",
        "      G.train()\n",
        "\n",
        "      if (e+1) % 5 == 0 or e == 0:\n",
        "          # Save the checkpoints.\n",
        "          torch.save(G.state_dict(), workspace_dir + ckpt_file_path + 'G_GP.pth')\n",
        "          torch.save(D.state_dict(), workspace_dir + ckpt_file_path + 'D_GP.pth')"
      ],
      "metadata": {
        "id": "75F27YzEpShx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_val(**setting)"
      ],
      "metadata": {
        "id": "RrvrE0K4lYnP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def inference(documents,model,**kwargs):\n",
        "  # documents\n",
        "    paths = list(documents.values())\n",
        "    workspace_dir, log_doc_path, ckpt_file_path, model_temp_save_path, out_doc_path = paths\n",
        "\n",
        "  # Model\n",
        "    sample_number, generator_in_dimension = list(model.values())\n",
        "    G = Generator(in_dim = generator_in_dimension)\n",
        "    G.load_state_dict(torch.load(workspace_dir + ckpt_file_path + 'G_GP.pth'))\n",
        "    G.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    G.to(device)\n",
        "    # Generate 1000 images and make a grid to save them.\n",
        "    sample_number = 1000\n",
        "    sample = torch.randn(sample_number, generator_in_dimension).to(device)\n",
        "    imgs_sample = (G(sample).data + 1) / 2.0\n",
        "\n",
        "    # log and save image\n",
        "    log_dir = workspace_dir+log_doc_path\n",
        "    filename = os.path.join(log_dir, 'result.jpg')\n",
        "    torchvision.utils.save_image(imgs_sample, filename, nrow=10)\n",
        "\n",
        "    # Show 32 of the images.\n",
        "    grid_img = torchvision.utils.make_grid(imgs_sample[:32].cpu(), nrow=10)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(grid_img.permute(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "    # Save the generated images.\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    for i in range(1000):\n",
        "        torchvision.utils.save_image(imgs_sample[i], f'output/{i+1}.jpg')\n"
      ],
      "metadata": {
        "id": "Jg0neoFboZL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference(**setting)"
      ],
      "metadata": {
        "id": "z6B1VNQ2whh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compress the images.\n",
        "%cd output\n",
        "!tar -zcf ../images.tgz *.jpg\n",
        "%cd .."
      ],
      "metadata": {
        "id": "jIiKQFxjtV67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spectral norm test\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# l1 = nn.Linear(3, 2, bias=False)\n",
        "# l2 = nn.Linear(2, 1, bias=False)\n",
        "\n",
        "# w1 = getattr(l1, 'weight')\n",
        "# w1_bar = nn.Parameter(w1.data)\n",
        "\n",
        "# w2 = getattr(l2, 'weight')\n",
        "# w2_bar = nn.Parameter(w2.data)\n",
        "\n",
        "# l1.register_parameter('weight_bar1', w1_bar)\n",
        "# del l1._parameters['weight'], w1\n",
        "\n",
        "# l2.register_parameter('weight_bar2', w2_bar)\n",
        "# del l2._parameters['weight'], w2\n",
        "\n",
        "# w1 = getattr(l1, 'weight_bar1')\n",
        "# setattr(l1, 'weight', w1 / 10000.)\n",
        "# print(f\"l1_stat_dict is {l1.state_dict()}\")\n",
        "\n",
        "# w2 = getattr(l2, 'weight_bar2')\n",
        "# w3 = torch.tensor(w2.shape)\n",
        "# # setattr(l2, 'weight', w2 / 10000.)\n",
        "# setattr(l2, 'weight', w3 / 10000.)\n",
        "# print(f\"l2_stat_dict is {l2.state_dict()}\")\n",
        "\n",
        "# x = torch.tensor([1., 1., 1.])\n",
        "# y1 = l1(x)\n",
        "# y2 = l2(y1)\n",
        "# y_ = torch.empty(y2.shape).normal_(0, 1)\n",
        "\n",
        "\n",
        "# loss = torch.mean(y2 - y_)\n",
        "# loss.backward()\n",
        "# print(l2.weight_bar2.grad)\n",
        "# print(l2.weight.grad)\n",
        "# print(l1.weight_bar1.grad)\n",
        "# print(l1.weight.grad)\n",
        "# print(y1)\n",
        "# print(y2)\n",
        "\n",
        "#print(l1.weight)\n",
        "#print(l1.state_dict())\n"
      ],
      "metadata": {
        "id": "YlQ6XPEC6BBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
